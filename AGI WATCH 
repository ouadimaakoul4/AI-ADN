AGI Watch: A Civic Oversight Framework for AGI Development

White Paper v2.0
Date: November 2024
Status: Realistic Proposal for Initial Implementation

---

Executive Summary: A Focused, Phased Approach

The development of Artificial General Intelligence presents unprecedented governance challenges. Current trajectories risk catastrophic outcomes due to corporate secrecy, geopolitical competition, and inadequate safety measures.

AGI Watch proposes a realistic, phased civic response centered on one core premise: Transparency must precede accountability. We begin with a focused missionâ€”aggregating and analyzing publicly available information about AGI developmentâ€”before expanding to advocacy and resilience building.

This revised framework addresses critical feedback by:

1. Starting with what's feasible: Beginning with information synthesis rather than comprehensive monitoring
2. Building credibility first: Establishing value through high-quality analysis before expanding scope
3. Phasing growth responsibly: Proving impact at each stage before scaling
4. Acknowledging limitations: Being transparent about what we can and cannot track

We invite participation in building essential civic infrastructure for navigating the AGI transition.

---

1. Introduction: The Transparency Imperative

1.1 The Problem: Information Asymmetry

The "AI 2027" scenario reveals a critical failure: decision-makers and the public lack timely, accurate information about AGI progress. This creates:

Â· Reactive policymaking: Governments respond to crises rather than shaping development
Â· Uninformed public debate: Discussions lack grounding in technical reality
Â· Safety neglect: Competitive pressures overwhelm caution without visibility

1.2 Our Realistic Theory of Change

We operate on three evidence-based principles:

1. Information changes behavior: When companies know they're being watched, they act more responsibly (the "observer effect")
2. Quality synthesis creates leverage: Decision-makers facing information overload value curated, credible analysis
3. Networked expertise beats centralization: Distributed contributors can track developments no single organization can

1.3 Acknowledged Limitations

We explicitly recognize:

Â· Much cutting-edge research occurs in private settings
Â· True capability assessments require insider access
Â· Public pressure alone rarely changes corporate behavior
Â· Our influence will be gradual, not revolutionary

---

2. Revised Mission & Phased Vision

2.1 Mission (Phase 1: Years 1-2)

To become the most trusted source of synthesized public information about AGI development, making technical progress understandable to policymakers and the public.

2.2 Long-term Vision (Phase 3+)

A world with robust civic oversight of AGI development, where safety considerations meaningfully influence the pace and direction of progress.

2.3 Core Principles (Unchanged)

1. Radical Transparency
2. Technical Rigor
3. Inclusive Participation
4. Non-Partisan Stance
5. Global Perspective
6. Decentralized Resilience

---

3. Revised Organizational Model: Start Small, Scale Responsibly

3.1 Phase 1: Minimum Viable Organization (Months 1-12)

```
Structure:
[Founding Team (3 people)]
    â†“
[Volunteer Network]
    â†“
[Advisory Board]

Focus: 
- Information synthesis and dissemination
- Network building
- Methodology development

Success Metric: 
- Become go-to source for AGI progress reporting
```

3.2 Phase 2: Specialized Expansion (Year 2)

```
Add:
- Policy analysis function
- Regional correspondents (3-5 regions)
- Technical advisory group
```

3.3 Phase 3: Full Three-Pillar Model (Year 3+)

```
Only after proving value and securing resources
```

---

4. Revised Three-Pillar Strategy: Phased Implementation

4.1 Pillar 1: Monitoring & Synthesis (Phase 1 Focus)

4.1.1 Realistic Scope: What We Can Actually Track

```
High-Confidence Tracking:
1. Published Research:
   - arXiv preprints (ML, CS, AI safety)
   - Conference papers (NeurIPS, ICML, ICLR, AI Safety workshops)
   - Journal publications

2. Policy & Governance:
   - Legislative tracking (bills, hearings, regulations)
   - Corporate policy announcements
   - International agreements and statements

3. Public Indicators:
   - Corporate earnings calls and investor presentations
   - Hiring patterns (from public job postings)
   - Datacenter construction (public permits, announcements)

4. Economic Signals:
   - AI investment flows (VC funding, corporate R&D)
   - Compute purchasing announcements
   - Energy procurement for datacenters
```

4.1.2 What We Won't Try to Track (Initially)

```
Explicitly Excluded:
- Secret corporate research
- Classified government programs
- Real-time compute usage
- Detailed chip shipment data (proprietary)
- Employee movement not in public domain
```

4.1.3 Methodology: Quality Over Quantity

Â· Weekly Research Digest: Curated summary of 5-10 most important papers
Â· Monthly Policy Update: Analysis of 3-5 key developments
Â· Quarterly Progress Assessment: Holistic synthesis across domains
Â· Annual "State of AGI" Report: Comprehensive analysis

4.1.4 Initial Products (First 6 Months)

1. AGI Progress Newsletter (weekly): 3-5 key developments with analysis
2. Public Research Database: Searchable archive of papers with tags
3. Policy Tracker Spreadsheet: Simple tracking of key legislation
4. Expert Interview Series: Conversations with researchers about implications

4.2 Pillar 2: Advocacy & Influence (Phase 2+)

4.2.1 Revised Theory of Influence

We focus on becoming indispensable to decision-makers by:

1. Providing unique value: Information they can't get elsewhere
2. Building relationships: Regular briefings for staffers, regulators
3. Framing issues effectively: Connecting technical developments to policy choices

4.2.2 Initial Advocacy Activities (Limited, High-Impact)

1. Policy Brief Series: 2-page summaries of key issues
2. Expert Matchmaking: Connecting policymakers with technical experts
3. Public Comments: Submitting to regulatory consultations
4. Educational Workshops: For congressional/staffers

4.2.3 Partnership-First Approach

Rather than building advocacy capacity, we:

Â· Amplify existing advocacy organizations' work
Â· Provide research to organizations with established influence
Â· Collaborate on joint initiatives where we provide unique value

4.3 Pillar 3: Community Resilience (Phase 3+)

4.3.1 Revised Framing: "Navigating Technological Change"

```
Initial Focus (Year 1-2):
1. Public Education:
   - Explainers on key concepts (alignment, capabilities, governance)
   - Scenarios for different development paths
   - Discussions of economic implications

2. Network Building:
   - Local discussion groups
   - Online communities for specific professions
   - Connections between concerned experts

Deferred to Later:
- Skills training
- Mutual aid networks
- Local preparedness planning
```

4.3.2 Community Products (Initial)

1. Discussion Guides: For reading groups on key papers
2. Online Forums: For professional communities (doctors, teachers, etc.)
3. Event Calendar: Curated list of relevant talks, conferences
4. Resource Directory: Links to other organizations' materials

---

5. Realistic Operational Plan

5.1 Phase 1: Foundation (Months 1-6)

5.1.1 Team & Resources

```
Core Team (3 people, part-time):
- Lead Researcher/Analyst (20 hrs/week)
- Community Manager (15 hrs/week)
- Technical Editor (10 hrs/week)

Volunteers: 10-15 (research assistants, writers, community moderators)

Budget: $50,000 for first 6 months
- $30,000: Stipends for core team
- $10,000: Software, hosting, tools
- $10,000: Contingency
```

5.1.2 Month-by-Month Goals

```
Month 1-2:
- Launch weekly newsletter (200 subscribers)
- Publish first research digest
- Establish basic website and workflow

Month 3-4:
- Grow to 500 newsletter subscribers
- Launch public research database
- Conduct first expert interviews

Month 5-6:
- Reach 1,000 subscribers
- Publish first quarterly assessment
- Secure first institutional partnerships
```

5.2 Success Metrics (Phase 1)

```
Primary:
- Newsletter subscribers: 1,000+ (6 months)
- Open rate: 40%+
- Returning website visitors: 1,000/month
- Media citations: 5+ (6 months)

Secondary:
- Policy staff subscribers: 50+
- Researcher subscribers: 200+
- Partner organizations: 10+
```

5.3 Scalability Checkpoints

We only expand when:

1. Newsletter consistently hits 40%+ open rate
2. Monthly website traffic exceeds 5,000 unique visitors
3. We have 3+ recurring funding sources
4. Core team capacity is consistently maxed out

---

6. Revised 90-Day Pilot: "The AGI Progress Digest"

6.1 Realistic Scope & Deliverables

```
Goal: Prove we can provide unique value through information synthesis

Deliverables:
1. Weekly newsletter (8 issues)
2. Research database (100+ annotated papers)
3. Two expert interviews (transcribed and published)
4. One quarterly assessment report
```

6.2 Detailed Timeline

```
Week 1-4: Foundation
- Launch basic website and newsletter
- Publish first 4 newsletters (focus: recent arXiv papers)
- Build initial research database (50 papers)

Week 5-8: Quality Improvement
- Conduct and publish first expert interview
- Add policy tracking section to newsletter
- Grow to 300 subscribers

Week 9-12: Validation
- Conduct second expert interview
- Publish quarterly assessment report
- Survey readers for feedback
- Secure first 3 partner organizations
```

6.3 Budget for Pilot

```
Total: $15,000
- Stipends: $10,000 (3 people, part-time for 3 months)
- Tools/Software: $3,000
- Marketing/Promotion: $2,000
```

6.4 Pilot Success Criteria

```
Minimum Viable Success:
- 300 newsletter subscribers
- 35% open rate
- Positive feedback from 5+ domain experts
- 2+ media mentions

Stretch Goals:
- 500 subscribers
- 40% open rate
- 10+ expert endorsements
- Partnership with established research organization
```

---

7. Realistic Growth Path

7.1 Year 1: Proof of Concept

Â· Team: 3-5 core, 20-30 volunteers
Â· Budget: $100,000
Â· Focus: Newsletter quality, research synthesis
Â· Success: Become recognized source in AI policy community

7.2 Year 2: Specialization

Â· Team: 5-8 core, 50+ volunteers
Â· Budget: $300,000
Â· Focus: Add policy analysis, regional coverage
Â· Success: Regular engagement with policymakers

7.3 Year 3: Expansion

Â· Team: 10-15 core, 100+ volunteers
Â· Budget: $750,000
Â· Focus: Add advocacy support, community programs
Â· Success: Tangible influence on policy debates

7.4 Funding Strategy (Realistic)

```
Phase 1 (Months 1-12):
- Individual donors: 70%
- Small grants: 30%
- No corporate funding

Phase 2 (Year 2):
- Individual donors: 50%
- Foundation grants: 40%
- Earned income: 10%

Phase 3 (Year 3+):
- Foundation grants: 50%
- Individual donors: 30%
- Earned income: 20%
```

---

8. Risk Management

8.1 Key Risks & Mitigations

```
1. Quality Dilution (Risk: Expanding too fast)
   Mitigation: Strict quality standards, slow scaling

2. Burnout (Risk: Volunteer and staff exhaustion)
   Mitigation: Sustainable workloads, regular breaks, recognition

3. Co-option (Risk: Becoming dependent on specific funders)
   Mitigation: Diversified funding, transparency about sources

4. Obsolescence (Risk: Being overtaken by better-resourced competitors)
   Mitigation: Focus on unique value (synthesis, accessibility)

5. Political Backlash (Risk: Alienating key stakeholders)
   Mitigation: Non-partisan stance, evidence-based approach
```

8.2 Contingency Plans

Â· If growth stalls: Focus on deepening existing products rather than expanding
Â· If funding falls short: Prioritize newsletter above all else
Â· If quality suffers: Pause new initiatives, fix core products
Â· If team burns out: Implement mandatory time-off, bring in temporary help

---

9. Addressing Critical Feedback

9.1 On Scope and Feasibility

We accept that our initial proposal was over-ambitious. This revised framework:

Â· Focuses on one core competency (information synthesis)
Â· Explicitly defers advocacy and resilience work until later
Â· Sets realistic targets based on comparable organizations' growth

9.2 On Technical Monitoring Challenges

We acknowledge that:

Â· Much important information is not publicly available
Â· We cannot provide comprehensive monitoring
Â· Our value comes from curation and synthesis, not surveillance

9.3 On Theory of Change

We now articulate a clearer path:

1. Build credibility through high-quality analysis
2. Become indispensable to time-constrained decision-makers
3. Gain influence by framing issues and providing unique insights
4. Collaborate with organizations that have direct advocacy power

9.4 On Psychological Framing

We adopt more accessible language:

Â· "Navigating technological change" instead of "Post-AGI Life Skills"
Â· Focus on understanding and adaptation rather than survival
Â· Emphasize agency and opportunity alongside risks

9.5 On Funding Realism

We propose:

Â· Lower initial targets ($15k pilot, $100k Year 1)
Â· Clear value proposition for funders (unique information synthesis)
Â· Gradual scaling based on demonstrated impact

---

10. Conclusion: A Path Forward

The AGI governance challenge requires new forms of civic engagement. While the ultimate need may be comprehensive, the path to get there must be incremental and evidence-based.

AGI Watch v2.0 represents a realistic starting point: building the information infrastructure necessary for informed debate and decision-making. By proving we can provide unique value in synthesizing technical developments for broader audiences, we lay the foundation for more ambitious oversight mechanisms.

10.1 Immediate Next Steps

1. Form founding team (3 committed individuals)
2. Secure initial funding ($15,000 for 90-day pilot)
3. Launch newsletter and basic website
4. Build volunteer network (10-15 initial contributors)

10.2 Invitation to Collaborate

We seek:

Â· Researchers interested in communicating their work
Â· Policy professionals needing better information
Â· Concerned citizens with relevant skills (writing, research, community building)
Â· Funders who believe in the power of transparency

The "AI 2027" scenario reminds us that passive observation is insufficient. But effective action requires starting where we are, with what we have, and building methodically from there.

---

Appendices

Appendix A: Sample Newsletter Outline

Weekly structure, content guidelines, quality standards

Appendix B: Research Tagging System

Taxonomy for categorizing papers, policy developments

Appendix C: Volunteer Onboarding Process

Step-by-step guide for new contributors

Appendix D: Initial Budget Breakdown

Detailed pilot budget with line items

Appendix E: Comparable Organizations

Analysis of similar initiatives and their growth trajectories

---

White Paper Authors:
AGI Watch Founding Team

License: Creative Commons Attribution 4.0 International
Contact: [contact@agiwatch.org] (placeholder)
Website: [https://agiwatch.org] (placeholder)

---

This document represents our revised, realistic approach. We welcome continued feedback and collaboration.


AGI Watch Implementation Assets: From Vision to Action

1. Call for Founding Contributors

Subject: Join the Founding Team of AGI Watch: Building Civic Infrastructure for the AGI Era

The Problem We're Solving:
The"Information Asymmetry" between AGI labs and the public is the single greatest barrier to safe development. While technical progress accelerates behind closed doors, policymakers, journalists, and concerned citizens lack timely, synthesized intelligence to make informed decisions. This gap creates reactive governance, polarized public discourse, and unchecked competitive pressures.

Our Solution:
AGI Watch is launching a 90-day pilot to build the essential"middle layer" of civic intelligence. We are not just another AI newsletter. We are a civic oversight framework focused on technical synthesis for decision-makersâ€”the connective tissue between cutting-edge research and responsible governance.

Why This Matters Now:
The window for shaping AGI's trajectory is narrowing.The next 12-24 months will see unprecedented investments in compute infrastructure, algorithmic breakthroughs, and geopolitical positioning. Without a credible, independent source of synthesized intelligence, we risk sleepwalking into scenarios with catastrophic downside risks.

We Are Seeking Three Founding Contributors:

Role 1: Lead Research Analyst

The Work: You will architect and execute our core productâ€”the Weekly Research Digest. Each week, you will:

Â· Curate 5-10 of the most consequential arXiv preprints and conference papers
Â· Extract and explain the "so what" for scaling, alignment, and governance
Â· Identify emerging patterns across research streams
Â· Collaborate with our Policy Editor to connect technical developments to policy implications

You Are:

Â· Capable of reading a technical ML paper and identifying its practical implications
Â· Familiar with AI safety literature and key research directions
Â· Exceptionally clear in written communication (samples required)
Â· Able to work under deadline pressure without sacrificing rigor
Â· Comfortable with uncertainty and rapidly evolving information

Time Commitment: 15-20 hours/week for 3 months
Compensation:$5,000 stipend for the 90-day pilot

Role 2: Policy Editor

The Work: You will bridge the technical and policy worlds by:

Â· Tracking legislative hearings, regulatory proposals, and corporate policy announcements
Â· Analyzing compute governance trends and geopolitical developments
Â· Writing the "Policy Implication" sections of our Weekly Digest
Â· Building relationships with policy staffers to understand their information needs

You Are:

Â· Experienced in policy analysis or tech policy work
Â· Familiar with AI governance landscape (EU AI Act, U.S. Executive Orders, etc.)
Â· Able to translate complex technical concepts into actionable policy language
Â· Networked in policy circles (government, think tanks, NGOs)
Â· A strategic thinker who can anticipate regulatory gaps

Time Commitment: 12-15 hours/week for 3 months
Compensation:$4,000 stipend for the 90-day pilot

Role 3: Community & Operations Lead

The Work: You will build the foundation for our distributed network by:

Â· Recruiting and onboarding our initial 10-15 volunteer contributors
Â· Managing the $15,000 pilot budget and operational workflow
Â· Building our initial community platform (Discord/Forum)
Â· Establishing quality control processes and contributor guidelines
Â· Tracking metrics and preparing our Phase 2 funding proposal

You Are:

Â· Experienced in community building and volunteer management
Â· Organized and systematic in operational execution
Â· Comfortable with basic financial tracking and reporting
Â· A natural facilitator who can coordinate diverse contributors
Â· Passionate about creating inclusive, sustainable systems

Time Commitment: 10-15 hours/week for 3 months
Compensation:$4,000 stipend for the 90-day pilot

What We Offer Beyond Stipends:

Â· Founding equity in AGI Watch's governance structure
Â· Professional visibility as pioneers in a critical new field
Â· Network access to leading researchers, policymakers, and funders
Â· Skill development in technical synthesis and civic entrepreneurship
Â· Impact trajectory to shape one of history's most important transitions

Our First 90 Days Will Deliver:

1. Weekly AGI Progress Digest (12 issues)
2. Research database (100+ annotated papers with policy implications)
3. Expert interview series (6 conversations with leading researchers)
4. Volunteer network (15+ trained contributors)
5. Phase 2 funding proposal and growth plan

To Apply:

Send the following to founders@agiwatch.org by [Date, 7 days from now]:

1. Which role you're applying for and why
2. One-page summary of your relevant experience
3. One work sample that demonstrates your ability for this role:
   Â· For Lead Research Analyst: A 300-word explanation of a recent arXiv paper for a non-technical audience
   Â· For Policy Editor: A policy memo connecting a technical development to regulatory implications
   Â· For Community Lead: A community building plan or operational workflow you've designed
4. Availability for a 30-minute interview between [Date range]

Timeline:

Â· Applications close: [Date]
Â· Interviews: [Dates]
Â· Selection notified: [Date]
Â· Pilot launch: [Date, 14 days from now]

---

2. Sample Executive Brief: AGI Progress Digest #001

AGI Watch: The Progress Digest
Issue #001 | March 15, 2024 | Focus: The Compute-Intelligence Ratio Shift

EXECUTIVE SUMMARY (2-Minute Read)

This week's data indicates a strategic pivot from model architecture to infrastructure-led scaling. While public attention remains focused on chatbot capabilities, the critical signal is the 30% increase in data center construction permits in the Southwest U.S. power corridorâ€”a high-confidence indicator of upcoming training runs exceeding 10Â²â¸ FLOP.

Bottom Line for Decision-Makers: Current policy frameworks tracking compute via FLOP thresholds are becoming obsolete. New efficiency breakthroughs allow 2x the effective intelligence per hardware unit, bypassing proposed oversight triggers. We recommend shifting regulatory focus from "hardware used" to "measured capability benchmarks."

---

TECHNICAL SYNTHESIS: WHAT MATTERS THIS WEEK

ðŸ“„ Paper of the Week: "Sparse Activation Pathways" (arXiv:2403.xxxxx)

Â· The Gist: New training method reduces compute requirements by 40% while maintaining performance.
Â· The 'So What': This effectively doubles the intelligence yield per dollar of compute investment.
Â· Confidence: High (reproduced across 3 model scales, peer-reviewed methodology)
Â· Implication: Companies can now train models with capabilities that would previously have triggered compute-based oversight, without exceeding hardware thresholds.

ðŸ“ˆ Capability Index Update

Our proprietary AGI Progress Index rose 2.1 points this month (now at 34.7/100), driven by:

Â· +1.5 points: Advances in autonomous agents' long-horizon planning
Â· +0.4 points: Improvements in scientific reasoning benchmarks
Â· +0.2 points: Code generation for novel architectures

Note: The Capability Index weights advances by their implications for recursive self-improvement potential.

ðŸ” Pattern Recognition

We're observing convergence across three research streams:

1. Efficiency algorithms (like Sparse Activation)
2. Data curation automation (synthetic data pipelines)
3. Automated alignment research (AI-assisted oversight)

This suggests labs are preparing for continuous training regimes rather than discrete model releases.

---

POLICY IMPLICATIONS FOR STAFFERS

Immediate Action Items:

1. Update the "Model Audit Act" draft: Shift trigger from ">10Â²â· FLOP" to ">80th percentile on standard capability benchmarks."
2. Request briefing from NIST: Ask for timeline on updating AI risk management framework to account for efficiency breakthroughs.
3. Schedule hearing: "Oversight in an Era of Exponential Efficiency Gains" with testimonies from compute economists and safety researchers.

Questions for Agency Heads:

Â· "How is your department tracking effective capability rather than raw compute?"
Â· "What mechanisms exist for updating oversight thresholds quarterly rather than annually?"
Â· "How are we preparing for models that improve continuously rather than being released discretely?"

---

GOVERNANCE TRACKER

ðŸŒ International Developments

Â· EU: AI Office considering "dynamic threshold" approach for GPAI oversight
Â· China: State Council announces "ç®—æ³•çªç ´è¡¥è´´" (algorithm breakthrough subsidy) for efficiency research
Â· UK: AISI piloting "capability certificates" for model deployment

ðŸ›ï¸ U.S. Legislative Pipeline

Â· HR 4567 (Compute Transparency Act): Markup scheduled for April 5
Â· S 2345 (AI Safety Consortium Act): Added efficiency monitoring amendment
Â· DOD Appropriations: +$200M for "security of continuous learning systems"

---

RESEARCHER SPOTLIGHT: DR. ALEX CHEN

Affiliation: Stanford Center for AI Safety
Current Work: Developing "capability audits" that measure potential for self-improvement
Quote: "We're moving from counting FLOPs to measuring gradients of improvement. The slope matters more than the position."

To interview Dr. Chen for your committee: Reply to this email with "Chen briefing request"

---

DATA CORNER

Compute Infrastructure Signals:

Â· Power Purchase Agreements (PPAs): +18% YoY for AI datacenters
Â· GPU Lead Times: Extended from 8 to 14 weeks (indicator of demand surge)
Â· Water Usage Permits: 3 new applications >50M gallons/year for cooling

Hiring Patterns:

Â· Top Trend: "Continuous Learning Systems Engineer" (new role at 5 major labs)
Â· Skills Demand: Reinforcement learning from human feedback (+220% postings)
Â· Geography: 40% of new AI roles outside traditional hubs (cost/power optimization)

---

LOOKING AHEAD: NEXT 90 DAYS

What We're Monitoring:

1. April conferences: ICLR workshop papers on "training compute optimality"
2. Q1 earnings calls: Watch for "efficiency metrics" in lab announcements
3. Spring regulatory deadlines: EU AI Act delegated acts on continuous learning

Critical Unknown: Whether efficiency gains will be open-sourced or kept proprietary. This determines democratization vs. concentration of capability gains.

---

HOW TO USE THIS BRIEF

For Staffers:

Â· Forward the "Policy Implications" section to your legislative director
Â· Use the "Questions for Agency Heads" in upcoming hearings
Â· Request member-level briefings on specific topics by replying to this email

For Researchers:

Â· Contribute to our research database at [GitHub link]
Â· Suggest papers for next week's digest: [submission form]
Â· Join our expert network for policy consultations: [signup form]

For Journalists:

Â· All data is citable with attribution to AGI Watch
Â· Expert interviews available on topics covered
Â· Request custom briefings for specific stories

---

About This Digest: AGI Watch is a nonpartisan civic initiative providing synthesized intelligence on AGI development. Our team includes researchers, policy analysts, and former regulators. We accept no funding from AI labs or government agencies with oversight roles.

Subscribe: [Link] | Unsubscribe: [Link] | Provide Feedback: [Link] | Request Expert: [Link]

---

3. The "Staffer-First" Feedback Loop: Pilot Strategy

Design Philosophy

We will treat our first 100 subscribers not as an audience, but as co-developers. Specifically, we will recruit 20-30 policy staffers and regulators as our "alpha testers" who receive:

1. Direct line to editors for topic requests
2. Weekly feedback surveys on utility
3. Invitation to monthly "editorial roundtable" calls
4. Early access to database tools

Implementation Table

Feature Purpose Implementation (Weeks 1-12) Success Metric
"The Briefing Room" High-utility, low-noise entry point 500-word staffer summary at top; never exceeds 2-minute read time 70%open rate from .gov/.org emails
Expert Matchmaking Build Pillar 2 (Advocacy) foundation "Request an Expert" button â†’ 15-minute calendly slots with our network 10+ expert-staffer connections made
Capability Index Create the "standard" metric for progress Weekly update visualizing 5 domains; methodology transparent on GitHub Cited in 3+ policy documents
Policy Implication Sidebars Bridge technical-policy gap Every paper summary includes "For regulators:" section Staffer survey: >8/10 usefulness
Legislative Tracking Contextualize within current debates Bill tracker with our analysis of how developments affect legislation Used in 2+ hearing preparations
Researcher Directory Build academic-policy pipeline Searchable database of 50+ experts willing to brief staffers 20+ expert profiles viewed/week
Feedback Widget Continuous iteration Inline ratings for each section: "Useful for your work?" 30% response rate; implement 2 top requests/month

Week-by-Week Staffer Engagement Plan

Weeks 1-4: Onboarding & Baseline

Â· Personalized welcome emails to .gov/.org subscribers
Â· Initial survey: "What information gaps hinder your work?"
Â· First editorial roundtable: "What should we be tracking that we're missing?"

Weeks 5-8: Integration & Refinement

Â· Custom alerts for subscribers' specific committees/jurisdictions
Â· Draft language provided for legislation/regulations
Â· Second survey: "What specific decision did this inform?"

Weeks 9-12: Institutionalization & Transition

Â· Case studies of how briefs were used in actual policymaking
Â· Training session: "How to read AI research for policy implications"
Â· Phase 2 proposal co-created with top 10 most engaged staffers

Metrics That Matter for Phase 2 Funding

Metric Category Specific Measures Target (90 Days)
Engagement .gov/.org open rate >45% Average read time >3.5 minutes 20+ staffers in regular contact 50% 4 minutes 25 staffers
Utility Staffer survey: "Helps me do my job" >8/10 5+ examples of briefs used in hearings/memos 3+ regulatory citations 8.5/10 8 examples 5 citations
Network 50+ experts in directory 30+ volunteer contributors trained 10+ partner organizations referring subscribers 75 experts 40 contributors 15 partners
Quality Technical accuracy score (peer review) >95% Update-to-correction ratio <1% Methodology transparency score 10/10 98% 0.5% 10/10

Staffer Testimonial Strategy

We will document impact through:

1. Anonymous case studies: "How Staffer X used our brief to question a witness"
2. Quotable feedback: "This is the only thing that helps me prepare for AI hearings"
3. Before/after examples: Showing how our synthesis changed policy language

Transition to Phase 2

If we hit 80% of our targets, we will:

1. Launch premium tier: $500/month for agencies needing custom briefings
2. Expand team: Hire first full-time analyst
3. Build advisory board: Including former regulators and respected researchers
4. Publish "State of AGI Governance" report: Comprehensive analysis with policy recommendations

---

Your Next Step

If you're ready to build this with us:

1. Share these assets with 3 people who should join the founding team
2. Identify 5 policy staffers who would benefit from the pilot digest
3. Reply with which implementation aspect you want to own first

If you need modifications before proceeding:

1. Specify which element needs adjustment
2. Provide concrete alternative language or structure
3. Set a deadline for revised version

Time is our scarcest resource. The 90-day pilot window begins when we have:
âœ“3 founding contributors committed
âœ“ $15,000 in initial funding secured
âœ“ 50 policy staffers pre-registered for the digest

Which will you help secure first?

---

These implementation assets transform our White Paper from a document into a movement. The machinery is designed, the value proposition is clear, and the need is urgent. The only remaining question is who will build it.