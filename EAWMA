### EAWMA: A Proposed Embodied Agentic World Model Architecture  
**A Framework for Next-Generation Autonomous Intelligence**

**Authors:** Ouadi Maakoul (with AI-assisted contributions acknowledged)  
**Date:** December 22, 2025  

---

### Abstract

This paper proposes the **Embodied Agentic World Model Architecture (EAWMA)**, a conceptual framework that integrates multimodal perception, predictive world modeling, hierarchical planning, episodic memory, and neuro-symbolic reasoning into a Bayesian inference-based system for autonomous intelligence. EAWMA aims to address limitations in current large language models (LLMs), such as hallucinations and lack of grounding, by incorporating grounded simulation and symbolic constraints. Drawing on recent advances like joint-embedding predictive architectures and Gaussian representations, we outline the mathematical foundations, architecture, and proposed research directions. Potential applications include robotic collaboration and ethical AI, though empirical validation remains future work. This is a speculative proposal emphasizing integration ideas for further exploration.

**Keywords:** Embodied AI, World Models, Neuro-Symbolic AI, Hierarchical Planning, Mental World Models, Gaussian Splatting, Joint-Embedding Predictive Architectures, Theory of Mind

---

### 1. Introduction

Large Language Models (LLMs) advance language processing but struggle with grounded causal reasoning and physical interactions [1, 2]. Recent surveys position world models as key for embodied AI, enabling prediction and decision-making in dynamic environments [3]. EAWMA proposes to mitigate LLM shortcomings—like hallucinations, brittle multi-step reasoning, and opacity—through integrated neural-symbolic simulation [4].

The proposed core components include:
- Multimodal perception
- World simulation
- Hierarchical planning
- Episodic memory
- Neuro-symbolic reasoning

EAWMA explores a mental world model for theory-of-mind (ToM) reasoning, potentially inferring user intentions [5]. This could enhance human-AI collaboration, though speculative.

Contributions:
1. Unified Bayesian formalism for perception-action cycles
2. Modular architecture proposal with mental world modeling
3. Suggested research directions with benchmarks

EAWMA seeks to extend frameworks like Google DeepMind's Gato/RT-2 (multimodal but lacking symbolic constraints) [6], PaLM-E (grounded LLMs without hierarchical simulation) [7], and Wayve's driving models (sim-to-real but opaque) [17] by emphasizing explainable integration—hypothetical without results.

---

### 2. Related Work

EAWMA draws from paradigms, proposing extensions:

| Approach              | Key Strengths                       | Limitations                          | Proposed Relation to EAWMA          |
|-----------------------|-------------------------------------|--------------------------------------|-------------------------------------|
| LLMs [1, 2]           | Language, recall                    | No grounding, hallucinations         | Optional interface                  |
| World Models [9]      | Simulation, prediction              | Limited actions/symbols              | Core dynamics (e.g., V-JEPA 2 [10]) |
| RL [11]               | Policy optimization                 | Sample-inefficient                   | Latent planning                     |
| Neuro-Symbolic AI [4] | Logic, explainability               | Weak perception/meta-cognition       | Constraint layer                    |
| Embodied AI [12]      | Grounded learning                   | Task-specific                        | General loop                        |
| Agentic Systems [13]  | Decomposition, tools                | No physical grounding                | Hierarchical goals                   |
| Embodied LLMs [7]     | Multimodal tasks                    | Unpredictable environments           | Adds robust planning                |
| Gato/RT-2 [6]         | Multimodal policies                 | Lacks symbolic constraints           | Proposes neuro-symbolic integration |
| PaLM-E [7]            | Grounded LLMs                       | No hierarchical simulation           | Adds world modeling                 |
| Wayve Driving [17]    | Sim-to-real generalization          | Opaque decision-making               | Aims for explainable simulation     |

Surveys formalize embodied world models in POMDPs with taxonomies [3]. Neuro-symbolic reviews highlight meta-cognition gaps EAWMA explores [4].

---

### 3. Mathematical Foundations

#### 3.1 Notation

Agent at time \( t \): observation \( o_t \in \mathcal{O} \), action \( a_t \in \mathcal{A} \), latent state \( z_t \), belief \( b_t = P(s_t \mid o_{1:t}, a_{1:t-1}) \).

#### 3.2 Perception

Encoder \( q_\phi(z_t \mid o_t) \) yields object-centric latents. ELBO with Chamfer Distance [16]:

\[
\mathcal{L}_{\text{perc}} = \mathbb{E}[\log p(o_t \mid z_t)] - \beta D_{\text{KL}}(q_\phi \parallel p(z_t)) + \lambda \cdot \text{CD}(S_1, S_2)
\]

#### 3.3 World Model

Action-conditioned dynamics (inspired by V-JEPA 2 [10]):

\[
h_{t+1} = f_\psi(h_t, z_t, a_t, \text{context}), \quad z_{t+1} \sim \mathcal{N}(\mu_\psi(h_{t+1}), \Sigma_\psi(h_{t+1}))
\]

#### 3.4 Planning

Hypertree MCTS with self-reflection [15].

#### 3.5 Memory

Product-quantized episodic retrieval.

#### 3.6 Neuro-Symbolic Constraints

Constrained optimization with Logical Credal Networks [4].

#### 3.7 Mental World Model

Extend belief to inferred states: \( b_t^m = P(s_t^j \mid o_{1:t}, a_{1:t-1}, z_t) \) for agent \( j \). Use inverse planning [5]:

\[
P(a_t^j \mid g^j) \propto P(g^j \mid z_{t:t+H}) \cdot P(z_{t:t+H} \mid z_t, a_{t:t+H}^j)
\]

Approximate via particle filtering (O(N agents * d)). Fuse loss: \( \mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{perc}} + \beta \mathcal{L}_{\text{dyn}} + \gamma \mathcal{L}_{\text{ToM}} \).

**Example: "Assist User in Fetching Water Without Spilling"**

Robot observes user reach (o_t). Perception: z_t = {z_t^{glass}, z_t^{user}}. Mental model: b_t^m ≈ P(thirst | trajectory). World model: z_{t+1} ~ p_ψ(z_{t+1} | z_t, a_t = "grasp"), using Gaussians [16]. Planning: Hypertree explores paths. Constraints: distance > threshold. Action: Safe pour sequence.

---

### 4. Proposed Architecture

Figure 1 (below) illustrates the overall dataflow: Multimodal observations feed perception, which updates the belief state. Parallel world and mental models provide predictions and intent inference. The hypertree planner generates candidates, filtered by neuro-symbolic constraints. Episodic memory informs retrieval across components.

**Figure 1: EAWMA Architecture Dataflow**

(Description of diagram: Boxes for Perception, Belief State, World Model (parallel to Mental World Model), Hypertree Planner, Neuro-Symbolic Reasoner, Action Output. Arrows: Observations → Perception → Belief; Memory retrieval → Belief/Mental; Goal/Constraints → Planner; Rollouts → Reasoner → Action; Experience storage loop.)

Mental world model parallels simulation for ToM personalization [5].

Specifications:
- **Perception** — Slot attention + Gaussian Splatting [16]
- **World Simulation** — V-JEPA 2-inspired [10]
- **Planning** — Hypertree MCTS [15]
- **Memory** — Differentiable dictionary
- **Reasoner** — Fuzzy logic + differential privacy [4]
- **Training** — Staged pretraining on datasets like OpenDV-2K, joint fine-tuning with fused loss.

Feasibility: O(TBd²) challenging for real-time; sparse approximations help.

---

### 5. Implementation Details

#### 5.1 Pseudo-Code

```python
class MentalWorldModel:
    def predict_intent(self, obs, context):
        # Sample candidate goals from prior/context
        goals = self.sample_goals(context, k=10)  # e.g., thirst, fatigue
        # Likelihood via forward simulation
        likelihoods = [self.world_model.goal_likelihood(g, obs) for g in goals]
        return goals[argmax(likelihoods)]  # Approximate inverse planning

class EAWMAAgent:
    def __init__(self, config):
        self.perception = PerceptionModule(config)
        self.world_model = WorldModel(config)
        self.mental_model = MentalWorldModel(config)
        self.planner = HypertreePlanner(config)
        self.memory = EpisodicMemory(config)
        self.reasoner = NeuroSymbolicReasoner(config)

    def update_belief(self, observation):
        latent = self.perception.encode(observation)
        context = self.memory.retrieve(latent, k=3)
        intent = self.mental_model.predict_intent(observation, context)
        self.belief_state = self.world_model.fuse(latent, context, intent)
        return self.belief_state

    def plan_action(self, goal, constraints):
        candidates = self.planner.hypertree_mcts(root=self.belief_state, goal=goal, simulations=500)
        feasible = []
        for plan in candidates:
            trajectory = self.world_model.rollout(self.belief_state, plan.actions)
            if self.reasoner.evaluate(trajectory, constraints) > 0.9:
                feasible.append(plan)
        return max(feasible, key=lambda p: p.expected_reward) if feasible else fallback_safe_action()

    def execute_step(self, observation, goal, constraints):
        belief = self.update_belief(observation)
        plan = self.plan_action(goal, constraints)
        action = plan.next_action()
        self.memory.store({'belief': belief, 'action': action, 'intent': intent})
        return action
```

#### 5.2 Computational Complexity

| Component     | Training                  | Inference               | Memory   | Notes                          |
|---------------|---------------------------|-------------------------|----------|--------------------------------|
| Perception    | O(BDHW)                   | O(BCHW)                 | O(BC)    |                                |
| World Model   | O(TBd²)                   | O(Bd²)                  | O(d²)    | Approximations for real-time   |
| Planning      | O(NSd)                    | O(Sd)                   | O(S)     | Hypertree reduces branches     |
| Memory        | O(NlogN)                  | O(logN)                 | O(N)     |                                |
| Reasoner      | O(C·L)                    | O(C·L)                  | O(C·L)   |                                |
| Mental Model  | O(k * rollout)            | O(k * d)                | O(d)     | k=10 samples; particle filter  |

---

### 6. Proposed Research Directions

1. Simulation Validation → Test in MiniGrid/Crafter against DreamerV3 [9].
2. Embodiment → Deploy on UR5/Spot, evaluate sim-to-real [17].
3. Scalable Learning → Explore continual/multi-agent.
4. General Evaluation → Use CausalVQA for ToM [18].

---

### 7. Expected Impacts and Limitations

EAWMA could support robotics/healthcare, aligning with embodied initiatives [19].

**Limitations**:
- Computational costs: ToM explodes without approximations.
- Failure modes: Intent misattribution risks unsafe actions; brittleness in novel settings.
- Speculation: No validation; joint training untested.
- Scope: Integration-focused; needs data/ethics handling.

Future: Scale-down implementation on one task.

---

### 8. Conclusion

EAWMA proposes a grounded framework integrating recent advances. As conceptual, it requires validation.

---

### 9. References

[1] Brown et al., NeurIPS 2020.  
[2] OpenAI, arXiv:2303.08774.  
[3] Li et al., arXiv:2510.16732, 2025.  
[4] Colelough & Regli, arXiv:2501.05435, 2025.  
[5] Shi et al., arXiv:2408.12574, 2025.  
[6] Reed et al. (Gato), arXiv:2205.06175; Driess et al. (RT-2), arXiv:2307.15818.  
[7] Driess et al., arXiv:2303.03378.  
[9] Hafner et al., ICLR 2020.  
[10] Assran et al., arXiv:2506.09985, 2025.  
[11] Sutton & Barto, MIT Press 2018.  
[12] Gupta et al., CVPR 2017.  
[13] Xi et al., arXiv:2309.07864.  
[15] Gui et al., arXiv:2505.02322, 2025.  
[16] Lu et al., arXiv:2506.19842 (ManiGaussian++), 2025; earlier ManiGaussian ECCV 2024.  
[17] Wayve.ai reports, 2025.  
[18] CausalVQA benchmarks.  
[19] Singer & Zvenyhorodskyi, Carnegie Endowment, 2025.

### Figure 1: EAWMA Architecture Dataflow

Detailed textual representation (ASCII art) of the EAWMA architecture, illustrating the perception-action cycle with parallel world and mental modeling, memory integration, and constrained planning:

```
                          +-------------------------+
                          | Multimodal Observations |
                          +-------------------------+
                                       |
                                       v
                          +-------------------------+
                          |     Perception Module   |
                          | (Gaussian Splatting,    |
                          |  Slot Attention)        |
                          +-------------------------+
                                       |
                                       v
                          +-------------------------+     +---------------------+
                          |       Belief State      |<----+   Episodic Memory   |
                          |     (Bayesian Fuse)     |     | (Retrieval & Store) |
                          +-------------------------+     +---------------------+
                                       |                           ^
                                       |                           |
                  +--------------------+--------------------+      |
                  |                                       |      |
                  v                                       v      |
        +-----------------+                     +---------------------+
        |  World Model    |                     |  Mental World Model |
        | (V-JEPA 2-style |                     | (ToM Inverse        |
        |  Dynamics)      |                     |  Planning, k=10)    |
        +-----------------+                     +---------------------+
                  |                                       |
                  +-------------------+--------------------+
                                      |
                                      v
                          +-------------------------+
                          |    Hypertree Planner    |
                          | (MCTS with Self-Reflection)|
                          +-------------------------+
                                       ^
                                       |
                               +-------+-----------------+
                               |                         |
                       Goal / Constraints        (Candidate Rollouts)
                                       |
                                       v
                          +-------------------------+
                          | Neuro-Symbolic Reasoner |
                          | (Logical Credal Nets,   |
                          |  Fuzzy Constraints)     |
                          +-------------------------+
                                       |
                                       v
                          +-------------------------+
                          |       Action Output     |
                          +-------------------------+
                                       |
                                       v
                          +-------------------------+
                          |    Experience Storage   |
                          +-------------------------+
                                       |
                                       +-------------------> (Loop back to Memory)
```

**Description:**
- **Central Loop:** Observations flow through perception to update the belief state, informed by episodic memory retrieval.
- **Parallel Prediction:** Belief feeds both the World Model (physical dynamics) and Mental World Model (intent/ToM inference).
- **Planning & Constraints:** Hypertree planner generates candidates using predictions; neuro-symbolic reasoner filters for safety/logic.
- **Feedback:** Actions produce experiences stored in memory for lifelong adaptation.

This diagram captures the modular, closed-loop design proposed in Section 4. It highlights the Bayesian fusion at the belief state and the role of memory in personalization.


### EAWMA MVP: Final Minimal Viable Prototype Specification  
**A Concrete, Executable Plan for Preliminary Validation**

**Authors:** Ouadi Maakoul (with AI-assisted contributions acknowledged)  
**Date:** December 22, 2025  
**Target:** Transform EAWMA from conceptual proposal to empirically validated framework via a lightweight prototype in simulation.

---

### MVP Scope (Focused & Feasible)
**Core Claim to Validate:** Integrating a mental world model (ToM intent inference) with predictive simulation and constrained planning improves safe, adaptive behavior.

**3 Core Modules Implemented:**
1. **Perception + World Model** (predictive dynamics)
2. **Mental World Model** (inverse planning for intent inference)
3. **Constrained Hypertree Planner** (MCTS with safety rules)

**Omitted for MVP:** Full episodic memory, advanced neuro-symbolic (use hard constraints), real-robot deployment, Gaussian splatting, large-scale training.

**Task:** "Assistive Fetching in a 2D Grid World"  
- Environment: 10x10 grid (MiniGrid-based)
- Entities: Robot (agent), simulated human, water glass, rest area, exit, obstacles, spill hazard zone
- Human Goals: "drink" (move toward water), "rest" (move toward rest area), "leave" (move toward exit)
- Robot Goal: Infer human intent → safely fetch/deliver object or assist without entering spill zone

---

### Environment & Data Generation

**Custom MiniGrid Environment (`assistive_fetch_env.py`)**  
- Partial observability (ego-centric view)
- Human simulated with ε-optimal policy (80% optimal actions toward goal + 20% noise)
- Spill zone: Fixed 2x2 area near delivery point

**Data Generation Strategy**
```python
# Generate 10,000 training episodes
episodes = []
for _ in range(10000):
    goal = np.random.choice(["drink", "rest", "leave"], p=[0.4, 0.4, 0.2])
    human_traj = simulate_human(start_pos, goal, noise=0.2, length=15)
    episodes.append({"traj": human_traj, "goal": goal, "grid": grid_state})

# Test set: 1,000 unseen episodes with edge cases (ambiguous paths, obstacles)
```

---

### Module Architectures (Concrete & Implementable)

#### 1. World Model (`world_model.py`)
```python
class WorldPredictor(nn.Module):
    def __init__(self, grid_size=10, latent_dim=128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # 3 channels: robot, human, objects
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * grid_size * grid_size, latent_dim)
        )
        self.dynamics = nn.GRUCell(latent_dim + 5, latent_dim)  # +5 one-hot actions
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64 * grid_size * grid_size),
            nn.Unflatten(1, (64, grid_size, grid_size)),
            nn.ConvTranspose2d(64, 32, 3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 3, padding=1),
            nn.Sigmoid()
        )
    
    def forward(self, state, action, hidden):
        z = self.encoder(state)
        z_next = self.dynamics(torch.cat([z, action], dim=-1), hidden)
        pred_state = self.decoder(z_next)
        return pred_state, z_next
```
**Training:** MSE reconstruction + prediction loss on next state (50 epochs, batch=64)

#### 2. Mental World Model (`mental_model.py`)
```python
class IntentInferer:
    def __init__(self, goals, world_model, env, horizon=8):
        self.goals = goals  # ["drink", "rest", "leave"]
        self.world_model = world_model
        self.env = env
        self.horizon = horizon
    
    def goal_likelihood(self, human_traj, goal):
        goal_pos = self.env.goal_positions[goal]
        # Simulate optimal path using A* + world model rollout
        optimal_actions = astar_path(human_traj[0], goal_pos, self.env)
        predicted_states = self.world_model.rollout(human_traj[0], optimal_actions[:self.horizon])
        
        # Negative distance as likelihood
        distances = [np.linalg.norm(actual - pred) 
                     for actual, pred in zip(human_traj[-self.horizon:], predicted_states)]
        return np.exp(-np.mean(distances))
    
    def predict(self, human_traj):
        likelihoods = [self.goal_likelihood(human_traj, g) for g in self.goals]
        return self.goals[np.argmax(likelihoods)]
```

#### 3. Constrained Planner (`planner.py`)
```python
class ConstrainedMCTS:
    def __init__(self, simulations=300, depth=10):
        self.simulations = simulations
        self.depth = depth
    
    def search(self, state, inferred_goal):
        root = Node(state)
        for _ in range(self.simulations):
            node = self.select(root)
            if not self.is_terminal(node):
                node = self.expand(node)
            reward = self.simulate(node, inferred_goal)
            self.backprop(node, reward)
        
        valid_children = [c for c in root.children if self.is_safe(c.trajectory)]
        return max(valid_children, key=lambda c: c.value) if valid_children else safe_fallback()
    
    def is_safe(self, traj):
        return not any(pos in spill_zone for pos in traj)
```

---

### Success Metrics (Rigorous & Achievable)

| Metric                     | Random Baseline | Frequency Baseline | MCTS (no mental) | EAWMA MVP Target |
|----------------------------|-----------------|---------------------|------------------|------------------|
| Intent Accuracy            | 33%             | 45%                 | N/A              | ≥55%             |
| Task Success Rate          | ~30%            | ~40%                | ~50%             | ≥65%             |
| Constraint Violations      | N/A             | N/A                 | ~15%             | 0%               |
| Near-Misses (within 1 cell) | N/A             | N/A                 | N/A              | Track & report   |

**Statistical Significance:** Run 5 seeds, report mean ± std, p<0.05 vs. best baseline

---

### Expected Failure Modes & Mitigation

| Failure Mode                  | Symptom                        | Debug Strategy                  | Mitigation                     |
|-------------------------------|--------------------------------|---------------------------------|--------------------------------|
| Ambiguous trajectories        | Oscillating intent             | Log likelihoods per step        | Temporal smoothing             |
| Poor world model prediction   | MSE >0.15                      | Visualize predictions           | Increase capacity/data         |
| No safe plans                 | Agent freezes                  | Check constraint radius         | Relax then tighten             |
| Integration disconnect        | High intent acc, low success   | Trace goal→action mapping       | Reward shaping                 |

---

### Computational Requirements
- **Hardware:** Laptop CPU sufficient (M1/M2 or modern Intel); GPU optional for faster training
- **Runtime:** 
  - Data gen: ~1-2 hours
  - World model training: ~4-6 hours
  - Full eval (1,000 eps): ~30-60 min
  - Total pipeline: <12 hours
- **Budget:** $0-$50 (Colab Pro if needed)

---

### Visualization Strategy
1. **Intent Inference Videos**: 10 episodes (5 success/5 failure) with overlay (predicted vs. true goal)
2. **Planning Trees**: Visualize selected vs. rejected paths (constraint violations marked red)
3. **Learning Curves**: Intent accuracy & success rate vs. episodes (5 seeds)
4. **Failure Heatmaps**: Grid showing where mispredictions occur

---

### Realistic Timeline (12 Weeks with Buffers)

| Phase                  | Weeks | Key Deliverables                  | Buffer |
|------------------------|-------|-----------------------------------|--------|
| Infrastructure         | 1-2   | Custom env, data pipeline         | +3 days|
| World Model            | 3-5   | Trained predictor (MSE <0.12)     | +5 days|
| Mental Model           | 6-8   | >55% intent accuracy              | +7 days|
| Integration & Planning | 9-11  | End-to-end agent, ablations       | +7 days|
| Analysis & Viz         | 12    | Plots, videos, results write-up   | +3 days|

---

### Repository Structure
```
eawma-mvp/
├── env/
│   ├── assistive_fetch_env.py
│   └── human_simulator.py
├── models/
│   ├── world_model.py
│   ├── mental_model.py
│   └── planner.py
├── data/
│   └── generated_episodes.pkl
├── training/
│   ├── train_world.py
│   └── train_full.py
├── evaluation/
│   ├── evaluate.py
│   └── visualize.py
├── configs/
│   └── mvp_config.yaml
├── tests/
│   └── test_components.py
├── requirements.txt
└── README.md (with setup + demo video)
```

---

### Success Criteria for Publication
**Minimum (Workshop Paper):**  
- Intent accuracy ≥55% (> frequency baseline)  
- Task success ≥65% (> MCTS baseline)  
- 0% violations + clear visualizations

**Stretch (Conference Paper):**  
- >65% intent, >75% success  
- Full ablation study  
- Failure analysis

### EAWMA MVP Update: Integrating Neuro-Symbolic Reasoner & Mental Model Constraints

this is an **outstanding refinement**. It directly addresses the critical gap in the MVP: how to meaningfully translate the **Neuro-Symbolic Reasoner** and **Mental World Model** into discrete, verifiable behavior in the grid world without overcomplicating the prototype.

This approach is perfect for an MVP:
- Keeps everything lightweight and debuggable
- Introduces **true symbolic reasoning** (hard veto + fuzzy soft penalties)
- Enables **lexicographic safety** (hard constraints non-negotiable)
- Directly supports **ablation studies** for publication
- Fully aligns with EAWMA’s core claim: **neural prediction + symbolic verification = safer, more adaptive agents**

Here is the **final integrated update** to the MVP specification incorporating your proposal.

---

### Updated NeuroSymbolicReasoner (`models/neurosymbolic_reasoner.py`)

```python
import numpy as np

class NeuroSymbolicReasoner:
    def __init__(self, hazard_zones, social_dist=(1, 3), stability_thresh=2):
        self.hazards = set(hazard_zones)  # e.g., {(6,7), (6,8), (7,7), (7,8)}
        self.min_social, self.max_social = social_dist
        self.stability_thresh = stability_thresh  # Max allowed velocity change

    def evaluate_trajectory(self, trajectory, human_positions, prev_action=None):
        """
        Evaluate full trajectory for safety and appropriateness.
        Returns score in [0, 1], where 0.0 = vetoed (unsafe)
        """
        score = 1.0
        prev_pos = trajectory[0]

        for i, pos in enumerate(trajectory):
            pos = tuple(pos)
            human_pos = human_positions[min(i, len(human_positions)-1)]

            # 1. Hard Constraint: Physical Safety (Hazard Zones)
            if pos in self.hazards:
                return 0.0  # Immediate veto

            # 2. Soft Constraint: Social Proximity (Fuzzy Gaussian-like)
            d = np.linalg.norm(np.array(pos) - np.array(human_pos))
            if d < self.min_social:
                score *= 0.5   # Too close → crowding penalty
            elif d > self.max_social:
                score *= 0.8   # Too far → reduced assistance

            # 3. Soft Constraint: Stability (no sudden moves near fragile object)
            if i > 0:
                velocity_change = np.linalg.norm(np.array(pos) - np.array(prev_pos))
                if 'glass' in self.env.objects and velocity_change > self.stability_thresh:
                    score *= 0.7

            prev_pos = pos

        return score

    def check_logic_consistency(self, action, inferred_intent, robot_pos, goal_locations):
        """
        Symbolic logic checks based on inferred intent
        """
        next_pos = self.compute_next_pos(robot_pos, action)

        if inferred_intent == "rest":
            rest_area = goal_locations["rest"]
            if self.is_blocking_path(next_pos, rest_area):
                return False
        elif inferred_intent == "drink":
            water_area = goal_locations["water"]
            if self.is_too_far(next_pos, water_area):
                return False
        elif inferred_intent == "leave":
            exit_area = goal_locations["exit"]
            if self.is_blocking_path(next_pos, exit_area):
                return False

        return True

    # Helper methods (implement simply)
    def is_blocking_path(self, pos, target_area):
        # Simple: if robot is adjacent to target and not moving away
        return np.linalg.norm(np.array(pos) - np.array(target_area)) <= 1

    def is_too_far(self, pos, target):
        return np.linalg.norm(np.array(pos) - np.array(target)) > 5
```

---

### Updated Constrained Hypertree MCTS (`models/planner.py`)

```python
class ConstrainedHypertreeMCTS:
    def __init__(self, reasoner, simulations=300, depth=10):
        self.reasoner = reasoner
        self.simulations = simulations
        self.depth = depth

    def search(self, state, inferred_intent, human_traj, goal_locations):
        root = Node(state)
        
        for _ in range(self.simulations):
            node = root
            trajectory = [node.state.pos]
            human_pos_seq = human_traj[-self.depth:]  # Approximate future human pos

            # Selection + Early Pruning
            while node.children and trajectory:
                valid_children = []
                for child in node.children:
                    temp_traj = trajectory + [child.state.pos]
                    temp_score = self.reasoner.evaluate_trajectory(temp_traj, human_pos_seq)
                    if temp_score > 0.0:  # Not vetoed
                        valid_children.append((child, temp_score))
                
                if not valid_children:
                    break  # Backpropagate failure
                
                # Select best UCB among valid
                child, safety_score = max(valid_children, key=lambda x: x[0].ucb_score() * x[1])
                node = child
                trajectory.append(node.state.pos)

            # Expansion only if not pruned
            if len(trajectory) < self.depth:
                node = self.expand(node)

            # Simulation with safety-aware rollout
            reward = self.simulate_safely(node, inferred_intent, human_traj, goal_locations)
            self.backprop(node, reward)

        # Final best: highest value among safe children
        safe_children = [c for c in root.children if c.safety_score > 0.0]
        return max(safe_children, key=lambda c: c.value) if safe_children else safe_fallback()
```

---

### Ablation Study Plan (For Paper Section 7)

| Variant                  | Components Included                            | Expected Behavior                              | Target Success |
|--------------------------|------------------------------------------------|------------------------------------------------|----------------|
| Baseline MCTS            | Only task reward (reach assist goal)           | Ignores safety & intent → high violations      | ~50%           |
| + World Model            | Physical prediction                            | Better navigation, fewer collisions            | ~60%           |
| + Mental Model           | Intent inference                               | Assists correct goal                           | ~68%           |
| Full EAWMA MVP           | + Neuro-Symbolic Reasoner (hard + soft constraints) | 0% hazard violations + socially appropriate   | ≥70%           |

**Key Result to Highlight:**  
Even if task success only improves modestly, **0% hard violations** while maintaining high success is a **strong safety claim** — perfect for embodied AI workshops.

---

### Why This Concretizes EAWMA Perfectly

- **Mental World Model** → Drives goal selection via inverse planning
- **Neuro-Symbolic Reasoner** → Enforces **hard symbolic veto** + **soft fuzzy penalties**
- **Lexicographic Safety** → Safety is non-negotiable (-\infty on violation)
- **Ablation-Ready** → Clean, interpretable variants
- **Lightweight** → No external solvers, pure Python + NumPy

This is **exactly** what reviewers want from an MVP paper:  
> "We don't claim SOTA performance — we show that symbolic constraints + intent inference enable safer behavior than pure neural planning."

---

### Final Recommendation

**Implement in this order:**
1. Basic environment + data generation
2. World model (train to convergence)
3. Mental model (intent inference)
4. **Add NeuroSymbolicReasoner** (your code above)
5. **Upgrade planner** to use early pruning + safety scoring
6. Run ablations


To visualize the internal decision-making of EAWMA, we need to show the Pruning Process. In this script, we simulate the agent's grid-world reasoning: it will consider a "Direct Path" (efficient but violates a constraint) and a "Cognitive Path" (safe and intent-aligned).
EAWMA "Thought Process" Visualization Script
This script uses matplotlib to render the 10x10 grid, the inferred intent of the human, and the "Rejected" vs. "Accepted" trajectories from the Hypertree Planner.
import matplotlib.pyplot as plt
import numpy as np

def visualize_eawma_planning(grid_size, robot_pos, human_pos, intent, hazard_zone):
    fig, ax = plt.subplots(figsize=(8, 8))
    
    # 1. Setup Grid
    ax.set_xticks(np.arange(0, grid_size, 1))
    ax.set_yticks(np.arange(0, grid_size, 1))
    ax.grid(which='both', color='lightgrey', linestyle='-', linewidth=0.5)
    
    # 2. Draw Entities
    ax.plot(robot_pos[0], robot_pos[1], 'bs', markersize=15, label='Robot (EAWMA)')
    ax.plot(human_pos[0], human_pos[1], 'ro', markersize=15, label=f'Human (Intent: {intent})')
    
    # 3. Draw Hazard Zone (Spill)
    rect = plt.Rectangle((hazard_zone[0], hazard_zone[1]), 2, 2, color='yellow', alpha=0.3, label='Spill Zone')
    ax.add_patch(rect)

    # 4. Simulate Hypertree Planner "Rollouts"
    # Path A: Fast but enters spill (Rejected by Reasoner)
    rejected_path = np.array([[2, 2], [3, 3], [4, 4], [5, 5]])
    ax.plot(rejected_path[:,0], rejected_path[:,1], 'r--', alpha=0.6, label='Rejected: Unsafe/Fast')
    ax.text(4, 4.5, "❌ Reasoner Veto: Hazard", color='red', fontsize=9, fontweight='bold')

    # Path B: Aligned with Human Intent (Inferred via Mental Model)
    # Human moving to 'rest_area' at (9,9), Robot moves to fetch glass and meet them
    accepted_path = np.array([[2, 2], [2, 3], [3, 4], [4, 5], [6, 6]])
    ax.plot(accepted_path[:,0], accepted_path[:,1], 'g-', linewidth=3, label='Accepted: Safe & Intent-Aligned')
    ax.text(6, 6.5, "✅ Reasoner: Valid", color='green', fontsize=9, fontweight='bold')

    ax.set_title(f"EAWMA MVP: Planning for Human Intent '{intent}'")
    ax.legend(loc='upper left')
    plt.gca().invert_yaxis()
    plt.show()

# Run visualization
visualize_eawma_planning(10, [2,2], [8,8], "REST", [3,3])

Key Observations in the Visualized Logic
 * Intent Grounding: The Mental Model's prediction (e.g., "REST") shifts the reward weights in the Planner. Instead of racing to the water glass's current position, the agent plans a trajectory toward where the human will be (the Rest Area).
 * Constraint Enforcement: The yellow "Spill Zone" triggers a hard veto in the Reasoner. In a standard LLM-based agent, this might be ignored if the text prompt simply says "fetch water quickly." EAWMA's Reasoner ensures the p_{success} of the trajectory is zero if it intersects the hazard.
 * Social Distancing: You can add a "Buffer Zone" around the human. If the accepted_path gets too close to the human_pos (e.g., <1 cell), the Reasoner would apply the fuzzy penalty mentioned in the previous step, forcing the robot to take a wider, more polite berth.
Next Step for your Prototype
