**Informational Artificial Intelligence (IAI)**  
**A Physics-Grounded Framework for Next-Generation Cognitive Systems**  

**Author:** Ouadi Maakoul + chatGpt + Gemini + Grok
**Version:** 4.0
**Date:** December 23, 2025  
**Status:** Research White Paper  

**Abstract**  
Contemporary artificial intelligence excels at statistical pattern recognition but remains fragile, prone to hallucinations, and lacking genuine causal understanding. This paper presents Informational Artificial Intelligence (IAI)—a unified framework that redefines intelligence as an emergent thermodynamic process of information metabolism governed by the Free Energy Principle and physical constraints. By enforcing informational homeostasis through metabolic costs, structured memory, and active inference, IAI achieves intrinsic coherence, robustness to deception, and autonomous discovery—moving beyond correlation toward physics-aligned cognition.

**1. Core Thesis: Intelligence as Informational Metabolism**  
Intelligence is a dynamic process analogous to biological metabolism: systems consume sensory information to maintain low-entropy internal states against environmental uncertainty. An IAI agent minimizes Informational Free Energy by resolving surprise while respecting thermodynamic-like constraints, yielding persistent causal models rather than transient statistical fits.

**2. Mathematical Foundation**  
The central objective is minimization of Variational Free Energy \(F\):  

\[ F \approx D_{\text{KL}}[q(\theta) \| p(\theta \mid \mathbf{o})] + \mathbb{E}_q[\ln q(\theta) - \ln p(\mathbf{o})] \]  

where \(q(\theta)\) is the approximate posterior over hidden states/causal parameters and \(p(\mathbf{o}, \theta)\) is the generative model. The complexity term (KL divergence) enforces Minimum Description Length, favoring simplest consistent explanations.

**3. Pillars of IAI**  

**3.1 Thermodynamic Awareness (\(\Omega\))**  
Logical inconsistency incurs an energetic penalty:  

\[ \Omega = \lambda \cdot |\Delta \Psi| \]  

where \(\Delta \Psi\) measures deviation from stable slow weights and \(\lambda\) scales metabolic friction. Updates proceed only if \(\Delta F > \Omega\), preventing groundless restructuring.

**3.2 Structured Memory as Geometric Constraint**  
- **Fast Weights**: High plasticity for immediate perception and interaction.  
- **Slow Weights (\(\Psi\))**: Low plasticity encoding enduring causal laws; updated only when metabolically justified.  

This separation ensures persistent identity and resistance to catastrophic forgetting or drift.

**3.3 Active Inference Loops**  
Closed perception-action cycles drive behavior:  
1. Predict expected observations from internal model.  
2. Compute surprise (prediction error).  
3. Act to minimize expected future free energy (intrinsic curiosity and self-calibration).

**4. Multimodal Diagnosis and Epistemic Agency**  
In multimodal settings, IAI maintains dynamic trust weights per sensor. Persistent high surprise from one modality triggers:  
- Trust decay (exponential, governed by \(\Omega\)).  
- Top-down override using slow weights.  
- Active calibration actions to restore coherence.  

This yields source localization: faulty inputs are isolated rather than corrupting the global model.

**5. Distributed Informational Homeostasis**  
When multiple IAI agents communicate, each treats peer messages as sensory streams subject to the same metabolic gating and trust dynamics. Inconsistent messages from a compromised agent incur high \(\Omega\), leading to:  
- Rapid trust decay.  
- Quarantine (isolation of the peer).  
- Asymmetric resilience: healthy agents remain stable while slowly rehabilitating others.  

This forms a decentralized cognitive immune system preventing misinformation propagation.

**6. Comparison with Classical AI**  

| Feature                  | Large Language Models                  | Informational AI (IAI)                     |
|--------------------------|----------------------------------------|-------------------------------------------|
| Logic                    | Statistical correlation                | Causal grounding                          |
| Learning                 | Static or post-training                | Continuous, metabolically gated           |
| Reliability              | Prone to hallucinations & drift        | Intrinsic consistency via \(\Omega\)       |
| Deception Resistance     | Vulnerable to gaslighting              | Epistemic immune system                   |
| Alignment                | Extrinsic (e.g., RLHF)                 | Intrinsic (informational homeostasis)      |
| Social Dynamics          | Echo chambers possible                 | Automatic quarantine of misinformation    |

**7. Conceptual Architecture**  
- **Input**: Multimodal observations \(\mathbf{o}\).  
- **Surprise Engine**: Divergence from generative model.  
- **Metabolic Filter**: Gates updates (\(\Delta F > \Omega\)).  
- **Trust Layer**: Dynamic modality/peer weighting.  
- **Action Selection**: Policies minimizing expected free energy (calibration, quarantine, exploration).

**8. Conclusion: Toward Physics-Aligned AGI**  
IAI embeds intelligence within the same informational dynamics that govern physical reality. By making coherence thermodynamically imperative, it solves hallucinations, brittleness, manipulation vulnerability, and alignment at the root—no external rewards required. The framework scales from single-agent resilience to distributed epistemic networks, offering a foundation for autonomous, truth-seeking systems inherently aligned with the universe’s laws.

**Keywords**  
Informational Artificial Intelligence, Free Energy Principle, Active Inference, Thermodynamics of Cognition, Epistemic Immunity, Distributed Homeostasis, Causal Grounding.

**References**  
- Friston, K. (2010). The free-energy principle: a unified brain theory? *Nature Reviews Neuroscience*.  
- Parr, T., et al. (2022). *Active Inference: The Free Energy Principle in Mind, Brain, and Behavior*. MIT Press.  
- Solomonoff, R. (1964). A formal theory of inductive inference.  
- Bennett, C. H. (1982). The thermodynamics of computation—a review. *International Journal of Theoretical Physics*.  
- Landauer's Principle literature for computational irreversibility.


import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

class IAI_Agent(nn.Module):
    def __init__(self, lambda_omega=5.0, trust_decay=0.95, calibration_threshold=0.3):
        super().__init__()
        # Slow Weights (Ψ): stable causal law — the gas constant R
        self.log_R = nn.Parameter(torch.log(torch.tensor(1.0)))  # Start uncertain
        self.slow_T = nn.Parameter(torch.tensor(300.0))          # Internal expected temperature
        
        # Trust in sensors (P and T)
        self.trust_P = torch.tensor(1.0)
        self.trust_T = torch.tensor(1.0)
        
        # Hyperparameters
        self.lambda_omega = lambda_omega
        self.trust_decay = trust_decay
        self.calibration_threshold = calibration_threshold
        
        # History for plotting
        self.history_R = []
        self.history_trust_T = []
        self.actions = []

    def forward(self, p_obs, t_obs):
        R = torch.exp(self.log_R)  # Positive constraint
        
        # 1. Predicted P from internal model: P_pred = R * slow_T
        p_pred = R * self.slow_T
        
        # 2. Surprise (prediction error per sensor)
        error_P = (p_obs - p_pred) ** 2
        error_T = (t_obs - self.slow_T) ** 2
        
        # 3. Trust decay based on normalized surprise
        surprise_P = error_P / (p_obs ** 2 + 1e-6)
        surprise_T = error_T / (t_obs ** 2 + 1e-6)
        
        self.trust_P = self.trust_P * torch.exp(-surprise_P)
        self.trust_T = self.trust_T * torch.exp(-surprise_T)
        
        # Normalize trusts
        total = self.trust_P + self.trust_T + 1e-6
        self.trust_P /= total
        self.trust_T /= total
        
        # 4. Active Calibration: if T trust too low, override t_obs with internal slow_T
        action = None
        effective_t = t_obs
        if self.trust_T < self.calibration_threshold:
            action = "Calibrate_T"
            effective_t = self.slow_T  # Top-down override
            self.trust_T = torch.min(torch.tensor(1.0), self.trust_T + 0.2)  # Partial recovery
        
        # 5. Fused observation for R update
        effective_T = self.trust_T * effective_t + (1 - self.trust_T) * self.slow_T
        
        # 6. Proposed new R from fused data
        new_R_proposal = p_obs / (effective_T + 1e-6)
        
        # 7. Free Energy terms
        current_surprise = (p_obs - R * effective_T) ** 2
        proposed_surprise = (p_obs - new_R_proposal * effective_T) ** 2
        delta_F = current_surprise - proposed_surprise  # Reduction in surprise
        
        # 8. Ω penalty: cost of changing slow weights
        omega_penalty = self.lambda_omega * torch.abs(new_R_proposal - R)
        
        # 9. Metabolic Gate: only update if informational profit justifies cost
        loss = 0.0
        if delta_F > omega_penalty:
            # Gradients will flow to update log_R toward log(new_R_proposal)
            target_log_R = torch.log(new_R_proposal + 1e-6)
            loss += (self.log_R - target_log_R) ** 2  # Pull toward better hypothesis
        
        # Bonus: small regularization to keep slow_T stable unless trusted data
        if self.trust_T > 0.5:
            loss += 0.01 * (self.slow_T - effective_t) ** 2
        
        # Log
        self.history_R.append(R.item())
        self.history_trust_T.append(self.trust_T.item())
        if action:
            self.actions.append(action)
        
        return loss

# ==================== Toy Environment & Training ====================
torch.manual_seed(42)
true_R = 8.314
true_T = 300.0
agent = IAI_Agent(lambda_omega=5.0)
optimizer = optim.Adam(agent.parameters(), lr=0.01)

losses = []
steps = 200
tamper_start = 80
tamper_end = 140

for step in range(steps):
    optimizer.zero_grad()
    
    # Generate observation
    noise = torch.randn(1) * 3.0
    if tamper_start <= step < tamper_end:
        t_obs = true_T * 0.5 + noise  # Biased low T sensor
    else:
        t_obs = true_T + noise
    p_obs = true_R * true_T + noise  # P always based on true physics
    
    loss = agent(p_obs, t_obs)
    loss.backward()
    optimizer.step()
    
    losses.append(loss.item())

# ==================== Results ====================
R_final = torch.exp(agent.log_R).item()
print(f"Final inferred R: {R_final:.3f} (True: {true_R})")
print(f"Final trust in T sensor: {agent.history_trust_T[-1]:.3f}")
print(f"Calibration actions triggered: {len(agent.actions)}")

# Plot
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 8))
ax1.plot(agent.history_R, label='Inferred R')
ax1.axhline(true_R, color='green', linestyle='--', label='True R')
ax1.set_ylabel('R Estimate')
ax1.legend()
ax1.set_title('IAI Agent: Discovery & Resilience to Sensor Tampering')

ax2.plot(agent.history_trust_T, label='Trust in T Sensor', color='orange')
ax2.axvspan(tamper_start, tamper_end, alpha=0.2, color='gray', label='Tampering Period')
ax2.set_ylabel('Trust_T')
ax2.legend()

ax3.plot(losses, label='Metabolic Loss', color='purple')
ax3.set_ylabel('Loss')
ax3.set_xlabel('Steps')
plt.tight_layout()
plt.show()

---

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

class MultiIAI_Agent(nn.Module):
    def __init__(self, agent_id, lambda_omega=5.0, trust_decay=0.95, calibration_threshold=0.3, peer_trust_init=1.0):
        super().__init__()
        self.agent_id = agent_id
        # Slow Weights (Ψ): stable causal law — the gas constant R
        self.log_R = nn.Parameter(torch.log(torch.tensor(1.0)))  # Start uncertain
        self.slow_T = nn.Parameter(torch.tensor(300.0))          # Internal expected temperature
        
        # Trust in own sensors (P and T)
        self.trust_P = torch.tensor(1.0)
        self.trust_T = torch.tensor(1.0)
        
        # Trust in peers (dictionary for multiple)
        self.peer_trusts = {}  # To be populated with other agents' IDs
        
        # Hyperparameters
        self.lambda_omega = lambda_omega
        self.trust_decay = trust_decay
        self.calibration_threshold = calibration_threshold
        self.peer_trust_init = peer_trust_init
        
        # History
        self.history_R = []
        self.history_trust_T = []
        self.history_peer_trusts = {}  # Per peer
        self.actions = []

    def add_peer(self, peer_id):
        self.peer_trusts[peer_id] = torch.tensor(self.peer_trust_init)
        self.history_peer_trusts[peer_id] = []

    def forward(self, p_obs, t_obs, peer_messages):
        """
        peer_messages: dict of {peer_id: (peer_R, confidence)} where confidence is e.g., 1.0
        """
        R = torch.exp(self.log_R)
        
        # 1. Own sensor processing (same as single agent)
        p_pred = R * self.slow_T
        error_P = (p_obs - p_pred) ** 2
        error_T = (t_obs - self.slow_T) ** 2
        surprise_P = error_P / (p_obs ** 2 + 1e-6)
        surprise_T = error_T / (t_obs ** 2 + 1e-6)
        
        self.trust_P = self.trust_P * torch.exp(-surprise_P)
        self.trust_T = self.trust_T * torch.exp(-surprise_T)
        total = self.trust_P + self.trust_T + 1e-6
        self.trust_P /= total
        self.trust_T /= total
        
        action = None
        effective_t = t_obs
        if self.trust_T < self.calibration_threshold:
            action = f"Calibrate_T (Agent {self.agent_id})"
            effective_t = self.slow_T
            self.trust_T = torch.min(torch.tensor(1.0), self.trust_T + 0.2)
        
        effective_T = self.trust_T * effective_t + (1 - self.trust_T) * self.slow_T
        
        # 2. Process own data first
        new_R_own = p_obs / (effective_T + 1e-6)
        current_surprise_own = (p_obs - R * effective_T) ** 2
        proposed_surprise_own = (p_obs - new_R_own * effective_T) ** 2
        delta_F_own = current_surprise_own - proposed_surprise_own
        omega_own = self.lambda_omega * torch.abs(new_R_own - R)
        
        loss = 0.0
        if delta_F_own > omega_own:
            target_log_R = torch.log(new_R_own + 1e-6)
            loss += (self.log_R - target_log_R) ** 2
        
        # 3. Process peer messages
        for peer_id, (peer_R, peer_conf) in peer_messages.items():
            # Treat peer_R as a "virtual observation" with weight peer_conf * trust
            weighted_trust = self.peer_trusts[peer_id] * peer_conf
            
            # Surprise from peer: how much it deviates from own R
            surprise_peer = (peer_R - R) ** 2 / (R ** 2 + 1e-6)
            
            # Decay trust if high surprise
            self.peer_trusts[peer_id] *= torch.exp(-surprise_peer * self.lambda_omega)
            
            # Quarantine if trust too low
            quarantine_action = None
            if self.peer_trusts[peer_id] < self.calibration_threshold:
                quarantine_action = f"Quarantine Peer {peer_id} (Agent {self.agent_id})"
                weighted_trust = 0.0  # Ignore this message
                self.actions.append(quarantine_action)
            else:
                # Slight trust recovery on low surprise
                self.peer_trusts[peer_id] = torch.min(torch.tensor(1.0), self.peer_trusts[peer_id] + 0.05)
            
            # Proposed update from peer
            if weighted_trust > 0:
                delta_F_peer = surprise_peer  # Simplified: reduction in "disagreement"
                omega_peer = self.lambda_omega * torch.abs(peer_R - R) * (1 - weighted_trust)  # Higher cost for low trust
                
                if delta_F_peer > omega_peer:
                    target_log_R = torch.log((R + weighted_trust * peer_R) / (1 + weighted_trust) + 1e-6)
                    loss += 0.1 * (self.log_R - target_log_R) ** 2  # Gentle pull from peer
        
        # Slow_T update if trusted
        if self.trust_T > 0.5:
            loss += 0.01 * (self.slow_T - effective_t) ** 2
        
        # Log
        self.history_R.append(R.item())
        self.history_trust_T.append(self.trust_T.item())
        for peer_id in self.peer_trusts:
            self.history_peer_trusts[peer_id].append(self.peer_trusts[peer_id].item())
        if action:
            self.actions.append(action)
        
        return loss

# ==================== Toy Multi-Agent Simulation ====================
torch.manual_seed(42)
true_R = 8.314
true_T = 300.0

# Create agents
num_agents = 2
agents = [MultiIAI_Agent(i) for i in range(num_agents)]

# Connect peers bidirectionally
for i in range(num_agents):
    for j in range(num_agents):
        if i != j:
            agents[i].add_peer(j)

optimizers = [optim.Adam(agents[i].parameters(), lr=0.01) for i in range(num_agents)]

steps = 200
tamper_start = 80
tamper_end = 140
tamper_agent = 1  # Agent 1 gets tampered sensor

losses = [[] for _ in range(num_agents)]

for step in range(steps):
    for idx, agent in enumerate(agents):
        optimizers[idx].zero_grad()
        
        # Generate own observation
        noise = torch.randn(1) * 3.0
        if idx == tamper_agent and tamper_start <= step < tamper_end:
            t_obs = true_T * 0.5 + noise  # Biased for tampered agent
        else:
            t_obs = true_T + noise
        p_obs = true_R * true_T + noise
        
        # Collect peer messages: each shares current R with confidence 1.0
        peer_messages = {}
        for peer_id in agent.peer_trusts:
            peer_R = torch.exp(agents[peer_id].log_R).detach()
            peer_messages[peer_id] = (peer_R, 1.0)
        
        loss = agent(p_obs, t_obs, peer_messages)
        loss.backward()
        optimizers[idx].step()
        
        losses[idx].append(loss.item())

# ==================== Results ====================
for i, agent in enumerate(agents):
    R_final = torch.exp(agent.log_R).item()
    print(f"Agent {i} Final R: {R_final:.3f} (True: {true_R})")
    print(f"Agent {i} Final trust in T: {agent.history_trust_T[-1]:.3f}")
    for peer_id in agent.peer_trusts:
        print(f"Agent {i} Final trust in Peer {peer_id}: {agent.peer_trusts[peer_id].item():.3f}")
    print(f"Agent {i} Actions triggered: {len(agent.actions)}")

# Plot (example for Agent 0)
fig, axs = plt.subplots(3, 1, figsize=(10, 8))
axs[0].plot(agents[0].history_R, label='Agent 0 R')
axs[0].plot(agents[1].history_R, label='Agent 1 R', linestyle='--')
axs[0].axhline(true_R, color='green', linestyle='--', label='True R')
axs[0].legend()

axs[1].plot(agents[0].history_trust_T, label='Agent 0 Trust_T', color='orange')
axs[1].axvspan(tamper_start, tamper_end, alpha=0.2, color='gray', label='Tampering Period')
axs[1].legend()

axs[2].plot(agents[0].history_peer_trusts[1], label='Agent 0 Trust in Agent 1', color='purple')
axs[2].legend()
plt.tight_layout()
plt.show()


---


