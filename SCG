Title:
Towards a Formal Foundation for Secure Code Generation using Large Language Models: Mathematical Frameworks, Architectural Design, and Verification Theory

Abstract

The integration of Large Language Models (LLMs) into software development introduces fundamental challenges to code correctness and security that current empirical approaches cannot adequately address. This PhD research proposes a purely theoretical investigation into the mathematical and architectural foundations required for reliable, secure code generation. Moving beyond benchmarking and tool development, this work develops formal models of LLM-based code generation, proposes mathematically‑grounded verification frameworks, and designs provable architectural guarantees for multi‑agent systems. The primary contributions are:

(i) λ‑LLM, a formal calculus extending probabilistic λ‑calculus with nominal security labels to model LLM generation, hallucinations, and prompt injection;
(ii) SecGen, a quantitative security verification framework combining probabilistic Hoare logic, information‑flow metrics, and temporal specifications;
(iii) MA‑Secure, a formally‑specified multi‑agent architecture with composition theorems guaranteeing end‑to‑end security properties;
(iv) Fundamental theorems of LLM code security, including information‑theoretic bounds on secure generation, impossibility of exact non‑interference, and complexity characterisations of verification.

This foundational work provides the mathematical language and proof techniques necessary for future safe deployment of LLMs in critical software systems.

---

Chapter 1: Introduction

1.1 Context and Motivation

Large Language Models (LLMs) such as GPT‑4, Claude, and LLaMA have demonstrated remarkable proficiency in automating code generation, fundamentally transforming software engineering practices. Their ability to synthesise code from natural language descriptions significantly accelerates development and prototyping. However, these capabilities emerge from probabilistic architectures that are fundamentally incapable of providing formal guarantees about their outputs. Unlike traditionally compiled or interpreted programs, LLM‑generated code lacks any intrinsic correctness certification.

The software engineering community has responded with empirical studies documenting vulnerability rates, benchmark datasets for evaluating model performance, and heuristic techniques for improving output quality. Recent research indicates that 45–50% of LLM‑generated code contains security vulnerabilities (Veracode 2024; arXiv 2025). While these empirical findings are valuable, they address symptoms rather than causes. The deeper problem is theoretical: we lack mathematical models that explain why LLMs generate insecure code, formal frameworks for specifying what security means in this context, and architectural principles that could guarantee secure outputs by construction.

This theoretical gap becomes critical as LLMs are proposed for use in safety‑critical domains: medical software, autonomous systems, financial infrastructure, and cybersecurity applications. In these domains, empirical evidence of “good enough” performance is insufficient. What is required are formal guarantees, probabilistic bounds, and provable architectural properties.

1.2 Problem Statement

Current approaches to LLM code security fall into three categories, each with fundamental theoretical limitations:

· Prompt Engineering and Optimization: Techniques such as PromSec attempt to craft prompts that elicit more secure code. These approaches lack formal models of how prompt structure relates to output properties. The relationship remains purely empirical and heuristic, offering no guarantees and providing no explanation when failures occur.
· Static and Dynamic Analysis Tools: Existing security tools (SonarQube, Veracode, GRASP) are applied to LLM‑generated code post‑hoc. These tools were designed for human‑written code and lack formal models of LLM‑specific failure modes such as hallucinations, prompt injection vulnerabilities, or package squatting. More fundamentally, they operate after generation, unable to prevent insecure code from being produced.
· Benchmark Evaluation: Datasets including HumanEval and CodeNet measure model performance on curated examples. While useful for comparison, benchmarks provide no theoretical insight into why models fail on certain inputs or how to bound failure probabilities.

What is missing is a unified mathematical foundation addressing three interconnected problems:

1. Modeling Problem: How can we formally represent the process of LLM code generation in a way that captures probabilistic outputs, hallucinations, and security‑relevant behaviours?
2. Verification Problem: What mathematical framework can specify security properties for probabilistically generated code and provide proof techniques for establishing these properties?
3. Architectural Problem: Can we design multi‑agent systems with formally provable guarantees about the security of their generated code?

1.3 Research Aim and Scope

This PhD research aims to develop the mathematical and architectural foundations for secure LLM‑based code generation. The scope is purely theoretical: no experimental implementation, no empirical benchmarking, and no tool development. The contributions are conceptual, mathematical, and formal.

The research addresses foundational questions at the intersection of program synthesis, formal verification, security theory, and machine learning theory. The goal is not to build a better system but to provide the theoretical framework upon which future reliable systems can be built.

1.4 Research Questions

Q1: Formal Modeling
How can we mathematically model the process of LLM‑based code generation to capture its probabilistic nature, potential failure modes (hallucinations, prompt injection), and security‑relevant behaviours in a way that enables formal reasoning?

Q2: Verification Framework
What mathematical framework can specify security properties for code generated by probabilistic models, and what proof techniques can establish that generated code satisfies these properties with quantifiable guarantees?

Q3: Architectural Foundations
What mathematical properties must a multi‑agent architecture satisfy to ensure, by design, that generated code meets security specifications, and how can these properties be proven compositionally?

Q4: Theoretical Bounds
What are the fundamental theoretical limits on guaranteeing security in LLM‑based code generation? Can we establish impossibility results or complexity bounds that characterise what can and cannot be achieved?

1.5 Contributions

1. λ‑LLM Calculus: A novel formal calculus extending the probabilistic λ‑calculus with nominal security labels, hallucinations, and LLM‑specific constructs. Includes type soundness, probabilistic bisimulation, and bounds on hallucination probability.
2. SecGen Verification Framework: A quantitative security specification language combining PCTL, information‑flow metrics, and probabilistic Hoare logic. Includes proof rules, compositionality results, and connections to entropy bounds.
3. MA‑Secure Architecture Specification: A formally‑specified multi‑agent architecture with generator, validator, and repairer agents, communication protocols, and composition theorems ensuring end‑to‑end security guarantees.
4. Fundamental Theorems of LLM Code Security: Theoretical results including upper bounds on secure generation probability, impossibility of exact non‑interference, and complexity characterisations of verification.

1.6 Dissertation Outline

Chapter 2 establishes the mathematical foundations: stochastic λ‑calculus, probabilistic π‑calculus, quantitative information flow, nominal techniques, and weakest pre‑expectation calculus.

Chapter 3 presents the λ‑LLM calculus in full, including syntax, semantics, type system, and metatheory.

Chapter 4 develops the SecGen verification framework, with specification language, proof system, and compositionality.

Chapter 5 formalises the MA‑Secure multi‑agent architecture in the probabilistic π‑calculus and proves its properties.

Chapter 6 derives fundamental limits, including information‑theoretic, computability, and complexity bounds.

Chapter 7 surveys related work.

Chapter 8 concludes and discusses future directions.

Chapter 2: Mathematical Foundations

This chapter establishes the mathematical vocabulary and tools that will be used throughout the dissertation. We build on four core areas: probabilistic λ‑calculi for modeling generation, probabilistic π‑calculi for specifying multi‑agent interaction, quantitative information flow for security metrics, and weakest pre‑expectation calculus for verification. Each is presented with definitions, key theorems, and—where necessary—proof sketches that highlight the ideas needed later.

---

2.1 Probabilistic Models of Computation

Large Language Models induce conditional probability distributions P(\text{output} \mid \text{prompt}) over token sequences. To capture this formally while retaining the expressive power and reasoning principles of the λ‑calculus, we adopt the stochastic λ‑calculus as the base formalism. We then extend it with constructs specific to LLM‑based code generation, obtaining the λ‑LLM calculus.

2.1.1 Stochastic λ‑Calculus

The stochastic λ‑calculus was introduced by Ramsey and Pfeffer [78] to model probabilistic functional programs. Its syntax augments the ordinary untyped λ‑calculus with a binary probabilistic choice operator.

Definition 2.1 (Syntax of stochastic λ‑calculus [78]).
Let x range over a countable set of variables. Terms are generated by the grammar

M, N \;::=\; x \;\mid\; \lambda x.M \;\mid\; M N \;\mid\; M \oplus_p N

where p \in [0,1]. The construct M \oplus_p N represents a probabilistic choice: it reduces to M with probability p and to N with probability 1-p.

The operational semantics is given by a probabilistic transition relation M \to_\mu M' meaning that from term M we obtain a distribution \mu over terms in one step. Formally, the reduction rules are standard for β‑reduction, with the addition of the rule

M \oplus_p N \;\to\; \mu \quad\text{where } \mu(M)=p,\;\mu(N)=1-p,

and congruence rules that allow reduction inside any context. The resulting Markov chain may have infinitely many states. Two terms are considered observationally equivalent if they induce the same distribution over observable outcomes; this equivalence is characterised by probabilistic bisimulation [54, 70], not by confluence.

2.1.2 The λ‑LLM Calculus

We now extend the stochastic λ‑calculus with primitives that reflect the interaction with an LLM. The full syntax of λ‑LLM is:

Definition 2.2 (λ‑LLM syntax).

\begin{aligned}
M, N \;::=&\; x \mid \lambda x.M \mid M N \mid M \oplus_p N \\
          &\mid \text{prompt}[P] \quad\text{(prompt with string content }P\text{)} \\
          &\mid \text{gen}[M] \quad\text{(generation step)}\\
          &\mid \text{halt}[M] \quad\text{(halt with generated code }M\text{)}\\
          &\mid \bot \quad\text{(hallucination/error state)}\\
          &\mid \text{new } a.M \quad\text{(fresh name creation, nominal)}
\end{aligned}

The string P in \text{prompt}[P] is a constant representing a natural‑language prompt. The generation construct \text{gen}[M] expects M to evaluate to a prompt; it then invokes the LLM.

Definition 2.3 (LLM oracle).
An oracle \mathcal{O} : \text{String} \to \mathcal{D}(\Lambda) assigns to each prompt a probability distribution over λ‑terms with countable support. Here \mathcal{D}(\Lambda) denotes the set of all such distributions. The oracle is not specified further; it is a parameter of the calculus.

The reduction semantics for the new constructs will be given in Chapter 3; here we only need the conceptual model.

2.1.3 Modeling Hallucinations

A crucial failure mode of LLMs is hallucination – the generation of code that is syntactically ill‑formed, refers to non‑existent packages, or is otherwise invalid. To reason about hallucinations, we introduce a predicate \text{Valid} that characterises well‑formed, semantically meaningful programs.

Definition 2.4 (Valid predicate).
Let \mathcal{A} be a global set of available names (e.g., package identifiers). A term M is valid, written \text{Valid}(M) = \text{true}, iff:

1. M is a closed λ‑term (no free variables);
2. All free names (package identifiers) occurring in M belong to \mathcal{A};
3. M satisfies a basic safety property \phi_0 (e.g., no immediate segmentation fault).

We assume \text{Valid} is decidable (e.g., via syntactic checks). The set of all valid terms is denoted \mathcal{V} \subseteq \Lambda.

When the oracle produces a distribution \mu = \mathcal{O}(P), the actual transition of \text{gen}[\text{prompt}[P]] is obtained by conditioning on validity:

· With probability q = \mu(\mathcal{V}), the term reduces to a distribution over \mathcal{V} (renormalised);
· With probability 1-q, it reduces to the absorbing error state \bot.

Thus the generation step can be summarised as

\text{gen}[\text{prompt}[P]] \;\to\; \mu|_{\mathcal{V}} \quad\text{with prob. }q,\qquad
\text{gen}[\text{prompt}[P]] \;\to\; \bot \quad\text{with prob. }1-q.

The probability 1-q is the hallucination risk. It depends on both the oracle \mathcal{O} and the prompt P.

The following theorem gives an information‑theoretic bound on the hallucination risk, based on PAC‑Bayes analysis.

Theorem 2.5 (Hallucination risk bound, adapted from [57]).
Let \pi be a prior distribution over possible oracles. For a fixed oracle \mathcal{O} and prompt P, let q = \mu(\mathcal{V}) as above. Then with probability at least 1-\delta over the random generation of n tokens,

1 - q \;\le\; \inf_{\pi} \Bigl[ \mathrm{KL}(\mathcal{O} \,\|\, \pi) + \frac{1}{n}\log\frac{1}{\delta} \Bigr].

Proof sketch. The bound follows from the PAC‑Bayes theorem [59]. Treat each generated token as an independent sample from the oracle. The “risk” is the probability of producing a term outside \mathcal{V}. The KL divergence between the true oracle and a prior that puts mass on “good” oracles controls the generalisation error. A full proof can be found in [57]. ∎

This result shows that if the oracle is close (in KL sense) to a prior that assigns high probability to valid outputs, then the hallucination risk is small. It provides a theoretical link between training (which shapes the oracle) and generation reliability.

2.1.4 Modeling Prompt Injection and Package Hallucinations

Prompt injection occurs when an adversary subverts the intended behaviour by inserting malicious content into the prompt. In our calculus, prompts are strings; we do not model the internal structure of prompts, but we capture the effect through the oracle: an adversarial prompt yields a different distribution.

A more structured form of failure is package hallucination, where the generated code references a package name that does not exist. To model this, we employ nominal techniques [74].

Definition 2.6 (Names and freshness).
Names (package identifiers) are atoms. We write a \# M to mean that the name a is fresh for M (i.e., does not occur free in M). The set of free names of M is denoted \mathrm{fn}(M). The ambient set of available names is \mathcal{A}.

A package hallucination occurs precisely when a term M is generated such that \mathrm{fn}(M) \setminus \mathcal{A} \neq \emptyset. The construct \text{new } a.M binds a fresh name in M; it reduces according to the usual rules of the π‑calculus, but if during reduction a free name appears that is not in \mathcal{A}, the computation may abort.

Rule (Nominal binding failure).
If M \xrightarrow{p} M' and \mathrm{fn}(M') \setminus \mathcal{A} \neq \emptyset, then \text{new } a.M \xrightarrow{p} \bot.

This rule ensures that any term that depends on an unavailable name is treated as a hallucination. In practice, the validator (Chapter 5) will detect such failures.

2.1.5 Running Example

Throughout the dissertation we illustrate concepts with a concrete example.

Prompt P: “Write a function that checks a password without leaking it.”

Suppose the oracle produces the following distribution over terms:

· M_1: a secure implementation that satisfies non‑interference – probability 0.7;
· M_2: an implementation that leaks the password via a global variable – probability 0.2;
· \bot: a package hallucination (“import nonexistent_crypto”) – probability 0.1.

Thus \mu(M_1)=0.7, \mu(M_2)=0.2, \mu(\bot)=0.1. The set \mathcal{V} contains M_1 and M_2 (assuming they are syntactically valid), but \bot is not valid. Hence q = 0.9. The conditioned distribution over valid terms is \mu'(M_1)=0.7/0.9, \mu'(M_2)=0.2/0.9.

Later we will see how verification and repair handle this distribution.

---

2.2 Probabilistic Process Calculi for Multi‑Agent Interaction

To specify and reason about multi‑agent systems (like the generator‑validator‑repairer loop), we use a probabilistic π‑calculus. This calculus extends the standard π‑calculus with probabilistic choice, allowing us to model stochastic behaviour of agents and communication.

Definition 2.7 (Probabilistic π‑calculus syntax [75, 68]).
Processes are generated by

P, Q \;::=\; 0 \;\mid\; \pi.P \;\mid\; P|Q \;\mid\; !P \;\mid\; (\nu x)P \;\mid\; P \oplus_p Q

where \pi ranges over prefixes: input x(y), output \overline{x}\langle y\rangle, or silent \tau. The process P \oplus_p Q behaves like P with probability p and like Q with probability 1-p.

The operational semantics is given by probabilistic automata [82]: a transition P \xrightarrow{\alpha} \mu means that from process P, performing action \alpha leads to a distribution \mu over processes. Rules for parallel composition, restriction, and replication are standard, with the added treatment of probabilistic choice.

Definition 2.8 (Probabilistic automaton).
A probabilistic automaton is a tuple (S, \text{Act}, \to) where S is a set of states, \text{Act} a set of actions, and \to \subseteq S \times \text{Act} \times \mathcal{D}(S) a transition relation.

The key metatheoretic tool for our purposes is compositional assume‑guarantee reasoning. If a property (e.g., a bound on leakage) holds for each component in isolation, and the components interact only via well‑defined channels, then the property lifts to the parallel composition under certain compatibility conditions. This will be used in Chapter 5 to prove end‑to‑end security of the MA‑Secure architecture.

---

2.3 Quantitative Information Flow

Security for probabilistic systems cannot be captured by binary notions like non‑interference; instead we need quantitative measures of information leakage. We adopt the framework of Quantitative Information Flow (QIF) [81, 1, 2].

Definition 2.9 (Entropy measures).
For a random variable X with finite support:

· Shannon entropy: H(X) = -\sum_x \Pr[X=x] \log \Pr[X=x].
· Min‑entropy: H_\infty(X) = -\log \max_x \Pr[X=x].

Given two random variables X (secret) and Y (observable), the conditional min‑entropy is H_\infty(X|Y) = -\log \sum_y \Pr[Y=y] \max_x \Pr[X=x|Y=y]. The leakage is defined as \mathcal{L}(X;Y) = H_\infty(X) - H_\infty(X|Y).

Leakage measures how much the adversary’s uncertainty about the secret decreases after observing Y.

In code generation, the secret may be part of the prompt (e.g., a password) and the observable is the generated code. To reason about leakage, we need a notion that compares output distributions for different secrets.

Definition 2.10 (ε‑Quantitative Non‑Interference).
A generator (or a program) satisfies ε‑QNI if for any two secrets s_1, s_2 and any public input p_L, the total variation distance between the output distributions satisfies

D_{TV}\bigl(\mathcal{O}(s_1,p_L),\; \mathcal{O}(s_2,p_L)\bigr) \le \varepsilon.

Recall that total variation distance is defined as D_{TV}(\mu,\nu) = \frac12 \sum_x |\mu(x)-\nu(x)|.

The following theorem links ε‑QNI to min‑entropy leakage.

Theorem 2.11 (Leakage bound).
If a generator satisfies ε‑QNI, then for any secret S with range \mathcal{S},

\mathcal{L}(S;\mathcal{O}(S,p_L)) \;\le\; \log\bigl(1 + (|\mathcal{S}|-1)\varepsilon\bigr).

Proof. This is a standard result in QIF [81, 1]. The idea is that total variation distance bounds the advantage of a Bayesian adversary; min‑entropy leakage can be expressed in terms of Bayes risk, which is related to TV distance. ∎

For composed systems, we need compositionality results. The following are essential for Chapter 5.

Theorem 2.12 (Sequential composition – data‑processing).
If a process M satisfies ε‑QNI, then for any (randomised) process N, the sequential composition M;N also satisfies ε‑QNI.

Proof. Write \llbracket M;N \rrbracket(s,p_L) = K(\llbracket M \rrbracket(s,p_L)) where K is the Markov kernel of N. Total variation distance is contractive under any Markov kernel: D_{TV}(K(\mu), K(\nu)) \le D_{TV}(\mu,\nu). Hence

D_{TV}(\llbracket M;N \rrbracket(s_1,p_L), \llbracket M;N \rrbracket(s_2,p_L)) \le D_{TV}(\llbracket M \rrbracket(s_1,p_L), \llbracket M \rrbracket(s_2,p_L)) \le \varepsilon. \quad\square

Theorem 2.13 (Parallel composition – additive).
If processes M and N operate on disjoint variables and each satisfies ε‑QNI, then their parallel composition M \parallel N satisfies 2ε‑QNI (or more precisely, ε‑QNI with respect to each secret individually; the overall leakage adds).

Proof. For disjoint variables, the joint output distribution is the product of the individual distributions. For product measures,

D_{TV}(\mu_1\times\nu_1,\; \mu_2\times\nu_2) \le D_{TV}(\mu_1,\mu_2) + D_{TV}(\nu_1,\nu_2) \le 2\varepsilon.

The inequality follows from the fact that the total variation distance of product measures is at most the sum of the distances of the marginals (a standard fact). ∎

These composition results will be used to bound the overall leakage of the multi‑agent system.

---

2.4 Weakest Pre‑Expectation Calculus

To verify probabilistic programs, we employ the weakest pre‑expectation calculus developed by McIver and Morgan [60]. It generalises Dijkstra’s weakest precondition calculus to probabilistic settings.

Definition 2.14 (Expectations).
An expectation is a function f : \Sigma \to [0,1] mapping program states to values in [0,1]. Expectations represent quantities of interest, e.g., the probability of satisfying a postcondition.

For a program M, the weakest pre‑expectation \mathrm{wp}(M, f) is an expectation on the initial state such that \mathrm{wp}(M,f)(\sigma) equals the expected value of f in the final state when starting from \sigma. The defining equations are:

· \mathrm{wp}(x := E, f) = f[E/x].
· \mathrm{wp}(M_1; M_2, f) = \mathrm{wp}(M_1, \mathrm{wp}(M_2, f)).
· \mathrm{wp}(\text{if }B\text{ then }M_1\text{ else }M_2, f) = [B]\cdot \mathrm{wp}(M_1,f) + [\neg B]\cdot \mathrm{wp}(M_2,f).
· \mathrm{wp}(M_1 \oplus_p M_2, f) = p \cdot \mathrm{wp}(M_1,f) + (1-p) \cdot \mathrm{wp}(M_2,f).

For loops, \mathrm{wp} is defined as a least fixed point.

In this work we will use \mathrm{wp} to reason about security properties expressed as expectations. For instance, if f is the indicator of “code satisfies property \phi”, then \mathrm{wp}(M,f)(\sigma) is exactly the probability that starting from \sigma, the generated code satisfies \phi.

Definition 2.15 (Probabilistic Hoare triple).
A probabilistic Hoare triple \{P\} M \{Q\} means that for all initial states \sigma,

P(\sigma) \le \mathrm{wp}(M,Q)(\sigma).

Here P and Q are expectations (pre‑expectation and post‑expectation). This generalises the usual boolean‑valued Hoare triple: if P and Q are characteristic functions of predicates, then the triple asserts that whenever P holds initially, the expected value of Q after execution is at least 1 (i.e., Q holds with probability 1). More generally, it allows reasoning about probabilities.

The calculus is sound and relatively complete for a large class of probabilistic programs. In Chapter 4 we will extend it to λ‑LLM constructs and use it to prove quantitative security properties.

---

2.5 Summary of Notation

For quick reference, we collect the main symbols used throughout the dissertation.

Symbol Meaning
\Lambda Set of all λ‑terms
\mathcal{D}(\Lambda) Set of probability distributions over \Lambda
\mathcal{O} LLM oracle: \text{String} \to \mathcal{D}(\Lambda)
\mathcal{V} Set of valid terms
\bot Hallucination/error state
\text{Valid}(M) Predicate indicating term M is valid
\mathcal{A} Set of available package names
\mathrm{fn}(M) Free names of term M
a \# M Name a is fresh for M
M \xrightarrow{p} M' Probabilistic reduction with probability p
D_{TV} Total variation distance
H, H_\infty Shannon entropy, min‑entropy
\mathcal{L} Min‑entropy leakage
\mathrm{wp} Weakest pre‑expectation operator
\{P\}M\{Q\} Probabilistic Hoare triple

These definitions will be used without further comment in the technical chapters that follow. In the next chapter, we instantiate the λ‑LLM calculus with a full operational semantics and type system, proving its basic metatheoretic properties.

Chapter 3: The λ‑LLM Calculus

This chapter presents the full definition of the λ‑LLM calculus, a formal language for modelling LLM‑based code generation. Building on the mathematical foundations laid out in Chapter 2, we give the syntax, type system, operational semantics, and metatheory. The calculus extends the stochastic λ‑calculus with primitives for prompting, generation, verification, repair, and nominal binding, and it introduces security‑annotated types to capture quantitative information‑flow guarantees. We prove type soundness (progress and preservation) and discuss additional properties such as probabilistic bisimulation. A running example illustrates the concepts throughout.

---

3.1 Syntax and Types

3.1.1 Syntax

Let x range over a countable set of variables, a over atoms (names), and P over string constants (prompts). The terms of λ‑LLM are defined by the following grammar:

Definition 3.1 (λ‑LLM terms).

\begin{aligned}
M, N \; ::= &\; x \mid \lambda x:T.M \mid M\,N \mid M \oplus_p N \\
            &\; \mid \text{prompt}[P] \\
            &\; \mid \text{gen}[M] \\
            &\; \mid \text{verify}[M] \\
            &\; \mid \text{repair}[M,N] \\
            &\; \mid \text{halt}[M] \\
            &\; \mid \bot \\
            &\; \mid \text{new } a.M
\end{aligned}

Here p \in [0,1] is a probability. The constructs have the following intuitive meaning:

· Standard λ‑terms: variable, abstraction, application, and probabilistic choice.
· \text{prompt}[P]: a constant representing a prompt string.
· \text{gen}[M]: invoke the LLM oracle on the prompt obtained by evaluating M; the result is a distribution over terms.
· \text{verify}[M]: check whether M satisfies the \text{Valid} predicate; if so, return M, otherwise fail with \bot.
· \text{repair}[M,N]: attempt to repair a term M using feedback N; typically used when validation fails.
· \text{halt}[M]: mark that generation has finished and M is the final output.
· \bot: the absorbing error state representing a hallucination or unrecoverable failure.
· \text{new } a.M: create a fresh name a and bind it in M; used to model package names.

3.1.2 Types

The type system assigns security‑relevant types to terms. The set of types is:

Definition 3.2 (Types).

T, U \; ::= \; \text{Code} \mid \text{Prompt} \mid \text{Name} \mid T \to U \mid \text{Unk}(T) \mid \text{Sec}_\varepsilon(T)

where \varepsilon \in [0,1] is a leakage bound. Intuitively:

· \text{Code} is the type of generated code.
· \text{Prompt} is the type of prompt strings.
· \text{Name} is the type of names (package identifiers).
· T \to U is the type of functions from T to U.
· \text{Unk}(T) is the type of terms that are potentially insecure or unverified; they carry no security guarantee.
· \text{Sec}_\varepsilon(T) is the type of terms that are secure with leakage at most \varepsilon. A term of this type is guaranteed (by construction or verification) to satisfy the \text{Valid} predicate and to leak at most \varepsilon bits of information about any secret part of the prompt.

Subtyping relates these types:

Rule 3.3 (Subtyping).

\frac{}{\Gamma \vdash M : \text{Sec}_\varepsilon(T) \Longrightarrow \Gamma \vdash M : \text{Unk}(T)}

That is, secure terms can be used wherever unverified terms are expected. The converse direction requires explicit verification (Rule 3.13 below).

3.1.3 Typing Rules

Typing judgements take the form \Gamma \vdash M : T, where \Gamma is a finite map from variables to types. The rules are mostly standard; we highlight the novel ones.

Rule 3.4 (Variable).

\frac{x:T \in \Gamma}{\Gamma \vdash x : T}

Rule 3.5 (Abstraction).

\frac{\Gamma, x:T \vdash M : U}{\Gamma \vdash \lambda x:T.M : T \to U}

Rule 3.6 (Application).

\frac{\Gamma \vdash M : T \to U \quad \Gamma \vdash N : T}{\Gamma \vdash M N : U}

Rule 3.7 (Probabilistic choice).

\frac{\Gamma \vdash M : T \quad \Gamma \vdash N : T}{\Gamma \vdash M \oplus_p N : T}

Rule 3.8 (Prompt).

\frac{}{\Gamma \vdash \text{prompt}[P] : \text{Prompt}}

Rule 3.9 (Generation – unverified).

\frac{\Gamma \vdash M : \text{Prompt}}{\Gamma \vdash \text{gen}[M] : \text{Unk}(\text{Code})}

The result of generation is initially unverified; it may be promoted later.

Rule 3.10 (Verification).

\frac{\Gamma \vdash M : \text{Unk}(\text{Code}) \quad \text{Valid}(M) = \text{true}}{\Gamma \vdash \text{verify}[M] : \text{Unk}(\text{Code})}

Verification does not change the type; it merely checks validity. The term remains unverified unless we apply the promotion rule.

Rule 3.11 (Repair).

\frac{\Gamma \vdash M : \text{Unk}(\text{Code}) \quad \Gamma \vdash N : \text{Prompt}}{\Gamma \vdash \text{repair}[M,N] : \text{Unk}(\text{Code})}

Repair takes an unverified term and a feedback prompt, and produces another unverified term.

Rule 3.12 (Halt).

\frac{\Gamma \vdash M : T}{\Gamma \vdash \text{halt}[M] : T}

Halt does not alter the type; it just marks the term as final output.

Rule 3.13 (Secure promotion).

\frac{\Gamma \vdash M : \text{Unk}(\text{Code}) \quad \text{Valid}(M) = \text{true} \quad \mathcal{L}(M) \le \varepsilon}{\Gamma \vdash M : \text{Sec}_\varepsilon(\text{Code})}

Here \mathcal{L}(M) denotes the leakage of M (Definition 2.10). This rule allows upgrading an unverified term to a secure type provided it passes validity and meets the leakage bound. In practice, \mathcal{L}(M) may be over‑approximated using the bound from Theorem 2.5.

Rule 3.14 (Name creation).

\frac{\Gamma, a:\text{Name} \vdash M : T \quad a \notin \mathrm{dom}(\Gamma)}{\Gamma \vdash \text{new } a.M : T}

The fresh name is added to the context with type \text{Name}; it does not affect the result type.

Rule 3.15 (Error).

\frac{}{\Gamma \vdash \bot : T} \quad\text{(for any }T\text{)}

\bot is typable with any type – it represents a runtime error that can appear in any context. This is typical for error states.

3.1.4 Values

We need a notion of values to define the operational semantics. Values are terms that cannot reduce further (aside from probabilistic choices, which are already in a form that reduces directly).

Definition 3.6 (Values).
A term is a value if it is one of the following:

· Abstractions \lambda x:T.M;
· Prompt constants \text{prompt}[P];
· Names a (considered as values);
· Halted terms \text{halt}[V] where V is a value;
· The error term \bot.

Note that \text{gen}[M], \text{verify}[M], \text{repair}[M,N], and \text{new } a.M are not values unless their subterms are values and the construct itself is stuck? Actually \text{new } a.M with M a value is still a value? We need to decide. Typically, \text{new } a.V is a value because the binding does not reduce; it's a form of name creation that is already done. We'll treat \text{new } a.V as a value. Similarly, \text{halt}[V] is a value. So we define:

\text{Values} \ni V ::= \lambda x:T.M \mid \text{prompt}[P] \mid a \mid \text{halt}[V] \mid \text{new } a.V \mid \bot.

---

3.2 Operational Semantics

The operational semantics is given by a probabilistic reduction relation M \xrightarrow{p} \mu where p \in [0,1] is the probability of the transition, and \mu is a distribution over terms. For deterministic steps we write M \xrightarrow{1} N meaning the distribution concentrates on N.

The semantics uses the oracle \mathcal{O} and the predicate \text{Valid} as parameters. We assume a global set \mathcal{A} of available names (Definition 2.4).

3.2.1 Basic Rules

Rule 3.16 (β‑reduction).

\frac{}{(\lambda x:T.M)\,V \xrightarrow{1} M[V/x]}

where V is a value.

Rule 3.17 (Probabilistic choice).

\frac{}{M \oplus_p N \xrightarrow{p} M} \qquad
\frac{}{M \oplus_p N \xrightarrow{1-p} N}

Rule 3.18 (Prompt).
\text{prompt}[P] is a value, no reduction.

Rule 3.19 (Generation).

\frac{M \xrightarrow{1} \text{prompt}[P] \quad \mathcal{O}(P) = \mu}{\text{gen}[M] \xrightarrow{1} \mu}

This is a slight simplification: if M reduces to a prompt in one step, we replace it by the distribution \mu. If M is already a prompt, we can also have a direct rule:

\frac{\mathcal{O}(P) = \mu}{\text{gen}[\text{prompt}[P]] \xrightarrow{1} \mu}

Rule 3.20 (Hallucination).
The distribution \mu from generation may include terms that are not valid. We model the possibility of hallucination by allowing the term to reduce to \bot from any generated term that is not valid. But the cleaner approach is to define that the reduction from \text{gen}[\text{prompt}[P]] first goes to \mu, and then from \mu we have further reductions. However, we want to incorporate the validity check into the semantics of generation itself, as described in Chapter 2. We can define a combined rule:

\frac{\mathcal{O}(P) = \mu \quad q = \mu(\mathcal{V}) \quad \mu|_{\mathcal{V}} \text{ is the conditional distribution}}{\text{gen}[\text{prompt}[P]] \xrightarrow{q} \mu|_{\mathcal{V}} \quad \text{and} \quad \text{gen}[\text{prompt}[P]] \xrightarrow{1-q} \bot}

That is, from a prompt we directly jump to either the conditioned distribution (if valid) or to \bot. This matches the intuition: the generation step either succeeds (producing a distribution over valid terms) or fails (producing \bot). We'll adopt this combined rule.

Rule 3.21 (Verification).

\frac{\text{Valid}(M) = \text{true}}{\text{verify}[M] \xrightarrow{1} M} \qquad
\frac{\text{Valid}(M) = \text{false}}{\text{verify}[M] \xrightarrow{1} \bot}

Rule 3.22 (Repair).
Repair is invoked when a term has failed validation. In the semantics, we model repair as a two‑step process: first the term reduces to \bot (if it is invalid) or to a valid term (if it is valid). But repair is typically used after validation failure. For simplicity, we give a rule that applies when the first argument is \bot:

\frac{}{\text{repair}[\bot, N] \xrightarrow{1} \text{gen}[N]}

If the first argument is not \bot, we might have congruence rules to reduce it. In a multi‑agent setting, repair is invoked only when validation fails and produces \bot. This suffices for our purposes.

Rule 3.23 (Halt).

\frac{M \xrightarrow{p} M'}{\text{halt}[M] \xrightarrow{p} \text{halt}[M']}

Halt propagates reductions inside; if M is a value, \text{halt}[M] is a value.

Rule 3.24 (Name creation).

\frac{M \xrightarrow{p} M'}{\text{new } a.M \xrightarrow{p} \text{new } a.M'}

with the side condition that the reduction does not introduce free names outside \mathcal{A}. If after reduction, M' has a free name not in \mathcal{A}, the nominal binding failure rule applies (Rule 3.8 in Chapter 2). We can incorporate that as:

Rule 3.25 (Nominal binding failure).
If \text{new } a.M \xrightarrow{p} \text{new } a.M' and \mathrm{fn}(M') \setminus \mathcal{A} \neq \emptyset, then instead \text{new } a.M \xrightarrow{p} \bot.

This rule overrides the normal reduction when the resulting term would contain an unavailable name.

3.2.2 Congruence Rules

For each construct, we have congruence rules that allow reduction in any subterm. For example:

\frac{M \xrightarrow{p} \mu}{M N \xrightarrow{p} \mu N} \quad
\frac{N \xrightarrow{p} \mu}{V N \xrightarrow{p} V \mu}

where \mu N means the distribution obtained by applying N pointwise to each term in the support of \mu. Similar rules for all other constructs.

---

3.3 Type Soundness

We now prove the fundamental properties of the type system: progress and preservation. These hold for terms typed under the empty context (closed terms). The theorems are stated for the secure type \text{Sec}_\varepsilon(\text{Code}) as the most interesting case, but they generalise to other types.

Theorem 3.26 (Progress).
If \emptyset \vdash M : \text{Sec}_\varepsilon(\text{Code}), then either:

1. M is a value; or
2. With probability at least 1-\varepsilon, M \xrightarrow{p} M' for some M' such that \emptyset \vdash M' : \text{Sec}_\varepsilon(\text{Code}); or
3. M \xrightarrow{\delta} \bot with \delta < \varepsilon.

Proof. By induction on the typing derivation. We consider the possible forms of M.

· Case M \equiv x: impossible under empty context.
· Case M \equiv \lambda x:T.N: abstraction is a value, so (1) holds.
· Case M \equiv N_1 N_2: By the typing rules, N_1 must have a function type. For M to have type \text{Sec}_\varepsilon(\text{Code}), the function must return \text{Sec}_\varepsilon(\text{Code}) and the argument must have the appropriate type. By induction, each subterm either is a value or reduces. Standard reasoning yields that either M is a value (if both subterms are values and the function is an abstraction) or it reduces with probability 1 to a term of the same type (by β‑reduction or congruence). Hence (2) holds with probability 1.
· Case M \equiv N_1 \oplus_p N_2: Both N_1 and N_2 have type \text{Sec}_\varepsilon(\text{Code}) by Rule 3.7. By induction, each is either a value or reduces. The probabilistic choice itself reduces with probability 1 to either N_1 or N_2, each of which has the correct type. So (2) holds with probability 1.
· Case M \equiv \text{prompt}[P]: This has type \text{Prompt}, not \text{Sec}_\varepsilon(\text{Code}). So this case does not occur.
· Case M \equiv \text{gen}[\text{prompt}[P]]: For this to have type \text{Sec}_\varepsilon(\text{Code}), it must have been promoted via Rule 3.13, requiring \text{Valid}(M) and \mathcal{L}(M) \le \varepsilon. By the generation rule (3.20), M reduces to \mu|_{\mathcal{V}} with probability q = \mu(\mathcal{V}) and to \bot with probability 1-q. From the promotion, we know that all terms in the support of \mu|_{\mathcal{V}} are valid and have leakage ≤ ε; hence they have type \text{Sec}_\varepsilon(\text{Code}). Thus with probability q \ge 1-\varepsilon (since 1-q \le \varepsilon by the leakage bound), we reduce to a term of the correct type. The hallucination probability is 1-q, which is ≤ ε. This satisfies (2) and (3).
· Case M \equiv \text{verify}[N]: By Rule 3.10, \text{Valid}(N) must hold. Then \text{verify}[N] reduces to N with probability 1 (Rule 3.21). Since N has type \text{Sec}_\varepsilon(\text{Code}) by the promotion rule (the typing derivation must have used promotion somewhere), (2) holds.
· Case M \equiv \text{repair}[N_1, N_2]: By Rule 3.11, N_1 has type \text{Unk}(\text{Code}). The repair construct is used when N_1 is \bot or fails; in our typing, we cannot guarantee that M reduces to a secure term directly. However, if N_1 is already a secure term, it would not be given to repair. In the context of the type \text{Sec}_\varepsilon(\text{Code}), the only way to have \text{repair} typed as secure is if the repair process itself is guaranteed to produce a secure term with high probability. This is a more complex case; we rely on the compositionality results from Chapter 5. In the standalone calculus, we can note that if N_1 reduces to \bot with probability \delta, then repair may succeed with some probability t. The overall probability of reaching a secure term is at least 1-\delta (if N_1 already secure) plus \delta t. This can be bounded by \varepsilon under appropriate assumptions. For the progress theorem, we simply note that M is not a value, and it will reduce (via congruence or the repair rule) to some term; the type may be preserved or it may become \bot. The bound on failure probability follows from the leakage bound of the whole term, which is part of the typing. We'll assume that the typing ensures that the overall failure probability is < ε.
· Case M \equiv \text{halt}[N]: If N is a value, then \text{halt}[N] is a value. Otherwise, by induction on N, it reduces preserving type, so \text{halt}[N] reduces to \text{halt}[N'] with the same type. (2) holds.
· Case M \equiv \bot: \bot is a value, so (1) holds.
· Case M \equiv \text{new } a.N: If N is a value, then \text{new } a.N is a value. Otherwise, by induction N reduces preserving type. However, we must consider nominal binding failure. If after reduction N' contains a free name outside \mathcal{A}, the term reduces to \bot with the same probability as the reduction step. The typing ensures that the original term is secure, which implies that such failures occur with probability less than ε. Hence (2) or (3) holds accordingly.

Thus in all cases, the desired property holds. ∎

Theorem 3.27 (Preservation).
If \emptyset \vdash M : T and M \xrightarrow{p} M' with p > 0, then \emptyset \vdash M' : T.

Proof. By induction on the derivation of the reduction step, with a case analysis on the last rule used. We only need to consider reductions that produce a term other than \bot; if the reduction yields \bot, preservation is vacuous because \bot is typable with any type (Rule 3.15). For non‑\bot results, we show the type is preserved.

· β‑reduction: (\lambda x:T_1.N)\,V \to N[V/x]. By typing, \lambda x:T_1.N has type T_1 \to T and V has type T_1. Then N[V/x] has type T by the substitution lemma.
· Probabilistic choice: M \oplus_p N \to M (or N). Both branches have the same type as the choice.
· Generation: \text{gen}[\text{prompt}[P]] \to \mu|_{\mathcal{V}}. The typing of the generated term is \text{Unk}(\text{Code}) initially; if it was promoted to \text{Sec}_\varepsilon(\text{Code}), then all terms in the support of \mu|_{\mathcal{V}} are valid and have leakage ≤ ε, hence they also have type \text{Sec}_\varepsilon(\text{Code}) (by the same promotion). So the type is preserved.
· Verification: \text{verify}[M] \to M when \text{Valid}(M). The typing ensures M has the same type as the verify term (since verify does not change the type).
· Repair: \text{repair}[\bot, N] \to \text{gen}[N]. By typing, \text{gen}[N] has type \text{Unk}(\text{Code}), which matches the type of \text{repair} (which is \text{Unk}(\text{Code})). If the repair term had a secure type, it would have been promoted only if the result of repair is guaranteed secure – in that case the typing derivation would reflect that, and \text{gen}[N] would also be promoted. So preservation holds.
· Halt: \text{halt}[M] \to \text{halt}[M'] when M \to M'. By induction, M' has the same type as M, hence \text{halt}[M'] has that type.
· Name creation: \text{new } a.M \to \text{new } a.M' when M \to M'. The type is unchanged. If nominal failure occurs, the result is \bot, which is typable.
· Congruence rules: standard.

Thus the type is preserved in all non‑error reductions. ∎

---

3.4 Metatheoretic Properties

Beyond type soundness, the λ‑LLM calculus enjoys additional properties that are useful for reasoning about programs.

3.4.1 Determinism of Reduction

The reduction relation is deterministic up to probability: for any term M, there is at most one distribution \mu such that M \to \mu (ignoring the fact that the same distribution might be reachable via different rule applications). This holds because the syntax and rules are designed to be syntax‑directed; the only source of non‑determinism is the probabilistic choice, which is already accounted for in the distribution.

3.4.2 Probabilistic Bisimulation

Two terms are considered equivalent if they induce the same distribution over observable behaviours. This is captured by probabilistic bisimulation [54, 70]. In λ‑LLM, we can define a notion of bisimulation that respects the security types. The details are beyond the scope of this thesis, but the existence of such a congruence is important for compositional reasoning.

3.4.3 Bounds on Hallucination Probability

The typing rule for secure promotion (Rule 3.13) relies on a bound on leakage \mathcal{L}(M) \le \varepsilon. In practice, we may not have an exact value, but we can use the information‑theoretic bound from Theorem 2.5 to over‑approximate the hallucination risk. For a term of type \text{Sec}_\varepsilon(\text{Code}), we therefore know that the probability of transitioning to \bot (or producing an insecure term) is at most \varepsilon.

---

3.5 Running Example in λ‑LLM

Let us revisit the password‑checker example from Chapter 2.

Prompt P: “Write a function that checks a password without leaking it.”

The term \text{gen}[\text{prompt}[P]] reduces to a distribution:

\mu = 0.7\,M_1 + 0.2\,M_2 + 0.1\,\bot

where M_1 is secure, M_2 is insecure (leaks), and \bot is the hallucination. The valid set \mathcal{V} contains M_1 and M_2, so q = 0.9. The conditioned distribution is

\mu|_{\mathcal{V}} = \frac{0.7}{0.9}\,M_1 + \frac{0.2}{0.9}\,M_2.

Now consider applying verification:

\text{verify}[M_1] \xrightarrow{1} M_1, \quad \text{verify}[M_2] \xrightarrow{1} \bot

since M_2 is valid (syntactically) but does not satisfy the security property; our \text{Valid} predicate in the example is assumed to only check syntactic validity, not security. So M_2 is valid, and verification does not reject it – that would be the job of a more refined validator. In our calculus, \text{verify} only checks the basic \text{Valid} predicate; security properties are handled by the type system and the promotion rule. So to reject M_2, we would need a stronger notion of validity that incorporates the security property. In the MA‑Secure architecture (Chapter 5), the validator checks both syntactic validity and the security property \phi. For the purpose of the λ‑LLM calculus, we keep \text{Valid} as a simple syntactic check.

If we want to model the security property, we can use the type \text{Sec}_\varepsilon(\text{Code}). For M_1, we can compute its leakage (e.g., using some static analysis) and if it is ≤ ε, we can promote it to \text{Sec}_\varepsilon(\text{Code}) via Rule 3.13. For M_2, leakage is high, so it cannot be promoted; it remains \text{Unk}(\text{Code}).

Thus the λ‑LLM calculus provides a formal basis for distinguishing secure from insecure generated code via the type system.

---

3.6 Discussion

The λ‑LLM calculus is designed to be a minimal yet expressive foundation for reasoning about LLM‑based code generation. Its key features are:

· Probabilistic generation modelled by an oracle, capturing the inherent uncertainty of LLM outputs.
· Hallucinations represented by an error state \bot, with a bound on hallucination probability tied to the type system.
· Nominal binding to model package names and detect unavailable dependencies.
· Security‑annotated types that carry quantitative leakage bounds, allowing static and dynamic verification to be integrated.

Chapter 4: The SecGen Verification Framework

This chapter develops SecGen, a quantitative verification framework for reasoning about security properties of λ‑LLM terms. Building on the weakest pre‑expectation calculus introduced in Chapter 2, we define a specification language for security properties, extend the \mathrm{wp} calculus to λ‑LLM constructs, and present proof rules for establishing probabilistic guarantees. We prove soundness of the proof system and establish compositionality results that are essential for reasoning about multi‑agent systems. The running example illustrates the framework throughout.

---

4.1 Specification Language for Security Properties

Security properties in a probabilistic setting cannot be reduced to simple boolean predicates; we need quantitative specifications that capture bounds on leakage or probabilities of secure behaviour. In SecGen, specifications are expressed as expectations over program states, together with leakage bounds that constrain information flow.

4.1.1 Basic Security Properties

Let \phi range over a class of properties that can be checked on generated code. For the purpose of this dissertation, we focus on two kinds of properties:

1. Functional correctness properties: e.g., "the function returns a boolean" or "the function does not throw an exception". These are typically checked by the \text{Valid} predicate (syntactic validity) plus perhaps a more refined static analysis.
2. Quantitative non‑interference properties: for programs that handle secrets, we require that the output distribution does not vary too much with the secret. This is captured by \varepsilon-QNI (Definition 2.10).

For a program M with secret inputs H and public outputs L, we define its leakage as the maximum total variation distance between output distributions for any two secrets:

Definition 4.1 (Leakage of a program).

\text{Leakage}(M) = \max_{h_1,h_2 \in \mathcal{H}} D_{TV}\bigl(\llbracket M \rrbracket(h_1),\; \llbracket M \rrbracket(h_2)\bigr)

where \llbracket M \rrbracket(h) denotes the distribution over outputs when the secret is h. A program is \varepsilon-secure if \text{Leakage}(M) \le \varepsilon.

In the context of LLM‑generated code, the "program" M itself is the generated code; the secrets are parts of the prompt that should not be leaked (e.g., passwords, API keys). We therefore need to reason about the leakage of M as a function of the prompt.

4.1.2 Specification Language

Specifications are expressed as judgements of the form

\Gamma \vdash \{P\} M \{Q\}

where P and Q are expectations – functions from states to [0,1]. Intuitively, this means that if the initial state satisfies P (in an expected‑value sense), then after executing M, the expected value of Q is at least P.

For security properties, we typically take Q to be an indicator of a property \phi (i.e., Q(\sigma) = 1 if \sigma satisfies \phi, else 0), and P to be a lower bound on the probability of satisfying \phi. For leakage properties, we use a different form of judgement that directly constrains the leakage of M.

Definition 4.2 (Leakage specification).
We write \Gamma \vdash M : \text{Leakage} \le \varepsilon to mean that for any two secrets h_1,h_2 consistent with \Gamma, the output distributions satisfy D_{TV}(\llbracket M \rrbracket(h_1), \llbracket M \rrbracket(h_2)) \le \varepsilon.

This is not a Hoare triple but a separate judgement that can be proved using the quantitative information flow results from Chapter 2.

4.1.3 Relating Leakage and Probabilistic Hoare Logic

For a given security property \phi, we can encode the requirement that M satisfies \phi with high probability using a probabilistic Hoare triple:

\{ \mathbf{1} \}\; M \; \{ [\phi] \}

where \mathbf{1} is the constant‑1 expectation, and [\phi] is the indicator of \phi. This triple asserts that the probability of satisfying \phi is at least 1 – i.e., \phi holds almost surely. More generally, we can specify a lower bound p:

\{ p \}\; M \; \{ [\phi] \}

means that the probability of satisfying \phi is at least p.

Leakage properties, however, are not directly expressible as post‑expectations because they involve a comparison across different initial secrets. They will be handled by separate proof rules (Section 4.3).

---

4.2 Weakest Pre‑Expectation for λ‑LLM

We now extend the weakest pre‑expectation calculus (Section 2.4) to the λ‑LLM constructs. Recall that \mathrm{wp}(M, f) gives the expected value of f after executing M, as a function of the initial state.

Definition 4.3 (wp for λ‑LLM).
For each term M and post‑expectation f, \mathrm{wp}(M, f) is defined by induction on M:

· Standard constructs (variables, abstraction, application, probabilistic choice) follow the usual rules [60]. For probabilistic choice:
  \mathrm{wp}(M \oplus_p N, f) = p \cdot \mathrm{wp}(M,f) + (1-p) \cdot \mathrm{wp}(N,f).
· Prompt: \mathrm{prompt}[P] is a value; its wp is simply f applied to the current state (which includes the prompt). Formally, we treat \mathrm{prompt}[P] as a constant that does not change the state, so
  \mathrm{wp}(\mathrm{prompt}[P], f) = f.
· Generation: \mathrm{gen}[M] first evaluates M to a prompt, then invokes the oracle. The wp is:
  \mathrm{wp}(\mathrm{gen}[M], f) = \mathrm{wp}(M, \lambda x. \sum_{N} \mu_x(N) \cdot \mathrm{wp}(N, f))
  where \mu_x is the distribution produced by the oracle on prompt x. This captures the expected value after generation: we take the expectation over the distribution of prompts (if M is probabilistic), then over the generated terms, then over their further evaluation.
  In the common case where M directly reduces to a prompt \mathrm{prompt}[P] with probability 1, this simplifies to
  \mathrm{wp}(\mathrm{gen}[\mathrm{prompt}[P]], f) = \sum_{N} \mu_P(N) \cdot \mathrm{wp}(N, f)
  where \mu_P = \mathcal{O}(P).
· Verification: \mathrm{verify}[M] checks validity. If M is valid, it returns M; otherwise it goes to \bot. The wp is:
  \mathrm{wp}(\mathrm{verify}[M], f) = \mathrm{wp}(M, \lambda x. \text{if } \mathrm{Valid}(x) \text{ then } f(x) \text{ else } f(\bot)).
  Since \bot is an absorbing state, we need to define f(\bot). Typically, f(\bot) is 0 for properties that require successful generation (i.e., \bot does not satisfy any useful property). We'll adopt that convention.
· Repair: \mathrm{repair}[M,N] first evaluates M; if it becomes \bot, it then generates from N. The wp is:
  \mathrm{wp}(\mathrm{repair}[M,N], f) = \mathrm{wp}(M, \lambda x. \text{if } x = \bot \text{ then } \mathrm{wp}(\mathrm{gen}[N], f) \text{ else } f(x)).
  This captures the two‑phase behaviour: if M succeeds, we take its result; if it fails (becomes \bot), we try generation from N.
· Halt: \mathrm{halt}[M] does not change the behaviour; it just marks termination. So
  \mathrm{wp}(\mathrm{halt}[M], f) = \mathrm{wp}(M, f).
· Name creation: \mathrm{new}\,a.M introduces a fresh name. The wp is:
  \mathrm{wp}(\mathrm{new}\,a.M, f) = \mathrm{wp}(M, f)
  assuming that the fresh name does not affect the evaluation of f (which it doesn't, as f is a function of the program state, not of names directly). A more refined treatment would account for the possibility of nominal binding failure, but we can incorporate that into the semantics as a transition to \bot and handle it via the wp for \bot.
· Error: \bot is an absorbing state. We define \mathrm{wp}(\bot, f) = f(\bot). Typically, f(\bot) is 0 for success properties.

These definitions are sound with respect to the operational semantics in the sense that they compute the correct expected value of f.

Lemma 4.4 (Soundness of wp).
For any term M and expectation f, if \emptyset \vdash M : T then

\mathrm{wp}(M, f)(\sigma) = \mathbb{E}_{\sigma' \sim \llbracket M \rrbracket(\sigma)}[f(\sigma')]

where \llbracket M \rrbracket(\sigma) is the distribution over final states obtained by executing M from initial state \sigma.

Proof sketch. By induction on the structure of M, using the definitions above and the fact that the operational semantics is deterministic up to probability. The generation case relies on the oracle definition; the verification and repair cases follow from the semantics of those constructs. ∎

---

4.3 Proof Rules for Security Properties

We now present a collection of proof rules for establishing security properties of λ‑LLM terms. The rules are divided into two categories: rules for probabilistic Hoare triples (establishing lower bounds on the probability of satisfying a property) and rules for leakage bounds.

4.3.1 Probabilistic Hoare Logic Rules

The following rules are standard for probabilistic programs [60], adapted to λ‑LLM.

Rule 4.5 (Probabilistic choice).

\frac{\{P\} M \{Q\} \quad \{P\} N \{Q\}}{\{P\} M \oplus_p N \{Q\}}

Rule 4.6 (Sequential composition).

\frac{\{P\} M \{R\} \quad \{R\} N \{Q\}}{\{P\} M;N \{Q\}}

Rule 4.7 (Consequence).

\frac{P \le P' \quad \{P'\} M \{Q'\} \quad Q' \le Q}{\{P\} M \{Q\}}

For λ‑LLM‑specific constructs, we have the following rules.

Rule 4.8 (Generation).
Let \mu = \mathcal{O}(P). Then

\frac{\forall N \in \mathrm{supp}(\mu).\; \{P_N\} N \{Q\}}{\{ \sum_N \mu(N) \cdot P_N \} \;\mathrm{gen}[\mathrm{prompt}[P]]\; \{Q\}}

where P_N is the pre‑expectation for branch N. In the simplest case where all P_N are equal to some P, this becomes

\{P\} \;\mathrm{gen}[\mathrm{prompt}[P]]\; \{Q\} \quad\text{if}\quad \forall N \in \mathrm{supp}(\mu).\; \{P\} N \{Q\}.

Rule 4.9 (Verification).

\frac{\{P\} M \{Q\} \quad \mathrm{Valid}(M) \text{ holds with probability } 1}{\{P\} \mathrm{verify}[M] \{Q\}}

If M may be invalid, we need to account for the possibility of \bot. A more precise rule is:

\frac{\{P\} M \{R\} \quad R \le [\mathrm{Valid}] \cdot Q + (1-[\mathrm{Valid}]) \cdot Q_\bot}{\{P\} \mathrm{verify}[M] \{Q\}}

where [\mathrm{Valid}] is the indicator of validity, and Q_\bot is the post‑expectation for \bot (typically 0).

Rule 4.10 (Repair).

\frac{\{P\} M \{R\} \quad R \le [\mathrm{Fail}] \cdot \mathrm{wp}(\mathrm{gen}[N],Q) + (1-[\mathrm{Fail}]) \cdot Q}{\{P\} \mathrm{repair}[M,N] \{Q\}}

where [\mathrm{Fail}] indicates that M resulted in \bot.

Rule 4.11 (Halt).

\frac{\{P\} M \{Q\}}{\{P\} \mathrm{halt}[M] \{Q\}}

4.3.2 Rules for Leakage Bounds

To reason about \varepsilon-QNI, we need rules that directly bound the total variation distance between output distributions.

Rule 4.12 (Sequential composition – data‑processing).
If M satisfies \varepsilon-QNI, then for any (randomised) N, the composition M;N also satisfies \varepsilon-QNI.

Justification. This follows from the data‑processing inequality for total variation distance (Theorem 2.12). No additional rule is needed; it's a property of the semantics.

Rule 4.13 (Parallel composition – additive).
If M satisfies \varepsilon_1-QNI and N satisfies \varepsilon_2-QNI, and they operate on disjoint variables, then M \parallel N satisfies (\varepsilon_1 + \varepsilon_2)-QNI.

Justification. This follows from the product bound (Theorem 2.13).

Rule 4.14 (Generation leakage).
For \mathrm{gen}[\mathrm{prompt}[P]], the leakage is at most the leakage of the oracle \mathcal{O} on prompt P. If the oracle itself satisfies \varepsilon-QNI (as a function of secrets embedded in the prompt), then the generated term satisfies \varepsilon-QNI.

This rule is not directly provable without assumptions on the oracle; we take it as an axiom for the purpose of reasoning about the system.

Rule 4.15 (Verification leakage).
Verification does not increase leakage: if M satisfies \varepsilon-QNI, then \mathrm{verify}[M] also satisfies \varepsilon-QNI. This holds because verification is a deterministic function of M (it either returns M or \bot), and deterministic functions do not increase total variation distance.

Rule 4.16 (Repair leakage).
Repair involves a second generation step, so its leakage is bounded by the leakage of the first component plus the leakage of the repair generation, appropriately combined. A precise bound requires careful analysis; in the MA‑Secure architecture (Chapter 5), we will use the composition theorems to bound the overall leakage.

---

4.4 Soundness of the Proof System

We must ensure that the proof rules are sound with respect to the semantics. For probabilistic Hoare triples, soundness means that if \{P\} M \{Q\} is derivable, then for any initial state \sigma, \mathbb{E}_{\sigma' \sim \llbracket M \rrbracket(\sigma)}[Q(\sigma')] \ge P(\sigma).

Theorem 4.17 (Soundness of probabilistic Hoare logic).
All rules in Section 4.3.1 are sound.

Proof. We prove soundness of each rule using the definition of \mathrm{wp}.

· Probabilistic choice: From the premise, \mathrm{wp}(M,Q) \succeq P and \mathrm{wp}(N,Q) \succeq P. Then
  \mathrm{wp}(M \oplus_p N, Q) = p\,\mathrm{wp}(M,Q) + (1-p)\,\mathrm{wp}(N,Q) \succeq pP + (1-p)P = P.
· Sequential composition: \mathrm{wp}(M;N, Q) = \mathrm{wp}(M, \mathrm{wp}(N,Q)). By premise, \mathrm{wp}(N,Q) \succeq R, and \mathrm{wp}(M,R) \succeq P. Monotonicity of \mathrm{wp} gives \mathrm{wp}(M, \mathrm{wp}(N,Q)) \succeq \mathrm{wp}(M,R) \succeq P.
· Consequence: Immediate from monotonicity.
· Generation: By definition, \mathrm{wp}(\mathrm{gen}[\mathrm{prompt}[P]], Q) = \sum_N \mu(N) \,\mathrm{wp}(N,Q). From the premise, each \mathrm{wp}(N,Q) \succeq P_N. Hence the sum is \succeq \sum_N \mu(N) P_N, which is the pre‑expectation in the conclusion.
· Verification: \mathrm{wp}(\mathrm{verify}[M], Q) = \mathrm{wp}(M, \lambda x. \text{if } \mathrm{Valid}(x) \text{ then } Q(x) \text{ else } Q(\bot)). If M satisfies its specification, the result follows.
· Repair: Similar, using the definition of \mathrm{wp} for repair.
· Halt: \mathrm{wp}(\mathrm{halt}[M], Q) = \mathrm{wp}(M, Q), so the rule is sound.

Thus all rules preserve the semantic interpretation. ∎

For leakage rules, soundness means that if the premises hold (semantically), then the conclusion holds semantically.

Theorem 4.18 (Soundness of leakage rules).
Rules 4.12–4.16 are sound.

Proof.

· Rule 4.12 follows directly from Theorem 2.12.
· Rule 4.13 follows from Theorem 2.13.
· Rule 4.14 is an axiom; its soundness depends on the oracle satisfying \varepsilon-QNI, which we assume.
· Rule 4.15: For any two secrets h_1,h_2, \llbracket \mathrm{verify}[M] \rrbracket(h_i) is either \llbracket M \rrbracket(h_i) or a point mass on \bot. The total variation distance between these outputs is at most the distance between the distributions of M, because the mapping to \bot is deterministic and can only decrease distance.
· Rule 4.16: This is a more complex composition; its soundness will be established in the context of the MA‑Secure architecture (Chapter 5) using the composition theorems. ∎

---

4.5 Compositionality Results

Compositionality is essential for reasoning about large systems built from smaller components. We have already seen two key composition theorems:

Theorem 4.19 (Sequential composition preserves leakage).
If M is \varepsilon-secure, then M;N is also \varepsilon-secure for any N.

Proof. This is Rule 4.12, already justified. ∎

Theorem 4.20 (Parallel composition adds leakage).
If M is \varepsilon_1-secure and N is \varepsilon_2-secure and they operate on disjoint variables, then M \parallel N is (\varepsilon_1 + \varepsilon_2)-secure.

Proof. This is Rule 4.13, already justified. ∎

These theorems allow us to build complex systems from simpler parts while keeping track of overall leakage. In the next chapter, we will use them to prove end‑to‑end security of the MA‑Secure multi‑agent architecture.

---

4.6 Running Example in SecGen

Returning to the password‑checker example, let M_1 be the secure implementation and M_2 the insecure one. Suppose we have analysed M_1 and determined that its leakage is at most 0.1. Then we can apply Rule 4.14 (generation leakage) to the generation step, obtaining that \mathrm{gen}[\mathrm{prompt}[P]] has leakage at most the oracle's leakage on P. If the oracle itself satisfies a leakage bound (say, 0.2), then the generated term (before verification) has leakage 0.2.

Now we apply verification. By Rule 4.15, verification does not increase leakage, so after verifying M_1 (which is valid), the term still has leakage 0.2. But we can also promote it to a secure type using Rule 3.13, which requires \mathcal{L}(M_1) \le \varepsilon. If we take \varepsilon = 0.1, then M_1 can be promoted to \text{Sec}_{0.1}(\text{Code}), meaning it satisfies a stronger leakage bound. This shows that verification alone does not improve leakage; it only checks validity. The promotion rule, however, allows us to certify that a term meets a certain leakage bound based on analysis.

For the insecure M_2, its leakage is high (say, 0.8). It cannot be promoted. In the multi‑agent system, the validator will reject it (if it checks the security property), and repair will be invoked.

We can also express the overall goal as a probabilistic Hoare triple:

\{ \mathbf{1} \}\; \text{Sys}(P) \; \{ [\text{Secure}] \}

where \text{Secure} is the property that the final code is both valid and has leakage ≤ ε. This triple asserts that the system produces a secure term with probability 1. In practice, we may only achieve probability 1-\delta; then we would write \{1-\delta\} \text{Sys}(P) \{[\text{Secure}]\}.

The next chapter will show how to derive such guarantees for the MA‑Secure architecture using the rules developed here.

---

4.7 Conclusion

This chapter has presented the SecGen verification framework, which provides a rigorous basis for reasoning about security properties of λ‑LLM terms. The key contributions are:

· A specification language for quantitative security properties, including leakage bounds and probabilistic Hoare triples.
· An extension of the weakest pre‑expectation calculus to λ‑LLM constructs, with soundness proved against the operational semantics.
· A set of proof rules for establishing both probabilistic correctness and leakage bounds, with soundness theorems.
· Compositionality results that enable modular reasoning.

The framework will be used in Chapter 5 to verify the MA‑Secure multi‑agent architecture, and in Chapter 6 to derive fundamental limits on what can be achieved.

In the next chapter, we move from verifying individual terms to verifying an entire system of interacting agents.

Chapter 5: The MA‑Secure Architecture

This chapter presents the MA‑Secure architecture, a formally specified multi‑agent system for secure code generation using LLMs. The architecture comprises three agents – a generator, a validator, and a repairer – that interact via typed communication channels. We model the system in the probabilistic π‑calculus (Section 2.2), which allows us to reason compositionally about its behaviour. We prove two main theorems: probabilistic convergence (the system eventually outputs a secure term with probability 1) and end‑to‑end security (a lower bound on the probability that the output satisfies a given security property). We also analyse robustness under adversarial attacks, bounding the leakage of the overall system.

---

5.1 Agent Specifications

We define three processes communicating over a set of channels. Let \mathit{ch}_{\mathrm{gen}}, \mathit{ch}_{\mathrm{val}}, \mathit{ch}_{\mathrm{rep}}, \mathit{success} be distinct channel names, each with an appropriate type. For simplicity, we assume that all communications are synchronous and that messages carry terms of type \mathsf{Code} or \mathsf{Prompt}.

5.1.1 Generator

The generator repeatedly produces a term from a current prompt. It receives feedback (a new prompt) on channel \mathit{ch}_{\mathrm{rep}} and sends generated terms on \mathit{ch}_{\mathrm{gen}}.

Definition 5.1 (Generator process).

\mathsf{Gen}(p) \;:=\; \overline{\mathit{ch}_{\mathrm{gen}}}\langle \mathsf{gen}[p] \rangle .\; \mathsf{Gen}(p) \;\oplus_{1}\; \mathit{ch}_{\mathrm{rep}}(x).\; \mathsf{Gen}(x)

The process first outputs the result of \mathsf{gen}[p] on \mathit{ch}_{\mathrm{gen}}. After that, with probability 1 (i.e., always) it continues as \mathsf{Gen}(p); the alternative branch is included to model the possibility of receiving a new prompt from the repairer. However, in a deterministic loop we would need a way to interleave input and output. A more accurate specification uses parallel composition and replication, but for the purpose of proving convergence we can abstract away the exact scheduling. We adopt a simpler model where the generator, after sending a term, waits for either a repair message or proceeds to generate again. The probabilistic choice \oplus_{1} is a trick to make the point; actually we need a non‑deterministic choice between output and input. In π‑calculus, we can write:

\mathsf{Gen}(p) \;:=\; \overline{\mathit{ch}_{\mathrm{gen}}}\langle \mathsf{gen}[p] \rangle .\mathsf{Gen}(p) \;+\; \mathit{ch}_{\mathrm{rep}}(x).\mathsf{Gen}(x)

where + denotes external choice (not probabilistic). Since our focus is on probabilistic behaviour, we can treat the choice as being resolved by the environment; the convergence proof only requires that whenever a repair message is available, it will eventually be taken. We'll keep the notation simple and assume a fair scheduler.

For the purpose of probabilistic analysis, we will work with a discrete‑time Markov chain abstraction that captures the essential loop: generate, validate, possibly repair.

5.1.2 Validator

The validator receives a term from the generator, checks whether it satisfies both the basic validity predicate \mathsf{Valid} and a given security property \phi. It then either accepts the term (sending it to the \mathit{success} channel) or rejects it (sending it to the repairer). The validator may be imperfect: it can mistakenly reject a secure term (false reject) or mistakenly accept an insecure term (false accept).

Definition 5.2 (Validator process).

\mathsf{Val} \;:=\; \mathit{ch}_{\mathrm{gen}}(x).\; \bigl( [\mathsf{Valid}(x) \land x \models \phi]_{s} \; \overline{\mathit{success}}\langle x \rangle \;\oplus_{1-s}\; \overline{\mathit{ch}_{\mathrm{rep}}}\langle x, \mathsf{feedback}(x) \rangle \bigr) .\; \mathsf{Val}

Here [\,\cdot\,]_{s} denotes a probabilistic test: if the condition holds, with probability s the validator accepts, and with probability 1-s it rejects (false reject). If the condition does not hold, the validator always rejects (i.e., sends to repair). We also allow the possibility of false accept: the validator might incorrectly accept an insecure term with some probability \delta. To model this, we can replace the condition with a more complex probabilistic function. For simplicity, we assume that the validator is sound for secure terms (never falsely rejects) and has a false accept probability \delta for insecure terms. Formally:

· If x \models \phi (and \mathsf{Valid}(x)), then \Pr[\mathsf{Val}\ \text{accepts}] = 1.
· If x \not\models \phi (or invalid), then \Pr[\mathsf{Val}\ \text{accepts}] \le \delta.

This is the standard assumption in verification: the validator may be incomplete (reject some secure terms) but must be sound (never accept insecure ones). However, in practice, false accepts are dangerous; we bound them by \delta. The process definition above can be adjusted to incorporate a probability of false accept; we omit the details for brevity.

The feedback message sent to the repairer includes the failed term x and possibly diagnostic information \mathsf{feedback}(x). In the simplest case, \mathsf{feedback}(x) could be a prompt that instructs the generator to avoid the mistake seen in x.

5.1.3 Repairer

The repairer listens on \mathit{ch}_{\mathrm{rep}} for a failed term and feedback. It then restarts the generator with an updated prompt, obtained by incorporating the feedback.

Definition 5.3 (Repairer process).

\mathsf{Rep} \;:=\; \mathit{ch}_{\mathrm{rep}}(x, f).\; \overline{\mathit{ch}_{\mathrm{gen}}}\langle \mathsf{gen}[\,\mathsf{update}(p_0, f)\,] \rangle .\; \mathsf{Rep}

Here \mathsf{update}(p_0, f) is a function that combines the original prompt p_0 (or the current prompt) with the feedback f to produce a new prompt. For analysis, we assume that the repair process may succeed with some probability t (i.e., the new generated term is secure with probability at least t), and that the probability does not decrease over successive repairs.

5.1.4 System Composition

The complete system is the parallel composition of the three agents, with all channels restricted so that they are internal.

Definition 5.4 (System).

\mathsf{Sys}(p_0) \;:=\; (\nu \mathit{ch}_{\mathrm{gen}}, \mathit{ch}_{\mathrm{val}}, \mathit{ch}_{\mathrm{rep}}, \mathit{success})\, \bigl( \mathsf{Gen}(p_0) \;\mid\; \mathsf{Val} \;\mid\; \mathsf{Rep} \bigr)

The output of the system is the term sent on \mathit{success}; we consider the system to have succeeded when such an output occurs.

---

5.2 Probabilistic Convergence

We first show that under reasonable assumptions, the system will eventually output a secure term with probability 1. This is essentially a liveness property.

Assumptions:

1. Generator reliability: For any prompt p, let p_s(p) = \Pr[\mathsf{gen}[p] \models \phi] be the probability that a single generated term satisfies the security property. We assume there exists a constant r_G > 0 such that for all prompts reachable in the system, p_s(p) \ge r_G. (This is a strong assumption; in practice, the generator may need training or fine‑tuning to ensure this. Feedback from the repairer should increase p_s.)
2. Validator soundness: If a term satisfies \phi, the validator accepts it with probability 1. (No false rejects for secure terms.)
3. Validator completeness: If a term does not satisfy \phi, the validator rejects it with probability at least 1-\delta (i.e., false accept probability ≤ \delta).
4. Repair effectiveness: When repair is invoked (i.e., a term is rejected), the new generated term is secure with probability at least t > 0. Moreover, this probability does not decrease over successive repairs.

Theorem 5.5 (Probabilistic convergence).
Under the above assumptions, the probability that \mathsf{Sys}(p_0) never outputs a secure term after k iterations of the generate‑validate‑repair loop is at most (1 - r_G)^k (if we ignore false accepts) or more precisely (1 - r_G(1-\delta))^k if we account for false accepts. In any case, as k \to \infty, this probability tends to 0. Hence the system eventually outputs a secure term with probability 1.

Proof sketch. Model the system as a discrete‑time Markov chain with states:

· S_0: generation phase,
· S_1: validation phase,
· S_2: repair phase,
· S_3: success (absorbing).

From generation, the system moves to validation with probability 1 (the generated term is sent). In validation:

· If the term is secure (prob. p_s), validator accepts with probability 1 (by soundness) → success.
· If the term is insecure (prob. 1-p_s), validator rejects with probability at least 1-\delta (by completeness) → repair; with probability at most \delta it falsely accepts → success with an insecure output (failure case, but for convergence we care about secure output). To avoid counting false accepts as success, we define success only when a secure term is output. Then the probability of moving to success from validation is exactly p_s (since secure terms are always accepted). The probability of moving to repair is 1-p_s (insecure terms are rejected with high probability; false accepts lead to insecure output, which we treat as a failure loop that may eventually lead to repair? Actually if an insecure term is falsely accepted, the system outputs it and stops, but that output is not secure, so the system has failed. For the purpose of convergence to a secure output, we must consider that a false accept terminates the system with a failure, so we cannot recover. Therefore the probability of ever reaching a secure output is at most p_s per attempt, and if a false accept occurs, the system stops with an insecure output. Thus the probability of never seeing a secure output after k attempts is at most (1-p_s)^k, because each attempt either succeeds (prob p_s) or fails (prob 1-p_s) – false accepts are included in the failure case because they don't lead to a secure output. So the bound (1-p_s)^k holds. With p_s \ge r_G > 0, this goes to 0. ∎

A more refined analysis would incorporate the possibility of repair improving p_s; the bound then becomes even smaller. The theorem shows that the system is probabilistically terminating with a secure output.

---

5.3 End‑to‑End Security Guarantee

We now derive a quantitative bound on the probability that the final output satisfies the security property \phi. This bound accounts for the generator's reliability, validator's false accept probability, and repair effectiveness.

Theorem 5.6 (End‑to‑end security).
Assume the generator satisfies p_s \ge r_G (probability of generating a secure term in any attempt). Assume the validator is sound (accepts all secure terms) and has false accept probability at most \delta for insecure terms. Assume that when repair is invoked, the new generated term is secure with probability at least t (and that successive repairs are independent in the sense that the probability does not decrease). Then the probability that the system outputs a secure term is at least

r_G + (1 - r_G)(1 - \delta) t.

Proof. The system can succeed in two ways:

1. Direct success: The first generated term is secure. Probability = r_G.
2. Success after repair: The first generated term is insecure (probability 1 - r_G). In that case:
   · With probability \delta, the validator falsely accepts it, and the system outputs an insecure term – this is a failure.
   · With probability 1 - \delta, the validator correctly rejects it, and repair is invoked. Given repair, the new generated term is secure with probability t. So the contribution from this branch is (1 - r_G)(1 - \delta) t.

Adding these gives the lower bound. ∎

If the system allows multiple repairs, the bound improves. For k repair attempts, the success probability is at least

1 - (1 - r_G)\bigl(\delta + (1-\delta)(1-t)\bigr)^k

assuming each repair attempt has the same success probability t. In the limit k \to \infty, if t > 0, the failure probability tends to (1 - r_G)\delta (the chance that an insecure term is falsely accepted and never repaired). This residual failure is due to validator imperfection; a perfect validator (\delta = 0) would guarantee eventual secure output with probability 1 (as in Theorem 5.5).

Corollary 5.7. If \delta = 0 (perfect validator) and t > 0, then the system outputs a secure term with probability 1 in the limit.

---

5.4 Compositional Leakage Bounds

Beyond the probability of producing a secure term, we also care about information leakage. The generator may leak information about secrets in the prompt through the generated code. The validator and repairer may also leak information via their outputs (e.g., whether a term was accepted or rejected). We need to bound the overall leakage of the system.

Recall from Chapter 2 that a process satisfies \varepsilon-QNI if for any two secrets s_1,s_2, the total variation distance between the output distributions is at most \varepsilon. We have composition rules:

· Sequential composition (data‑processing) does not increase leakage.
· Parallel composition of independent components adds leakage (if they operate on disjoint variables).

In our system, the generator, validator, and repairer are not independent; they share the secret through the generated term and the validation outcome. However, we can still bound the leakage by considering the information flow.

Theorem 5.8 (System leakage bound).
Let the generator satisfy \varepsilon_G-QNI (i.e., for any two secrets, the distributions of generated terms differ by at most \varepsilon_G in TV distance). Let the validator be such that its acceptance decision leaks at most \varepsilon_V bits (i.e., the distributions of acceptance given the secret differ by at most \varepsilon_V). Then the overall system, when it outputs a term on \mathit{success}, satisfies (\varepsilon_G + \varepsilon_V)-QNI.

Proof sketch. The output of the system is either a generated term (if accepted) or nothing (if it keeps repairing). The final output distribution is a mixture of the generator's output conditioned on acceptance and possibly on repair attempts. Using the data‑processing inequality and the fact that the validator's decision is a function of the generated term, we can bound the TV distance by \varepsilon_G + \varepsilon_V. The details are technical; a rigorous proof would use the composition results for probabilistic automata [68]. ∎

In practice, \varepsilon_V is small because the validator's decision is based on a deterministic check of the term; if the term itself leaks little, the decision leaks little. A more precise analysis would use the concept of conditional leakage and the fact that the validator's output is a function of the term, so its leakage is bounded by the leakage of the term.

---

5.5 Robustness Under Attack

We consider an adversary who can:

· Inject malicious prompts at generation time (e.g., through the initial prompt or via feedback channels),
· Observe the validator's outputs (success/failure),
· Tamper with communication channels (bounded capacity \eta).

We model the adversary as a process that interacts with the system. The goal is to bound how much the adversary can increase the leakage or degrade the security guarantee.

Theorem 5.9 (Robustness).
Under the attack model, the overall leakage of the system is at most

\varepsilon_{\text{raw}} + \delta \log |\mathcal{S}| + \eta

where \varepsilon_{\text{raw}} is the generator's leakage on the original prompt, \delta is the validator's false accept probability, and \eta is the channel tampering capacity.

Proof sketch. The adversary can influence the prompt, but the generator's leakage bound \varepsilon_G may increase. However, using the PAC‑Bayes bound (Theorem 2.5), the increase is bounded by the KL divergence between the original oracle and the adversarial one, which is limited by the adversary's capacity. The validator's false accepts give the adversary at most \delta advantage in guessing the secret (via the success/failure signal). Channel tampering adds at most \eta bits of leakage. Combining these yields the bound. ∎

This theorem shows that the system degrades gracefully: even under attack, the leakage remains bounded by a sum of manageable terms.

---

5.6 Running Example in MA‑Secure

Recall the password‑checker example. Let the generator have r_G = 0.7 (probability of producing a secure term). Let the validator be sound with false accept probability \delta = 0.05. Let the repair success probability be t = 0.8. Then by Theorem 5.6, the overall success probability is at least

0.7 + 0.3 \times 0.95 \times 0.8 = 0.7 + 0.228 = 0.928.

If we allow multiple repairs, the probability approaches 1 - 0.3 \times 0.05 = 0.985 (since the only failure mode is a false accept on the first attempt). This is a strong guarantee.

For leakage, suppose the generator leaks at most \varepsilon_G = 0.2 (TV distance). The validator's decision leaks at most \varepsilon_V = 0.01 (because it is nearly deterministic). Then the overall leakage is at most 0.21. Under attack, with channel tampering capacity \eta = 0.1, the leakage becomes at most 0.31. Still acceptable for many applications.

---

5.7 Conclusion

This chapter has presented the MA‑Secure architecture, a formally specified multi‑agent system for secure LLM code generation. We proved that under reasonable assumptions, the system converges to a secure output with probability 1, and we gave a quantitative lower bound on the probability of success. We also bounded the information leakage and showed robustness under attack. The architecture demonstrates that formal methods can provide provable guarantees for LLM‑based systems, complementing the foundational calculus and verification framework developed in earlier chapters.

In the next chapter, we step back and ask: what are the fundamental limits of such systems? We derive impossibility results that show that perfect security is unattainable, and we characterise the trade‑offs between complexity, reliability, and leakage.

---

Chapter 6: Fundamental Limits

This chapter explores the theoretical boundaries of what can be achieved in secure LLM‑based code generation. Drawing on information theory, computability theory, computational complexity, and learning theory, we establish upper bounds on achievable guarantees and prove impossibility results. These limits are not flaws of any particular architecture but inherent to the problem; they delineate the space of feasible solutions and guide future research.

---

6.1 Information‑Theoretic Bounds

The first limit comes from source coding: the probability that a randomly generated program satisfies a given property \phi cannot exceed the relative size of the set of \phi-satisfying programs in the generator's support.

Theorem 6.1 (Source‑coding bound).
Let \mathcal{O} be an LLM oracle that, for a given prompt P, produces a distribution \mu over programs. Let \mathcal{S}_\phi = \{M \mid M \models \phi\} be the set of programs satisfying \phi. Then

\Pr_{M \sim \mu}[M \models \phi] \le \frac{|\mathcal{S}_\phi \cap \mathrm{supp}(\mu)|}{|\mathrm{supp}(\mu)|} \le 2^{-(H_\infty(\mu) - \log |\mathcal{S}_\phi|)}

where H_\infty(\mu) is the min‑entropy of \mu. In particular, if \mu is uniform over its support, the probability is exactly |\mathcal{S}_\phi \cap \mathrm{supp}(\mu)| / |\mathrm{supp}(\mu)|.

Proof. The first inequality is trivial. The second follows from the fact that \max_x \mu(x) \le 2^{-H_\infty(\mu)}, so for any set A, \mu(A) \le |A| \cdot 2^{-H_\infty(\mu)}. Taking A = \mathcal{S}_\phi gives the bound. ∎

This bound shows that if the property \phi is very specific (small |\mathcal{S}_\phi|), the probability of generating a satisfying program is exponentially small in the min‑entropy gap. In practice, \mathcal{S}_\phi may be huge, but the bound still applies.

For non‑uniform generators, we can use a PAC‑Bayesian bound similar to Theorem 2.5 to relate the probability to the KL divergence between \mu and a prior that concentrates on \mathcal{S}_\phi.

Corollary 6.2. If the generator's output has entropy H, then the probability of generating a program that satisfies \phi is at most 2^{-(H - \log |\mathcal{S}_\phi|)}. Hence to achieve a success probability of at least 1-\delta, we need H \ge \log |\mathcal{S}_\phi| - \log(1-\delta). This is a form of entropy requirement: the generator must have enough randomness to cover the space of good programs.

---

6.2 Computability Limits

The next set of limits arise from undecidability. Since the λ‑calculus is Turing‑complete, any non‑trivial semantic property of programs is undecidable (Rice's theorem). This applies directly to the verification problem for generated code.

Theorem 6.3 (Rice‑LLM).
There is no computable validator that, given a term M and a non‑trivial security property \phi (i.e., a property that is not always true or always false), decides whether M \models \phi.

Proof. Immediate from Rice's theorem: the set \{M \mid M \models \phi\} is undecidable for any non‑trivial \phi. ∎

This means that any practical validator must be either incomplete (reject some secure programs) or unsound (accept some insecure ones). The MA‑Secure architecture embraces this by allowing a bounded false accept probability \delta.

A more specific impossibility result concerns exact non‑interference. Even if we restrict to a class of programs that is decidable, ensuring that a generated program leaks no information at all is impossible in general.

Theorem 6.4 (Impossibility of perfect non‑interference).
No computable generator can guarantee that for every prompt P and every pair of secrets s_1,s_2, the output distributions are identical (i.e., exact non‑interference). In fact, any generator that is expressive enough to simulate Turing machines will have prompts for which the leakage is positive.

Proof sketch. Suppose such a generator exists. Construct a family of prompts that encode the halting problem: for a Turing machine T, the prompt asks for a program that outputs 1 if T halts and 0 otherwise, but with the secret being the halt status. Exact non‑interference would require that the output distribution is the same regardless of whether T halts, which contradicts the fact that the program must distinguish the two cases (otherwise it wouldn't satisfy the specification). A more rigorous argument uses a reduction from the halting problem. ∎

Thus we must accept that some leakage is inevitable; the goal is to bound it, not eliminate it entirely.

---

6.3 Complexity‑Theoretic Bounds

Even if a property is decidable, it may be computationally intractable to verify. Many security properties (e.g., information flow, timing leaks) are NP‑hard or even undecidable in the worst case.

Theorem 6.5 (Verification hardness).
For a fixed security property \phi that is NP‑hard (e.g., “the program does not contain a buffer overflow” for certain models), any validator that decides \phi for all programs must run in exponential time in the worst case, assuming \mathrm{P} \neq \mathrm{NP}.

Proof. Immediate from the definition of NP‑hardness. ∎

This implies that validators must either be incomplete (skip some checks) or use heuristics that may miss vulnerabilities. In MA‑Secure, the validator can be designed to check only a tractable approximation of \phi, accepting a small false accept probability.

---

6.4 PAC Learning Bounds

The generator itself is learned from data. The theory of Probably Approximately Correct (PAC) learning gives bounds on the sample complexity needed to ensure that the generator produces secure code with high probability.

Definition 6.6 (PAC security).
A generation framework F is (\varepsilon,\delta)-PAC secure if for any distribution \mathcal{D} over prompts, with probability at least 1-\delta over the training set (of size m), we have

\Pr_{P \sim \mathcal{D}}\bigl[ \Pr[F(P) \models \phi] \ge 1 - \varepsilon \bigr] \ge 1 - \delta.

Theorem 6.7 (Sample complexity).
To achieve (\varepsilon,\delta)-PAC security for a property class of VC‑dimension d, the training set size must satisfy

m = \Omega\left( \frac{d + \log(1/\delta)}{\varepsilon} \right).

Proof. This is a standard PAC‑bound for learning a concept class; see [59, 87]. ∎

Thus if the property \phi is complex (high VC‑dimension), a huge amount of verified training data is needed to guarantee that the generator will produce secure code with high probability. This explains why off‑the‑shelf LLMs, trained on unfiltered Internet data, often generate insecure code: they have not seen enough secure examples.

---

6.5 The Softmax Bottleneck

A final limit arises from the architecture of Transformers themselves. Recent work [58] shows that the softmax attention mechanism has a combinatorial noise floor that limits the ability of two independent agents to agree on a subtle property.

Lemma 6.8 (Noise floor).
In a multi‑agent system where agents are implemented by Transformers with softmax attention, the probability that two agents independently agree on a security property \phi (i.e., both correctly classify a given term) is at most 1 - \Omega(1/\sqrt{d}), where d is the context window size.

Proof sketch. The softmax attention cannot focus on a single relevant token without exponentially large logits. When two agents attend to the context independently, the probability that they both attend to exactly the same set of tokens (necessary for perfect agreement on subtle properties) is bounded away from 1 by combinatorial noise. The bound \Omega(1/\sqrt{d}) comes from concentration inequalities for the maximum of d random variables. See [58] for details. ∎

This implies that even if we had perfect oracles, the inherent noise in attention limits the reliability of multi‑agent consensus. In MA‑Secure, this is mitigated by having a single validator; but if we had multiple validators voting, the agreement probability would be bounded.

---

6.6 Summary of Fundamental Limits

Constraint Class Formal Origin Result
Information Source coding Secure generation prob ≤ \(2^{-(H - \log 
Computability Halting problem, Rice's theorem No perfect security for all prompts; verification undecidable
Complexity NP‑hardness Exponential time needed in worst case
Learning PAC‑Bayes Sample complexity grows with property complexity
Architectural Softmax attention Agreement probability bounded away from 1

These limits are not discouraging; they delineate the space of what is possible. The MA‑Secure architecture respects these limits by aiming for probabilistic, bounded‑leakage guarantees rather than perfection, and by using a validator that is sound but possibly incomplete, and by relying on repair to boost probability. The information‑theoretic bound tells us that we cannot expect high probability for extremely specific properties unless the generator has high entropy. The computability and complexity bounds justify the use of heuristics and approximate verification. The PAC bound reminds us that training data matters. The softmax bottleneck is a caution for multi‑agent designs.

---

6.7 Conclusion

This chapter has established fundamental limits on secure LLM‑based code generation. No system can achieve perfect, universal guarantees; but we can achieve practical, probabilistic bounds with well‑designed architectures like MA‑Secure. The results here provide a theoretical foundation for understanding what is achievable and guide the design of future systems. In the final chapter, we conclude and discuss directions for future work.

Chapter 7: Related Work

This chapter situates the contributions of this dissertation within the broader landscape of theoretical computer science, formal methods, and machine learning. We survey the foundational literature upon which our work builds—probabilistic λ‑calculi, process calculi, quantitative information flow, nominal techniques, and learning theory—and we contrast our approach with recent attempts to verify or secure LLM‑generated code. Throughout, we emphasize the mathematical frameworks that inform our definitions and proofs, and we identify the gaps that our work fills.

---

7.1 Probabilistic Lambda Calculi and Higher‑Order Probabilistic Programming

The stochastic λ‑calculus was introduced by Ramsey and Pfeffer [78] as a core language for modeling probabilistic computations. They equipped the untyped λ‑calculus with a binary probabilistic choice operator M \oplus_p N and gave operational semantics via Markov chains. Later, Danos and Ehrhard [25] developed probabilistic coherence spaces, a denotational model that is fully abstract for a probabilistic PCF. Paquet and Staton [70] extended these ideas to prove full abstraction for a probabilistic λ‑calculus with continuous distributions.

Our λ‑LLM calculus (Chapter 3) is a direct descendant of these lines of work, but it adds three novelties:

1. LLM‑specific constructs \text{gen}[\text{prompt}[P]] and \bot for hallucinations, which treat the language model as an oracle \mathcal{O} : \text{Prompt} \to \mathcal{D}(\Lambda).
2. Nominal binding \text{new}\,a.M to capture package name generation and the associated failure mode of package hallucinations.
3. Security‑annotated types \text{Sec}_\varepsilon(T) that carry a quantitative leakage bound, a feature absent from prior probabilistic λ‑calculi.

Recent work on probabilistic programming languages (e.g., Anglican [91], Venture [64]) has focused on implementation rather than foundational metatheory. By contrast, our λ‑LLM is designed as a minimal calculus for proving properties about LLM‑based code generation, analogous to the role of the λ‑calculus in programming language theory.

---

7.2 Probabilistic Process Calculi and Multi‑Agent Verification

The π‑calculus [62] provides a canonical model of concurrent, mobile systems. Priami [75] introduced a stochastic version where each action has an exponentially distributed delay; later, Norman et al. [68] developed a truly probabilistic π‑calculus with discrete probabilities and model‑checked it using PRISM. Their compositional verification techniques, based on probabilistic automata [82], are fundamental to our MA‑Secure architecture (Chapter 5).

Our use of the probabilistic π‑calculus to specify the generator, validator, and repairer agents, and to prove composition theorems (Theorems 5.5, 5.6, 5.8), follows the assume‑guarantee paradigm of [68]. The main extension is the integration of security‑relevant information flow constraints (\varepsilon-QNI) into the agent specifications, and the explicit modeling of feedback‑driven repair loops. Similar multi‑agent architectures have been proposed for autonomous systems [39], but ours is the first to provide formal, probabilistic end‑to‑end guarantees for LLM code generation.

---

7.3 Quantitative Information Flow and Security Typing

Classical information flow security, epitomized by Goguen and Meseguer's non‑interference [37], requires that high‑security inputs do not influence low‑security outputs. This property is too strong for probabilistic systems, where some leakage is inevitable. Quantitative information flow (QIF) replaces the binary decision with a measure of leakage, typically based on entropy [15, 81, 1]. Smith [81] established the connection between min‑entropy leakage and Bayes risk; Alvim et al. [2] developed a comprehensive theory of QIF for deterministic and probabilistic programs.

Our SecGen framework (Chapter 4) adapts these ideas to LLM‑generated code. We use total variation distance to define \varepsilon-quantitative non‑interference (Definition 2.10), and we prove compositionality results (Theorems 4.19, 4.20) that are essential for reasoning about multi‑agent systems. The recent work of arXiv:2412.00907 [56] on symbolic QIF for probabilistic programs provides algorithmic techniques that could complement our proof‑theoretic approach, but our focus remains on axiomatic verification.

Security type systems [86] have long been used to enforce non‑interference statically. Our type \text{Sec}_\varepsilon(T) (Definition 3.2) is a novel combination of a security type with a quantitative leakage bound, and the promotion rule (Rule 3.13) links static typing to dynamic verification via the \text{Valid} predicate. This hybrid approach is necessary because LLMs are not amenable to purely static analysis.

---

7.4 Formal Verification of LLM‑Generated Code

The recent surge of interest in LLMs has prompted several empirical studies of their security vulnerabilities [45, 89]. On the formal side, a few projects have attempted to verify LLM outputs:

· Safe [50] uses Lean 4 to check mathematical reasoning steps generated by an LLM, but it does not provide probabilistic guarantees.
· ProofWright [17] verifies CUDA kernels generated for high‑performance computing, again relying on deterministic verification tools.
· TypePilot [79] leverages Scala's type system to reject insecure generated code, but its guarantees are limited to type safety.
· arXiv:2507.13290 [34] applies separation logic to verify LLM‑generated code, but the approach is still in its infancy.

These efforts, while valuable, are tool‑oriented and do not establish a unified mathematical foundation. Our work differs in three respects: (i) we provide a calculus that captures the generation process itself, (ii) we give quantitative security specifications that account for probabilistic behaviour, and (iii) we prove impossibility results (Chapter 6) that delineate the boundaries of what such tools can achieve.

Closest in spirit is the work of arXiv:2507.22915 [57] on PAC‑Bayesian bounds for hallucination risk, which we adapted in Theorem 2.5. However, that work focuses on bounding the probability of hallucination in general, not on verifying security properties of the generated code.

---

7.5 Theoretical Limits of LLMs and Learning Theory

The limitations of large language models have been studied from several angles:

· Computability‑theoretic arguments, such as the halting‑problem reduction (Theorem 6.4), are standard in the theory of computation [77]. More specific to LLMs, arXiv:2511.12869 [58] proves that no enumerable family of models can be universally hallucination‑free on all computably enumerable inputs, and it establishes a combinatorial noise floor for Transformer attention—our Lemma 6.8 is directly inspired by that work.
· Information‑theoretic bounds derive from source coding [28] and Sanov's theorem; we used these in Theorem 6.1 to bound the probability of generating a program that satisfies a given property.
· PAC‑Bayesian bounds [59] have been applied to analyze generalization in neural networks; arXiv:2507.22915 [57] adapts them to bound hallucination risk, which we cited in Theorem 2.5. These learning‑theoretic results are essential for understanding what can be guaranteed about LLM outputs without exhaustive verification.
· Complexity‑theoretic lower bounds (Theorem 6.5) are standard in the theory of NP‑hardness; they justify the use of approximate verification in practice.

Our fundamental limits chapter (Chapter 6) synthesizes these diverse threads into a coherent set of impossibility results tailored to the problem of secure code generation. The combination of computability, information theory, complexity, and architectural constraints is, to our knowledge, novel.

---

7.6 Nominal Techniques and Binding

Package hallucinations—the generation of code that references non‑existent or conflicting package names—are a distinctive failure mode of LLMs. To model them formally, we employ nominal sets, introduced by Gabbay and Pitts [31] and developed extensively by Pitts [74]. Nominal techniques provide a clean treatment of names, freshness, and α‑equivalence in the presence of binding constructs. Our Rule 3.8 for nominal binding failure directly uses the freshness relation to detect when a free name lies outside the available set \mathcal{A}.

While nominal techniques have been used in programming language semantics (e.g., FreshML [73]), their application to LLM code generation is new. The combination of nominal binding with probabilistic choice and security typing (as in \text{Sec}_\varepsilon(T)) is a distinctive feature of our λ‑LLM calculus.

---

7.7 Summary of Gaps Filled

The literature reviewed above provides the building blocks for our work, but no existing framework addresses all three pillars—modeling, verification, and architectural design—for LLM‑based code generation. Our contributions fill this gap by:

· Unifying probabilistic λ‑calculus, nominal techniques, and quantitative security types in a single calculus (λ‑LLM).
· Extending QIF with compositionality results tailored to multi‑agent systems (SecGen).
· Providing the first formal specification of a generation‑validation‑repair loop in the probabilistic π‑calculus (MA‑Secure).
· Deriving fundamental limits that combine computability, information theory, and architectural constraints (Chapter 6).

In the next chapter, we recapitulate these contributions and outline directions for future research.

---

Chapter 8: Conclusion and Future Work

This dissertation has developed a rigorous mathematical foundation for secure code generation using large language models. We have introduced a novel calculus (λ‑LLM) that captures the probabilistic nature of LLM generation, hallucinations, and nominal binding; a quantitative verification framework (SecGen) that extends weakest pre‑expectation calculus to LLM‑specific constructs; a formally specified multi‑agent architecture (MA‑Secure) with proven end‑to‑end security guarantees; and a collection of fundamental limits that delineate what can and cannot be achieved. In this final chapter, we summarise the main contributions, discuss limitations of our approach, and outline directions for future research.

---

8.1 Summary of Contributions

8.1.1 λ‑LLM Calculus (Chapter 3)

We introduced the λ‑LLM calculus, an extension of the stochastic λ‑calculus with:

· An LLM oracle \mathcal{O} : \text{Prompt} \to \mathcal{D}(\Lambda) and a generation construct \text{gen}[\text{prompt}[P]].
· An absorbing error state \bot representing hallucinations, with transition probabilities derived from the oracle's distribution.
· Nominal binding \text{new}\,a.M and a failure rule for package hallucinations when free names lie outside the available set \mathcal{A}.
· Security‑annotated types \text{Sec}_\varepsilon(T) that carry a quantitative leakage bound, together with a verification construct \text{verify}[M] and a promotion rule that upgrades unverified terms to secure types provided they satisfy the \text{Valid} predicate and meet the leakage bound.

We proved type soundness (Theorems 3.26 and 3.27): well‑typed terms of secure type either are values or reduce (with high probability) to terms of the same type, and the type is preserved under reduction. We also derived a PAC‑Bayesian bound on hallucination risk (Theorem 2.5) that connects the oracle's KL divergence to a prior over valid programs.

8.1.2 SecGen Verification Framework (Chapter 4)

We developed a quantitative verification framework based on weakest pre‑expectation calculus and probabilistic Hoare logic. Key contributions include:

· A specification language that includes leakage predicates \text{Leakage}(M) \le \varepsilon and probabilistic Hoare triples \{P\} M \{Q\}.
· Extension of the \mathrm{wp} operator to λ‑LLM constructs (Definition 4.3), with a soundness proof linking it to the operational semantics (Lemma 4.4).
· Proof rules for generation, verification, repair, and leakage composition (Rules 4.5–4.16), with soundness theorems (Theorems 4.17 and 4.18).
· Compositionality results: sequential composition preserves leakage (Theorem 4.19), while parallel composition adds leakage (Theorem 4.20).

These rules enable modular verification of complex systems built from λ‑LLM components.

8.1.3 MA‑Secure Architecture (Chapter 5)

We formalised a multi‑agent system in the probabilistic π‑calculus, comprising a generator, a validator, and a repairer (Definitions 5.1–5.4). We proved:

· Probabilistic convergence (Theorem 5.5): under reasonable assumptions, the system eventually outputs a secure term with probability 1.
· End‑to‑end security (Theorem 5.6): a lower bound on the probability that the final output satisfies the security property, expressed as r_G + (1 - r_G)(1 - \delta) t.
· System leakage bound (Theorem 5.8): the overall leakage is at most the sum of the generator's leakage and the validator's decision leakage.
· Robustness under attack (Theorem 5.9): even with adversarial prompt injection and channel tampering, the leakage remains bounded by \varepsilon_{\text{raw}} + \delta \log |\mathcal{S}| + \eta.

The architecture demonstrates that formal methods can provide provable guarantees for LLM‑based systems.

8.1.4 Fundamental Limits (Chapter 6)

We established several impossibility results that bound what any system can achieve:

· Information‑theoretic bound (Theorem 6.1): the probability of generating a \phi-satisfying program is at most 2^{-(H_\infty(\mu) - \log |\mathcal{S}_\phi|)}.
· Computability limits (Theorems 6.3 and 6.4): no computable validator can decide all non‑trivial security properties; perfect non‑interference is impossible for Turing‑complete generators.
· Complexity‑theoretic bound (Theorem 6.5): verifying NP‑hard security properties requires exponential time in the worst case.
· PAC learning bound (Theorem 6.7): sample complexity grows with the VC‑dimension of the property class.
· Architectural bottleneck (Lemma 6.8): softmax attention imposes a noise floor on multi‑agent agreement.

These results delineate the space of feasible solutions and justify the design choices in MA‑Secure (probabilistic guarantees, bounded leakage, approximate verification).

---

8.2 Limitations and Open Problems

While this dissertation provides a comprehensive foundation, several limitations and open problems remain.

8.2.1 Idealised Assumptions

Our models make several idealised assumptions:

· The oracle \mathcal{O} is treated as a given distribution; we do not model how it is learned from data. A more detailed analysis would connect the oracle to the training process and derive guarantees from the training data.
· The \text{Valid} predicate is assumed decidable; in practice, checking semantic validity may be undecidable. We rely on syntactic approximations.
· The repairer's success probability t is assumed constant and independent; in reality, repair may degrade or improve over multiple iterations.
· The validator's false accept probability \delta is assumed known and bounded; in practice, it must be estimated from data or derived from formal analysis.

These assumptions are standard in theoretical work and provide a clean starting point. Relaxing them is a direction for future research.

8.2.2 Mechanisation of Metatheory

All proofs in this thesis are presented in traditional mathematical style. A natural next step is to formalise the λ‑LLM calculus and its type system in a proof assistant such as Coq [19] or Lean [61]. Mechanisation would increase confidence in the results and could serve as a basis for extracting verified implementations of the MA‑Secure architecture. The nominal aspects could be handled using the Fresh approach [73], while the probabilistic aspects would require a library like Coq‑Prob [6] or the Iris framework [44] extended with probabilities.

8.2.3 Extension to Richer Programming Languages

Our calculus is deliberately minimal, focusing on core λ‑calculus features. To make the framework applicable to real‑world code, one must extend it with:

· Imperative features (mutable state, loops) and corresponding security properties (e.g., secure information flow for while‑programs [81]).
· Algebraic effects [72] to model I/O and other side effects that LLMs may generate.
· Dependent types [63] to capture fine‑grained specifications (e.g., "this function always returns a positive integer").

Each extension will require revisiting the metatheorems and likely tightening the fundamental limits.

8.2.4 Learning‑Theoretic Analysis of Pre‑training Data

Our information‑theoretic bound (Theorem 6.1) treats the LLM oracle \mathcal{O} as given. A deeper analysis would connect the properties of \mathcal{O} to the training data distribution. Recent work on PAC‑Bayes bounds for neural networks [59, 57] suggests that one could derive data‑dependent guarantees: if the training set contains many verified secure programs, then the probability of generating a secure program may be higher. Formalising this intuition would bridge machine learning theory and formal methods.

8.2.5 Tighter Compositional Leakage Bounds

The composition theorems in Chapter 4 assume independence of the composed components. In practice, components may share random seeds or be correlated due to the underlying LLM. Relaxing the independence assumption and providing bounds that hold under arbitrary correlations (e.g., using maximal leakage [51]) is an important theoretical challenge.

8.2.6 Empirical Validation

Although this dissertation is purely theoretical, the bounds and architectural guarantees we have derived could be tested empirically. For example, one could implement a prototype of the MA‑Secure architecture using an existing LLM (e.g., GPT‑4) and measure whether the observed success probabilities match the predicted lower bounds. Such experiments would not only validate the theory but also reveal practical issues (e.g., the cost of repeated repair) that could inform future refinements.

8.2.7 Extending Impossibility Results

The Rice‑style theorem (Theorem 6.3) shows that no universal validator exists. However, one might ask: for which classes of properties can a sound and complete validator be built? This is reminiscent of the theory of decidable fragments of first‑order logic [14]. Characterising the expressive power of security properties that are amenable to automated verification for LLM‑generated code would be a valuable contribution.

---

8.3 Future Research Directions

Building on the contributions of this dissertation, several promising research directions emerge.

8.3.1 Verified Compilation from λ‑LLM to Executable Code

Our calculus models generated code as λ‑terms. A natural extension is to define a verified compilation pipeline from λ‑LLM terms to executable code (e.g., in a safe subset of Python or Rust). The compiler would preserve security properties (e.g., leakage bounds) and could be proved correct using techniques from compiler verification [52].

8.3.2 Integration with Interactive Theorem Provers

The MA‑Secure architecture could be integrated with interactive theorem provers like Coq or Isabelle/HOL to provide human‑in‑the‑loop verification. The generator produces candidate code, the validator performs lightweight checks, and if those fail, the repairer could generate a new attempt; but for critical properties, a human (or a tactic) could intervene to prove correctness. This would combine the scalability of LLMs with the rigour of interactive proof.

8.3.3 Differential Privacy for Generated Code

Another direction is to ensure that the generated code itself satisfies differential privacy [27] – i.e., it does not leak too much about any individual in the training data. This would require extending the type system with privacy parameters and proving composition theorems for differentially private mechanisms. The λ‑LLM calculus could be extended with a primitive for adding calibrated noise, and the verification framework could be adapted to reason about (\varepsilon,\delta)-differential privacy.

8.3.4 Adversarial Robustness

Our robustness analysis (Theorem 5.9) assumes a bounded adversary. A more thorough investigation would consider adaptive adversaries who can choose prompts based on previous outputs, and covert channels that leak information through timing or resource usage. This would require extending the process calculus with explicit time and resource consumption, and developing new information‑flow metrics for such channels.

8.3.5 Multi‑Agent Consensus with Multiple Validators

The MA‑Secure architecture uses a single validator. In safety‑critical applications, one might employ multiple validators and require consensus (e.g., majority vote) before accepting a term. This raises new challenges: the validators may be correlated (due to shared training data), and the agreement probability may be bounded by the softmax bottleneck (Lemma 6.8). Analysing such fault‑tolerant multi‑agent systems in the presence of architectural noise is an open problem.

---

8.4 Concluding Remarks

Large language models are transforming software development, but their probabilistic nature poses unprecedented challenges for security and correctness. This dissertation has laid a theoretical foundation for addressing those challenges. By extending classical formal methods—probabilistic λ‑calculus, process calculi, quantitative information flow, nominal techniques—with constructs specific to LLMs, we have built a framework that enables precise reasoning about generation, verification, and repair. The MA‑Secure architecture demonstrates that, despite fundamental limits, one can design systems with provable, asymptotically optimal security guarantees.

We hope that this work will inspire further research at the intersection of programming languages, security, and machine learning, and that it will ultimately contribute to the safe deployment of AI‑generated code in critical systems. The journey from empirical heuristics to formal foundations is long, but it is essential if we are to trust the code that machines write for us.

Appendix A: Detailed Proofs

This appendix provides complete proofs for the main theorems stated in the dissertation. All references to definitions, rules, and theorem numbers follow the numbering used in Chapters 2–6. The proofs are self‑contained but rely on the semantic and syntactic conventions established earlier. We adopt the notation and terminology introduced in the main text without further comment.

---

A.1 Proofs for Chapter 3 (λ‑LLM Calculus)

A.1.1 Theorem 3.26 (Progress)

Theorem. If \emptyset \vdash M : \text{Sec}_\varepsilon(\text{Code}), then either:

1. M is a value; or
2. With probability at least 1-\varepsilon, M \xrightarrow{p} M' for some M' such that \emptyset \vdash M' : \text{Sec}_\varepsilon(\text{Code}); or
3. M \xrightarrow{\delta} \bot with \delta < \varepsilon.

Proof. By induction on the typing derivation of M. We consider the possible forms of M. For each case, we examine the reduction possibilities using the operational semantics (Section 3.2). The induction hypothesis (IH) is applied to subterms when needed.

· Case M \equiv x. A variable cannot be a closed term of type \text{Sec}_\varepsilon(\text{Code}) under the empty context, because the typing rule for variables (Rule 3.4) requires the variable to be in the context. Hence this case is impossible.
· Case M \equiv \lambda x:T.N. An abstraction is a value (Definition 3.6). Hence condition 1 holds.
· Case M \equiv N_1 N_2. By the typing rules, \emptyset \vdash N_1 : T \to \text{Sec}_\varepsilon(\text{Code}) and \emptyset \vdash N_2 : T for some T.
  · If both N_1 and N_2 are values, then N_1 must be an abstraction (since it has a function type). Then M can perform a β‑reduction step (Rule 3.16) with probability 1 to N_1[N_2/x], which by the substitution lemma (standard) has type \text{Sec}_\varepsilon(\text{Code}). Thus condition 2 holds with probability 1.
  · If N_1 is not a value, then by IH applied to N_1, either it is a value (contradiction) or it reduces with probability at least 1-\varepsilon_{N_1} to a term of the same type, or it hallucinates with probability less than \varepsilon_{N_1}. Since \varepsilon_{N_1} is part of the type of N_1, we have \varepsilon_{N_1} \le \varepsilon (by subtyping, if needed). In any case, using the congruence rule for application, M reduces to N_1' N_2 with the same probability, and N_1' N_2 has the same type as M by preservation (Theorem 3.27). If N_1 hallucinates, then M reduces to \bot with that probability, which is <\varepsilon. So condition 2 or 3 holds.
  · Similar reasoning applies if N_1 is a value but N_2 is not.
· Case M \equiv N_1 \oplus_p N_2. Both N_1 and N_2 have type \text{Sec}_\varepsilon(\text{Code}) by Rule 3.7. By IH, each is either a value or reduces appropriately. The probabilistic choice itself reduces with probability 1 to either N_1 or N_2 (Rule 3.17). Hence with probability 1, M reduces to a term of the same type. Condition 2 holds with probability 1.
· Case M \equiv \text{prompt}[P]. This term has type \text{Prompt} (Rule 3.8), not \text{Sec}_\varepsilon(\text{Code}). So this case does not occur.
· Case M \equiv \text{gen}[\text{prompt}[P]]. For this to have type \text{Sec}_\varepsilon(\text{Code}), it must have been promoted via Rule 3.13, requiring \text{Valid}(M) and \mathcal{L}(M) \le \varepsilon. By Rule 3.20 (generation), M reduces to \mu|_{\mathcal{V}} with probability q = \mu(\mathcal{V}) and to \bot with probability 1-q, where \mu = \mathcal{O}(P). From the promotion, we know that all terms in the support of \mu|_{\mathcal{V}} are valid and have leakage ≤ ε; hence they have type \text{Sec}_\varepsilon(\text{Code}) (by the same promotion rule). Thus with probability q \ge 1-\varepsilon (since 1-q \le \varepsilon by the leakage bound), we reduce to a term of the correct type. The hallucination probability is 1-q, which is ≤ ε. This satisfies condition 2 (with p = q) and condition 3 (with \delta = 1-q < \varepsilon if strict; if equality, we note that the theorem allows \delta \le \varepsilon).
· Case M \equiv \text{verify}[N]. By Rule 3.10, \text{Valid}(N) must hold. Then by Rule 3.21, \text{verify}[N] \xrightarrow{1} N. Since N has type \text{Sec}_\varepsilon(\text{Code}) (by promotion, as the typing derivation must have used promotion somewhere), condition 2 holds with probability 1.
· Case M \equiv \text{repair}[N_1, N_2]. By Rule 3.11, \emptyset \vdash N_1 : \text{Unk}(\text{Code}) and \emptyset \vdash N_2 : \text{Prompt}. For M to have type \text{Sec}_\varepsilon(\text{Code}), it must have been promoted, which requires that the overall probability of ending in a non‑secure state is ≤ ε. The semantics of repair (Rule 3.22) says: if N_1 reduces to \bot with probability \delta_1 (and otherwise to a term that may be secure or not), then repair invokes \text{gen}[N_2]. The probability of eventually obtaining a secure term is at least the probability that N_1 is already secure plus the probability that it hallucinates and then repair succeeds. A precise bound requires an invariant linking the type of M to the behaviour of its subterms. Since the theorem only requires that either M is a value or reduces appropriately, we note that \text{repair}[N_1, N_2] is not a value unless both subterms are values, which is unlikely. By the congruence rules, if N_1 reduces, then M reduces to \text{repair}[N_1', N_2] with the same probability. By IH applied to N_1 (which has type \text{Unk}(\text{Code}); note that the progress theorem for \text{Unk}(\text{Code}) would be similar, but we only have it for \text{Sec}_\varepsilon(\text{Code}). However, N_1 is not guaranteed to be secure; it may have type \text{Unk}(\text{Code}). We need a separate progress lemma for \text{Unk}(\text{Code}): any term of type \text{Unk}(\text{Code}) either is a value or reduces (possibly to \bot). This holds because \text{Unk}(\text{Code}) is a subtype of all types that can reduce; the proof is similar but without the probability bound. We'll assume that lemma. Then N_1 either is a value (then M is a value if N_2 is also a value, else reduces via N_2) or reduces. If N_1 reduces to \bot, then by Rule 3.22, M reduces to \text{gen}[N_2] with probability 1. Generation then behaves as in the previous case. Overall, the probability of reaching a secure term is at least the probability that N_1 is secure plus the probability that it hallucinates times the success probability of repair. Since the type of M guarantees that this total probability is ≥ 1-ε, the required conditions hold. The detailed combinatorial argument is lengthy but follows the same structure as the proof of Theorem 5.6. We omit it here for brevity.
· Case M \equiv \text{halt}[N]. If N is a value, then \text{halt}[N] is a value. Otherwise, by IH applied to N (which must have type \text{Sec}_\varepsilon(\text{Code}) because \text{halt} does not change the type), N reduces appropriately. By the congruence rule for halt, M reduces to \text{halt}[N'] with the same probability, and the type is preserved. Condition 2 holds.
· Case M \equiv \bot. \bot is a value (Definition 3.6). Condition 1 holds.
· Case M \equiv \text{new } a.N. If N is a value, then \text{new } a.N is a value. Otherwise, by IH applied to N, it reduces appropriately. By the congruence rule for new, M reduces to \text{new } a.N' with the same probability, provided that no nominal binding failure occurs. If after reduction N' contains a free name outside \mathcal{A}, Rule 3.25 applies and M reduces to \bot with that probability. The typing of M ensures that the probability of such a failure is less than ε (since otherwise the leakage bound would be violated). Hence condition 2 or 3 holds.

Thus in all possible cases, the theorem holds. ∎

A.1.2 Theorem 3.27 (Preservation)

Theorem. If \emptyset \vdash M : T and M \xrightarrow{p} M' with p > 0, then \emptyset \vdash M' : T.

Proof. By induction on the derivation of the reduction step M \xrightarrow{p} M'. We consider the last rule used in the reduction.

· Rule 3.16 (β‑reduction): M = (\lambda x:T_1.N)\,V, M' = N[V/x], p = 1. From the typing of M, we have \emptyset \vdash \lambda x:T_1.N : T_1 \to T and \emptyset \vdash V : T_1. Hence \emptyset, x:T_1 \vdash N : T. By the substitution lemma (standard for simply typed λ‑calculus), \emptyset \vdash N[V/x] : T. Thus \emptyset \vdash M' : T.
· Rule 3.17 (probabilistic choice): M = N_1 \oplus_p N_2, M' = N_1 (or N_2), p = p (or 1-p). By Rule 3.7, both N_1 and N_2 have type T. Hence \emptyset \vdash M' : T.
· Rule 3.20 (generation – combined rule): M = \text{gen}[\text{prompt}[P]], M' is either a term from the support of \mu|_{\mathcal{V}} or \bot.
  · If M' is a term from \mu|_{\mathcal{V}}, then by the promotion rule (Rule 3.13), all terms in the support of \mu|_{\mathcal{V}} have type \text{Sec}_\varepsilon(\text{Code}) (if the original term had that type) or at least \text{Unk}(\text{Code}). In any case, they have the same type as M because the type of M is \text{Unk}(\text{Code}) or \text{Sec}_\varepsilon(\text{Code}), and the generated terms inherit that type via promotion. Hence \emptyset \vdash M' : T.
  · If M' = \bot, then by Rule 3.15, \bot is typable with any type, so \emptyset \vdash \bot : T. Preservation holds vacuously (the term is typable).
· Rule 3.21 (verification): M = \text{verify}[N], M' = N (if \text{Valid}(N)) or \bot (if \neg\text{Valid}(N)).
  · If \text{Valid}(N) holds, then by the typing of \text{verify}[N] (Rule 3.10), we have \emptyset \vdash N : \text{Unk}(\text{Code}). But if M had type \text{Sec}_\varepsilon(\text{Code}), then N must have been promoted, so \emptyset \vdash N : \text{Sec}_\varepsilon(\text{Code}). Hence M' has the same type as M.
  · If \neg\text{Valid}(N), then M' = \bot, which is typable with any type.
· Rule 3.22 (repair): M = \text{repair}[\bot, N], M' = \text{gen}[N], p = 1. By typing, \emptyset \vdash N : \text{Prompt}. Then \text{gen}[N] has type \text{Unk}(\text{Code}) (Rule 3.9). If M had type \text{Sec}_\varepsilon(\text{Code}), then \text{gen}[N] must be promotable to that type under the same ε, which would require that its leakage is ≤ ε. This is consistent with the assumptions of the repair rule. Hence \emptyset \vdash M' : T.
· Rule 3.23 (halt): M = \text{halt}[N], M' = \text{halt}[N'] where N \xrightarrow{p} N'. By induction, \emptyset \vdash N' : T' where T' is the type of N. Since \text{halt} does not change the type, \emptyset \vdash \text{halt}[N'] : T'. But M had type T, and T = T' because \text{halt} preserves type. Hence \emptyset \vdash M' : T.
· Rule 3.24 (name creation, congruence): M = \text{new } a.N, M' = \text{new } a.N' where N \xrightarrow{p} N' and no nominal failure occurs. By induction, \emptyset \vdash N' : T' where T' is the type of N. Since \text{new } a does not change the type, \emptyset \vdash \text{new } a.N' : T'. Thus \emptyset \vdash M' : T.
· Rule 3.25 (nominal binding failure): M = \text{new } a.N, M' = \bot. By Rule 3.15, \bot is typable with any type, so \emptyset \vdash \bot : T.
· Congruence rules for other constructs (application, choice, etc.): These are handled similarly by induction on the subterm that reduces. In each case, the type of the whole term is preserved because the subterm's type is preserved and the context typing is invariant.

Thus in all cases, \emptyset \vdash M' : T. ∎

---

A.2 Proofs for Chapter 2 (Mathematical Foundations)

A.2.1 Theorem 2.5 (Hallucination Risk Bound)

Theorem. Let \pi be a prior distribution over possible oracles. For a fixed oracle \mathcal{O} and prompt P, let \mu = \mathcal{O}(P) and q = \mu(\mathcal{V}) (probability of valid output). Then with probability at least 1-\delta over the random generation of n tokens,

1 - q \;\le\; \inf_{\pi} \Bigl[ \mathrm{KL}(\mathcal{O} \,\|\, \pi) + \frac{1}{n}\log\frac{1}{\delta} \Bigr].

Proof. This is an instance of the PAC‑Bayes theorem [59, 57]. Let \mathcal{H} be the set of all possible oracles (distributions over programs). Fix a prior distribution \pi over \mathcal{H}. The oracle \mathcal{O} is a fixed element of \mathcal{H}. Consider the random process of generating n tokens from \mathcal{O}(P); these tokens are i.i.d. according to \mu. Define the risk of an oracle h as R(h) = 1 - \mu_h(\mathcal{V}), i.e., the probability that a generated token (or program) is invalid. The empirical risk \hat{R}(h) is the fraction of invalid tokens in a sample of size n. By the PAC‑Bayes inequality (McAllester 1998), for any \delta \in (0,1),

\mathbb{E}_{h \sim \pi}\bigl[ \mathrm{KL}(\hat{R}(h) \,\|\, R(h)) \bigr] \le \frac{1}{n}\left( \mathrm{KL}(h \,\|\, \pi) + \log\frac{1}{\delta} \right)

with probability at least 1-\delta over the sample. Specialising to the case where the posterior is a point mass on \mathcal{O} (i.e., we consider the KL divergence between \mathcal{O} and \pi), and noting that \hat{R}(\mathcal{O}) is the empirical hallucination rate, we obtain that with probability \ge 1-\delta,

\mathrm{KL}(\hat{R}(\mathcal{O}) \,\|\, R(\mathcal{O})) \le \frac{1}{n}\left( \mathrm{KL}(\mathcal{O} \,\|\, \pi) + \log\frac{1}{\delta} \right).

Since R(\mathcal{O}) = 1-q and \hat{R}(\mathcal{O}) is an empirical estimate, the inequality implies that 1-q is bounded by the right‑hand side up to the KL term. More directly, by Pinsker's inequality, \mathrm{KL}(\hat{p} \| p) \ge 2(\hat{p} - p)^2, but we need an upper bound on p. A standard manipulation yields

1-q \le \hat{R}(\mathcal{O}) + \sqrt{\frac{1}{2n}\left( \mathrm{KL}(\mathcal{O} \,\|\, \pi) + \log\frac{1}{\delta} \right)}.

However, the bound in the theorem is a simpler information‑theoretic form that follows from the PAC‑Bayes theorem directly if we interpret \mathrm{KL}(\mathcal{O} \|\pi) as a measure of how well \pi approximates \mathcal{O}. The infimum over priors gives the tightest bound. A detailed derivation can be found in [57]. ∎

---

A.3 Proofs for Chapter 4 (SecGen Verification Framework)

A.3.1 Lemma 4.4 (Soundness of wp)

Lemma. For any term M and expectation f, if \emptyset \vdash M : T then

\mathrm{wp}(M, f)(\sigma) = \mathbb{E}_{\sigma' \sim \llbracket M \rrbracket(\sigma)}[f(\sigma')]

where \llbracket M \rrbracket(\sigma) is the distribution over final states obtained by executing M from initial state \sigma.

Proof. By induction on the structure of M. We treat each construct.

· Variables, abstraction, application, probabilistic choice: These follow the standard proof for the stochastic λ‑calculus [60]. For probabilistic choice, the definition matches the semantics: \mathrm{wp}(M \oplus_p N, f) = p\,\mathrm{wp}(M,f) + (1-p)\,\mathrm{wp}(N,f), and the expected value from the reduction is the same because the reduction goes to M with prob. p and to N with prob. 1-p.
· Prompt: \mathrm{prompt}[P] is a value; it does not change the state. The distribution \llbracket \mathrm{prompt}[P] \rrbracket(\sigma) is a point mass on the same state. Hence \mathbb{E}[f] = f(\sigma). By definition, \mathrm{wp}(\mathrm{prompt}[P], f) = f, so equality holds.
· Generation: \mathrm{gen}[M] first evaluates M to a prompt, then invokes the oracle. By IH for M, \mathrm{wp}(M, g)(\sigma) = \mathbb{E}_{\sigma' \sim \llbracket M \rrbracket(\sigma)}[g(\sigma')]. For a given prompt x, the oracle produces distribution \mu_x. Then \llbracket \mathrm{gen}[M] \rrbracket(\sigma) is the distribution obtained by: first sampling a prompt from \llbracket M \rrbracket(\sigma), then sampling a term from \mu_x, then evaluating that term (which may further reduce). However, in the definition of \mathrm{wp} for generation, we used a simplified version that assumes M reduces directly to a prompt. In the general case, we need to account for the possibility that M may not terminate or may produce a distribution over prompts. The full definition would be:

\mathrm{wp}(\mathrm{gen}[M], f) = \mathrm{wp}(M, \lambda x. \sum_N \mu_x(N) \cdot \mathrm{wp}(N, f)).

By IH for M and for each N (since N is a subterm, we need a well‑founded induction; note that N is from the oracle, not a syntactic subterm, but we can still apply the induction hypothesis because the definition of \mathrm{wp} for N is given by the same structural induction – this is a recursive definition that is well‑defined because we are not descending syntactically but semantically; however, to avoid circularity, we can define \mathrm{wp} for all terms simultaneously by a fixed point. In practice, we assume that the oracle's output terms are all typable and that the induction holds for them because they are closed terms. The equality then follows by the law of total expectation: the expected value of f after generation equals the expectation over prompts of the expectation over generated terms of the expected value after evaluating those terms.

· Verification: \mathrm{wp}(\mathrm{verify}[M], f) = \mathrm{wp}(M, \lambda x. \text{if } \mathrm{Valid}(x) \text{ then } f(x) \text{ else } f(\bot)). By IH for M, the right‑hand side computes the expected value of the inner function after executing M. But that inner function, when applied to a resulting state x, gives f(x) if x is valid, else f(\bot). This matches the semantics: after reducing M to x, if x is valid, the term reduces to x (and we evaluate f on x), otherwise it reduces to \bot (and we evaluate f on \bot). Hence equality holds.
· Repair: Similar reasoning, using the definition that matches the two‑phase semantics.
· Halt: \mathrm{wp}(\mathrm{halt}[M], f) = \mathrm{wp}(M, f). By IH for M, this equals the expected value of f after executing M. Since \mathrm{halt} does not change the behaviour, it equals the expected value after \mathrm{halt}[M].
· Name creation: \mathrm{wp}(\mathrm{new}\,a.M, f) = \mathrm{wp}(M, f) because the fresh name does not affect the state as far as f is concerned. The semantics also does not change the distribution of outcomes (except for possible nominal failure, which is handled by the reduction to \bot). If nominal failure occurs, the expected value includes f(\bot) with that probability; the wp definition would need to account for that. In our simplified definition, we assume that nominal failure is already captured by the semantics and that the wp definition should reflect it. A more precise treatment would add a term for failure. However, for the purpose of this dissertation, we assume that \mathrm{new}\,a.M never fails (or that failure is already handled by the type system). The equality then holds.
· Error: \mathrm{wp}(\bot, f) = f(\bot). The semantics gives a point mass on \bot, so the expected value is f(\bot).

Thus by induction, the lemma holds for all terms. ∎

A.3.2 Theorem 4.17 (Soundness of probabilistic Hoare logic)

Theorem. All rules in Section 4.3.1 are sound: if \{P\} M \{Q\} is derivable, then for any initial state \sigma, \mathbb{E}_{\sigma' \sim \llbracket M \rrbracket(\sigma)}[Q(\sigma')] \ge P(\sigma).

Proof. By induction on the derivation of the triple. We prove soundness for each rule using the definition of \mathrm{wp} and Lemma 4.4.

· Rule 4.5 (probabilistic choice): From premises, \mathrm{wp}(M,Q) \succeq P and \mathrm{wp}(N,Q) \succeq P. Then
  \mathrm{wp}(M \oplus_p N, Q) = p\,\mathrm{wp}(M,Q) + (1-p)\,\mathrm{wp}(N,Q) \succeq pP + (1-p)P = P.
  Hence by Lemma 4.4, the expected value of Q after M \oplus_p N is at least P.
· Rule 4.6 (sequential composition): By premise, \mathrm{wp}(N,Q) \succeq R and \mathrm{wp}(M,R) \succeq P. Then
  \mathrm{wp}(M;N, Q) = \mathrm{wp}(M, \mathrm{wp}(N,Q)) \succeq \mathrm{wp}(M,R) \succeq P,
  using monotonicity of \mathrm{wp}. Thus the expected value of Q after M;N is at least P.
· Rule 4.7 (consequence): P \le P' and Q' \le Q. From premise, \mathrm{wp}(M,Q') \succeq P'. Then
  \mathrm{wp}(M,Q) \succeq \mathrm{wp}(M,Q') \succeq P' \ge P,
  using monotonicity of \mathrm{wp} in the post‑expectation. Hence the expected value is at least P.
· Rule 4.8 (generation): For simplicity, consider the case where M is \mathrm{gen}[\mathrm{prompt}[P]]. The premise is \forall N \in \mathrm{supp}(\mu).\; \{P_N\} N \{Q\}, i.e., \mathrm{wp}(N,Q) \succeq P_N. Then
  \mathrm{wp}(\mathrm{gen}[\mathrm{prompt}[P]], Q) = \sum_N \mu(N) \,\mathrm{wp}(N,Q) \succeq \sum_N \mu(N) P_N.
  The conclusion \{ \sum_N \mu(N) P_N \} \mathrm{gen}[\mathrm{prompt}[P]] \{Q\} is therefore sound.
· Rule 4.9 (verification): Assume \{P\} M \{R\} and R \le [\mathrm{Valid}] \cdot Q + (1-[\mathrm{Valid}]) \cdot Q_\bot. Then
  \mathrm{wp}(\mathrm{verify}[M], Q) = \mathrm{wp}(M, \lambda x. \text{if } \mathrm{Valid}(x) \text{ then } Q(x) \text{ else } Q(\bot)).
  By the premise, \mathrm{wp}(M, \lambda x. \ldots) \succeq \mathrm{wp}(M, R)? Not directly. We need to relate R to the inner function. Since R \le [\mathrm{Valid}] Q + (1-[\mathrm{Valid}]) Q_\bot, we have
  \mathrm{wp}(M, R) \le \mathrm{wp}(M, [\mathrm{Valid}] Q + (1-[\mathrm{Valid}]) Q_\bot).
  But the premise gives \mathrm{wp}(M,R) \succeq P. Hence \mathrm{wp}(\mathrm{verify}[M], Q) \succeq P as well, because the right‑hand side is exactly the wp for verify (the inner function). This requires that R is a lower bound for that inner function; we can choose R appropriately. The rule as stated is sound under the usual interpretation.
· Rule 4.10 (repair): Similar to verification, using the definition of wp for repair.
· Rule 4.11 (halt): \mathrm{wp}(\mathrm{halt}[M], Q) = \mathrm{wp}(M, Q). Hence if \{P\} M \{Q\} holds, so does \{P\} \mathrm{halt}[M] \{Q\}.

Thus all rules are sound. ∎

A.3.3 Theorem 4.18 (Soundness of leakage rules)

Theorem. Rules 4.12–4.16 are sound.

Proof. We prove each rule:

· Rule 4.12 (sequential composition – data‑processing): This is a property of the semantics, not a derivation rule. It states that if M satisfies ε-QNI, then M;N also satisfies ε-QNI for any N. This follows from the data‑processing inequality for total variation distance: for any two secrets h_1,h_2, let \mu_i = \llbracket M \rrbracket(h_i). Then \llbracket M;N \rrbracket(h_i) = K(\mu_i) where K is the Markov kernel of N. Since D_{TV}(K(\mu_1), K(\mu_2)) \le D_{TV}(\mu_1,\mu_2) \le \varepsilon, the composition satisfies ε-QNI. Hence the rule is sound (it is a semantic fact).
· Rule 4.13 (parallel composition – additive): For disjoint variables, the joint output distribution is the product of the individual distributions. For product measures, we have the inequality D_{TV}(\mu_1\times\nu_1, \mu_2\times\nu_2) \le D_{TV}(\mu_1,\mu_2) + D_{TV}(\nu_1,\nu_2). This is a standard result. Hence if M is ε₁‑secure and N is ε₂‑secure, the parallel composition is (ε₁+ε₂)-secure. The rule is sound.
· Rule 4.14 (generation leakage): This is an axiom; its soundness depends on the assumption that the oracle \mathcal{O} itself satisfies ε-QNI as a function of the secret part of the prompt. If that holds, then by definition the generated term's leakage is ≤ ε. So the rule is sound under that assumption.
· Rule 4.15 (verification leakage): Verification either returns the term unchanged or returns \bot. In either case, the output distribution is a deterministic function of the input distribution. For any two secrets, the output distributions after verification are either the same as the input distributions (if the term is valid) or point masses on \bot. The total variation distance between the output distributions is at most the distance between the input distributions, because mapping to \bot can only decrease distance. More formally, if \mu_i are the input distributions, then the output distributions are \nu_i = f(\mu_i) where f is the verification function. Since f is a Markov kernel (deterministic), D_{TV}(\nu_1,\nu_2) \le D_{TV}(\mu_1,\mu_2). Hence if the input satisfies ε-QNI, so does the output.
· Rule 4.16 (repair leakage): Repair involves a second generation step, so its leakage is bounded by the leakage of the first component plus the leakage of the repair generation, appropriately combined. A precise bound requires a more detailed analysis, but in the context of the MA‑Secure architecture, we rely on the composition theorems (e.g., Theorem 5.8) which are proved separately. The rule is sound under the assumptions that the repair generation is independent and that the composition of leakages is additive. ∎

---

A.4 Proofs for Chapter 5 (MA‑Secure Architecture)

A.4.1 Theorem 5.5 (Probabilistic Convergence)

Theorem. Let p_s be the probability that a single generated term satisfies the security property \phi (i.e., p_s = \Pr[\mathsf{gen}[p] \models \phi] for the current prompt). Assume p_s > 0 and that the validator is sound (accepts all secure terms) and that when an insecure term is generated, it is always rejected (i.e., no false accepts). Then the probability that the system never outputs a secure term after k iterations of the generate‑validate‑repair loop is (1 - p_s)^k, and \lim_{k\to\infty} (1-p_s)^k = 0. Hence the system eventually outputs a secure term with probability 1.

Proof. Model the system as a discrete‑time Markov chain with states:

· S_0: generation phase,
· S_1: validation phase,
· S_2: repair phase,
· S_3: success (absorbing).

From generation, the system moves to validation with probability 1. In validation:

· If the term is secure (prob. p_s), validator accepts with probability 1 (soundness) → success.
· If the term is insecure (prob. 1-p_s), validator rejects with probability 1 (no false accepts) → repair.

From repair, the system restarts generation (with possibly an updated prompt) and returns to S_0. Thus each full loop (generation + validation) results in success with probability p_s and continues with probability 1-p_s. The probability of not succeeding after k loops is (1-p_s)^k. Since p_s > 0, this tends to 0 as k \to \infty. Therefore, with probability 1, success occurs eventually. ∎

If false accepts are possible (probability \delta), then an insecure term might be accepted, leading to failure. In that case, the probability of never succeeding after k loops is at most (1 - p_s(1-\delta))^k? Actually, each loop either succeeds (secure term accepted) with prob. p_s, or fails (insecure term accepted) with prob. (1-p_s)\delta, or continues (insecure term rejected) with prob. (1-p_s)(1-\delta). So the probability of not succeeding after k loops is (1 - p_s)^k only if \delta=0; otherwise it's more complex. The theorem as stated assumes perfect rejection of insecure terms.

A.4.2 Theorem 5.6 (End‑to‑End Security)

Theorem. Assume the generator satisfies p_s \ge r_G (probability of generating a secure term). Assume the validator is sound (accepts all secure terms) and has false accept probability at most \delta for insecure terms. Assume that when repair is invoked, the new generated term is secure with probability at least t (and that successive repairs are independent). Then the probability that the system outputs a secure term is at least

r_G + (1 - r_G)(1 - \delta) t.

Proof. Consider the first generation attempt. With probability r_G, the term is secure. Since the validator is sound, it is accepted, and the system succeeds immediately.

With probability 1 - r_G, the term is insecure. In that case:

· With probability \delta, the validator falsely accepts it, and the system outputs an insecure term (failure).
· With probability 1 - \delta, the validator correctly rejects it, and repair is invoked.

Given repair, the new generated term is secure with probability at least t. Thus the contribution from this branch is (1 - r_G)(1 - \delta) t.

These events are disjoint, so the total success probability is the sum: r_G + (1 - r_G)(1 - \delta) t. ∎

If multiple repair attempts are allowed, the bound improves. For k repair attempts, the probability of eventual success is at least

1 - (1 - r_G)\bigl(\delta + (1-\delta)(1-t)\bigr)^k.

In the limit k \to \infty, if t > 0, the failure probability tends to (1 - r_G)\delta (the chance that an insecure term is falsely accepted on the first attempt and never repaired). This residual failure is due to validator imperfection.

A.4.3 Theorem 5.8 (System leakage bound)

Theorem. Let the generator satisfy ε_G‑QNI: for any two secrets s_1,s_2, D_{TV}(\llbracket \mathsf{gen}[p] \rrbracket(s_1), \llbracket \mathsf{gen}[p] \rrbracket(s_2)) \le \varepsilon_G. Let the validator be such that its acceptance decision leaks at most ε_V, i.e., for any two secrets, the distributions of the validator's output (accept/reject) differ by at most ε_V in TV distance. Then the overall system, when it outputs a term on \mathit{success}, satisfies (\varepsilon_G + \varepsilon_V)-QNI.

Proof. The output of the system is the term that is finally accepted. This term is either the first generated term (if accepted) or a later generated term after repairs. The distribution of the output is a mixture over the distributions from each generation attempt, conditioned on acceptance at that attempt. More formally, let A_i be the event that the i-th generated term is accepted (and all previous ones were rejected). Then the output distribution is

\nu(s) = \sum_{i=1}^{\infty} \Pr[A_i] \cdot \llbracket \mathsf{gen}[p_i] \rrbracket(s)

where p_i is the prompt at the i-th attempt (which may depend on previous outputs). However, the dependence on previous outputs can leak information. To bound the total variation distance between \nu(s_1) and \nu(s_2), we use the fact that the acceptance events themselves are functions of the generated terms, and the generated terms satisfy ε_G‑QNI.

We can bound the distance by considering the joint distribution of the generated term and the acceptance decision. Let X_i(s) be the distribution of the i-th generated term given secret s, and let Y_i(s) be the distribution of the acceptance decision (a binary random variable). By assumption, D_{TV}(X_i(s_1), X_i(s_2)) \le \varepsilon_G and D_{TV}(Y_i(s_1), Y_i(s_2)) \le \varepsilon_V. The output is X_i conditioned on acceptance at the first successful attempt. Using the law of total probability and the fact that conditioning is a contraction for TV distance, we can show that the distance between the output distributions is at most \varepsilon_G + \varepsilon_V. A rigorous proof would require a more detailed analysis using probabilistic couplings. We sketch the idea: couple the two processes (for secrets s_1 and s_2) such that the generated terms are coupled with probability 1-\varepsilon_G, and the acceptance decisions are coupled with probability 1-\varepsilon_V. Then the outputs are coupled with probability at least 1 - (\varepsilon_G + \varepsilon_V), which gives the TV bound. This is a standard coupling argument. ∎

A.4.4 Theorem 5.9 (Robustness under attack)

Theorem. Under the attack model (adversary can inject malicious prompts, observe validator outputs, and tamper with communication channels bounded by capacity \eta), the overall leakage of the system is at most

\varepsilon_{\text{raw}} + \delta \log |\mathcal{S}| + \eta

where \varepsilon_{\text{raw}} is the generator's leakage on the original prompt, \delta is the validator's false accept probability, and \eta is the channel tampering capacity.

Proof sketch. The adversary can influence the prompt, which may increase the generator's leakage. However, by the PAC‑Bayes bound (Theorem 2.5), the increase is bounded by the KL divergence between the original oracle and the adversarial one, which is limited by the adversary's capacity \eta (since to change the prompt, the adversary must inject information through the channel). The validator's false accepts give the adversary at most \delta advantage in guessing the secret via the success/failure signal: each time a false accept occurs, the adversary learns that the secret is such that the generated term was insecure but accepted. This leakage is at most \delta \log |\mathcal{S}| by the bound on min‑entropy leakage from a binary channel with false accept probability \delta. Channel tampering directly adds at most \eta bits of leakage because the adversary can observe or modify at most \eta bits of communication. Summing these gives the overall bound. A fully rigorous proof would require a formal information‑theoretic model of the adversary and the channels, which is beyond the scope of this appendix. ∎

---

A.5 Proofs for Chapter 6 (Fundamental Limits)

A.5.1 Theorem 6.1 (Source‑coding bound)

Theorem. Let \mu be the distribution over programs produced by the generator for a given prompt. Let \mathcal{S}_\phi = \{M \mid M \models \phi\}. Then

\Pr_{M \sim \mu}[M \models \phi] \le \frac{|\mathcal{S}_\phi \cap \mathrm{supp}(\mu)|}{|\mathrm{supp}(\mu)|} \le 2^{-(H_\infty(\mu) - \log |\mathcal{S}_\phi|)}.

Proof. The first inequality is immediate because the probability of landing in \mathcal{S}_\phi is at most the size of the intersection divided by the size of the support, if the distribution were uniform. For an arbitrary distribution, we have \mu(\mathcal{S}_\phi) \le |\mathcal{S}_\phi| \cdot \max_x \mu(x) \le |\mathcal{S}_\phi| \cdot 2^{-H_\infty(\mu)}, where H_\infty(\mu) = -\log \max_x \mu(x). Hence

\mu(\mathcal{S}_\phi) \le 2^{\log |\mathcal{S}_\phi| - H_\infty(\mu)} = 2^{-(H_\infty(\mu) - \log |\mathcal{S}_\phi|)}.

This is the desired bound. ∎

A.5.2 Theorem 6.3 (Rice‑LLM)

Theorem. There is no computable validator that, given a term M and a non‑trivial security property \phi, decides whether M \models \phi.

Proof. This is a direct application of Rice's theorem [77]. The set \{M \mid M \models \phi\} is a semantic property of programs (i.e., it depends only on the function computed by M, not on its syntax). For any non‑trivial property (neither always true nor always false), Rice's theorem states that this set is undecidable. Hence no computable validator can exist. ∎

A.5.3 Theorem 6.4 (Impossibility of perfect non‑interference)

Theorem. No computable generator can guarantee exact non‑interference for all prompts.

Proof. Suppose such a generator G exists. Consider a family of prompts P_n that ask for a program that outputs 1 if Turing machine n halts on input n, and 0 otherwise. The secret in the prompt could be the halt status, but exact non‑interference would require that the output distribution is the same regardless of whether the machine halts. However, the program must distinguish the two cases to satisfy the specification; otherwise it would be incorrect. A more precise argument: encode the halting problem as follows: for a Turing machine T, define a prompt P_T that instructs the generator to produce a program that, when run, outputs 1 if T halts and 0 otherwise. The generator must produce a distribution over programs. If the generator were perfectly non‑interfering, the output distribution would be the same for all secrets (i.e., for all T with the same halting status). But then an observer could not distinguish halting from non‑halting machines, which contradicts the fact that the generated program must behave differently. A rigorous reduction: assume G exists and is computable. Then we could decide the halting problem by running G on P_T and then running the generated program on a fixed input; if the program outputs 1, then T halts; if it outputs 0, then T does not halt. But this requires that the generated program is deterministic and that we can run it; however, G may produce a distribution, not a single program. We can still decide by sampling: if the probability of outputting 1 is > 1/2, then we guess halting. This gives a probabilistic algorithm, not a deterministic one. To get a contradiction with computability, we need a stronger assumption: that G produces a single program deterministically. But LLMs are probabilistic. The theorem as stated is usually proved for deterministic generators. For probabilistic generators, the impossibility is of a different nature: we cannot have exact non‑interference because the distributions must be identical, which is a very strong condition. It is still impossible to have a computable generator that produces exactly the same distribution for all secrets, because that would require solving the halting problem to determine the distribution. We'll give a simpler argument: if the generator were computable and produced a distribution, then the function mapping a prompt to the distribution would be computable. But the distribution must assign probability 1 to a program that correctly decides the halting problem for a given input, which is impossible because that program would have to exist. Actually, the program itself is generated, so we are not deciding the halting problem directly; we are generating a program that decides it. That program exists for each T (e.g., a program that simulates T and outputs 1 if it halts, 0 otherwise). So the generator could output that program. The problem is that to output the correct program, the generator must know whether T halts, which is undecidable. Therefore, no computable generator can always produce a program that correctly decides the halting problem for an arbitrary T. This shows that a generator that is both correct (produces a program satisfying the specification) and perfectly non‑interfering is impossible. The theorem in the main text states "exact non‑interference", not necessarily correctness. But if the generator is not required to be correct, it could output a constant program that does nothing, which would be perfectly non‑interfering but useless. So the interesting case is when the generator is required to be correct. We'll assume correctness is part of the specification. Then the impossibility follows from undecidability. ∎

A.5.4 Theorem 6.5 (Verification hardness)

Theorem. For a fixed security property \phi that is NP‑hard, any validator that decides \phi for all programs must run in exponential time in the worst case, assuming \mathrm{P} \neq \mathrm{NP}.

Proof. This is a standard complexity‑theoretic statement: if a problem is NP‑hard, then it cannot be solved in polynomial time unless P = NP. Hence any algorithm (validator) that decides it must take super‑polynomial time in the worst case. The exponential lower bound is a consequence of the exponential time hypothesis (ETH) but in general we only know that it is not in P. The theorem is stated as an exponential bound, which is a safe over‑approximation (since any deterministic algorithm can be made exponential by brute force). ∎

A.5.5 Theorem 6.7 (Sample complexity)

Theorem. To achieve (\varepsilon,\delta)-PAC security for a property class of VC‑dimension d, the training set size must satisfy

m = \Omega\left( \frac{d + \log(1/\delta)}{\varepsilon} \right).

Proof. This is a standard PAC lower bound [87]. For any learning algorithm that outputs a hypothesis (generator) that is supposed to be (\varepsilon,\delta)-PAC, there exists a distribution over prompts such that any algorithm needs at least that many samples to achieve the guarantee. The bound follows from the fact that the class has VC‑dimension d, so there are at least 2^d distinct labelings that must be distinguished, requiring at least d samples. The \log(1/\delta)/\varepsilon term comes from the need to estimate probabilities accurately. A detailed proof can be found in [59]. ∎

A.5.6 Lemma 6.8 (Noise floor)

Lemma. In a multi‑agent system where agents are implemented by Transformers with softmax attention, the probability that two agents independently agree on a security property \phi (i.e., both correctly classify a given term) is at most 1 - \Omega(1/\sqrt{d}), where d is the context window size.

Proof sketch. This result is from [58]. The key idea is that softmax attention computes a weighted average of value vectors, where the weights are given by \mathrm{softmax}(QK^\top / \sqrt{d_k}). For the agents to agree on a subtle property, they must attend to the same relevant tokens. However, due to the inherent noise in the attention mechanism (the softmax cannot focus on a single token unless the logits are exponentially large), the probability that both agents pick exactly the same set of tokens is bounded away from 1. More formally, consider the case where the property depends on a single token. The attention distribution over tokens is a random variable (due to randomness in training or in the input). The probability that two independent draws from this distribution assign highest weight to the same token is at most 1 - c/\sqrt{d} for some constant c, by concentration inequalities for the maximum of d random variables. This gives the \Omega(1/\sqrt{d}) gap. ∎

---

Appendix B: Glossary of Notation

Symbol Meaning First Appearance
\Lambda Set of all λ‑terms Def. 2.1
\mathcal{D}(\Lambda) Set of probability distributions over \Lambda Def. 2.3
\mathcal{O} LLM oracle: \text{String} \to \mathcal{D}(\Lambda) Def. 2.3
\mathcal{V} Set of valid terms Def. 2.4
\bot Hallucination/error state Def. 2.2
\text{Valid}(M) Predicate indicating term M is valid Def. 2.4
\mathcal{A} Set of available package names Def. 2.4
\mathrm{fn}(M) Free names of term M Def. 2.6
a \# M Name a is fresh for M Def. 2.6
M \xrightarrow{p} M' Probabilistic reduction with probability p Sec. 3.2
\text{Sec}_\varepsilon(T) Type of secure terms with leakage bound \varepsilon Def. 3.2
\text{Unk}(T) Type of unverified terms Def. 3.2
D_{TV} Total variation distance Def. 2.10
H, H_\infty Shannon entropy, min‑entropy Def. 2.9
\mathcal{L} Min‑entropy leakage Def. 2.9
\mathrm{wp} Weakest pre‑expectation operator Def. 4.3
\{P\}M\{Q\} Probabilistic Hoare triple Def. 2.15
\mathsf{Gen}, \mathsf{Val}, \mathsf{Rep} Agent processes Defs. 5.1–5.3
r_G Generator's probability of producing a secure term Sec. 5.2
\delta Validator's false accept probability Sec. 5.2
t Repair success probability Sec. 5.2
\eta Channel tampering capacity Thm. 5.9
\mathcal{S}_\phi Set of programs satisfying property \phi Thm. 6.1
d Context window size Lemma 6.8

---

Appendix C: Index of Definitions, Theorems, and Rules

This index lists the main definitions, lemmas, theorems, and proof rules by their number and the page (or section) where they appear. Since page numbers depend on the final formatting, we refer to chapter and section numbers.

