An Integrated Probabilistic Framework for Rare Earth Element Discovery and Economic Valuation Under Geological and Market Uncertainty: The GEO-ECON Model

Chapter 1: Introduction

1.1 Strategic Context and Motivation

The valuation of rare earth element (REE) deposits represents a high-dimensional problem at the confluence of geoscience and financial economics. These critical materials underpin modern technologies and energy transition infrastructures, yet their project financing is hampered by profound, compounded uncertainties. Geological uncertainty arises from the inherent sparsity of direct subsurface measurements and the complex, multimodal nature of ore distribution. Economic uncertainty stems from volatile, cross-correlated global markets influenced by technological shifts and geopolitical factors. Traditional valuation methodologies exhibit a critical epistemological flaw: they decouple these domains. Sophisticated geostatistical models that quantify spatial resource uncertainty are systematically reduced to deterministic inputs—typically single-estimate tonnages and grades—for subsequent financial analysis. This financial analysis, in turn, often relies on simplistic, static price forecasts. This sequential reduction collapses the joint probability space, obscuring the tail risks and dependency structures that ultimately determine project viability, leading to potential misallocation of capital in a strategically vital sector.

1.2 Knowledge Gap and Thesis Objective

A review of extant literature reveals a constitutive gap. While advanced methods exist for probabilistic geological modeling (producing ensembles of equi-probable earth models) and for stochastic financial forecasting, no unified framework formally integrates them within a single computational grammar. Specifically, there is an absence of a Bayesian-coupled system that simultaneously preserves the high-resolution, spatially correlated uncertainty of lithology and grade and the path-dependent, temporally correlated uncertainty of multi-element price processes. The objective of this thesis is to construct and formalize such a framework. It aims to develop a rigorous methodology for quantifying the total uncertainty of an REE project's economic value by defining a joint generative model for geology and markets and propagating this combined uncertainty through a valuation function.

1.3 Original Theoretical Contributions

This thesis makes the following primary theoretical contributions:

1. A Novel Integrated Bayesian Architecture: The GEO-ECON framework provides the first formal mathematical integration of Plurigaussian Simulation (PGS) for categorical lithology fields, Gaussian Mixture Models (GMMs) for multimodal, multivariate grade distributions, and multivariate mean-reverting processes for commodity prices into a cohesive stochastic system.
2. A Formalized Coupling of Spatial and Temporal Stochastic Processes: It establishes a rigorous mathematical procedure for generating joint realizations {G^(r)(x), C^(r)(x), P^(r)_t} where spatial dependency (geology) and temporal dependency (markets) are maintained, allowing for the consistent calculation of emergent metrics like the covariance between grade heterogeneity and price paths.
3. A Foundation for Probabilistic Risk Decomposition: The framework is designed to enable global variance-based sensitivity analysis (e.g., Sobol' indices) on the resulting Net Present Value (NPV) distribution. This formally quantifies the contribution of each stochastic input—from discrete lithological boundaries to the volatility parameters of dysprosium prices—to the overall financial risk.

Chapter 2: Literature Review & Theoretical Foundations

2.1 Geostatistics and Probabilistic Resource Modeling

Traditional geostatistics, rooted in variography and kriging, often assumes stationarity and unimodality within homogeneous domains. For complex REE deposits, this is insufficient. This thesis builds upon advanced spatial models including Plurigaussian Simulation (PGS), which uses truncated Gaussian random fields to honor complex geological rules and contact relationships, and Markov Random Fields (MRFs), which model spatial dependency through local conditional probabilities. These methods provide the mathematical basis for generating ensembles of 3D lithological realizations, moving beyond single "best-estimate" models.

2.2 Machine Learning for Geochemical Distribution Modeling

The assumption of a unimodal (e.g., lognormal) distribution for element grades within a rock type is frequently violated. Gaussian Mixture Models (GMMs), fitted via the Expectation-Maximization (EM) algorithm, provide a probabilistic framework for identifying sub-populations (e.g., background, anomalous, ore-grade) within geochemical data. Their capacity to model multivariate correlations between different REEs is crucial for deposits where elements co-vary in specific mineralogical phases.

2.3 Stochastic Processes in Commodity Economics

Financial models for mineral project valuation have evolved from deterministic forecasts to stochastic processes. While Geometric Brownian Motion (GBM) is common, empirical evidence for many commodities suggests mean-reversion due to long-run supply-demand equilibria. The Ornstein-Uhlenbeck (OU) process and its multivariate extensions provide a more robust theoretical foundation for modeling REE prices, capturing cyclicality and the dynamic correlations between different elements (e.g., Nd and Pr).

2.4 Decision Theory Under Deep Uncertainty

Classical Discounted Cash Flow (DCF) analysis is ill-equipped for deep, non-parametric uncertainty. Decision theory under uncertainty, utilizing full probability distributions of outcomes, forms the normative basis for valuation. Global Sensitivity Analysis (GSA), particularly the Sobol' variance-decomposition method, is established as the mathematical tool for apportioning output uncertainty to specific inputs in non-linear, non-additive models, informing where information-gathering yields the highest value.

2.5 Synthesis: The Theoretical Integration Gap

The literature demonstrates sophisticated, siloed advancements in spatial statistics and financial econometrics. However, the theoretical integration of these domains—creating a closed-form mathematical system where the output distribution of an economic metric is derived from first principles of geological and market randomness—remains unformalized. This thesis contends that such integration is not merely an application but a novel methodological contribution, defining the GEO-ECON framework to bridge this gap.

Chapter 3: The GEO-ECON Mathematical Framework

3.1 Formal Problem Definition

Let the spatial domain of the deposit be Ω ⊂ ℝ³. We define:

· G(x): A categorical random field representing lithology at location x ∈ Ω, taking values in {g₁, g₂, ..., g_K}.
· C(x): A continuous multivariate random field representing the vector of D REE concentrations, C(x) ∈ ℝ^D.
· P_t: A continuous multivariate stochastic process representing the time-dependent price vector for the D REEs, P_t ∈ ℝ^D_+ for t ∈ [0, T].

The core problem is to define the joint probability distribution P(G, C, P) such that we can generate a coherent ensemble E = { (G^(r), C^(r), P^(r)) } for r = 1, ..., R. The economic value V is a deterministic functional Φ (e.g., a DCF model) of a realization: V^(r) = Φ(G^(r), C^(r), P^(r)). The ultimate object of study is the induced distribution P_V derived from the pushforward of P(G, C, P) through Φ.

3.2 Module A: Probabilistic Lithology Model (PGS Formalism)

Lithology is modeled as a function of underlying latent Gaussian fields. Let Y₁(x), Y₂(x) be independent, stationary Gaussian random functions. The lithology G(x) is determined by truncating these fields according to a geological rule map:
G(x) = g_k if and only if (Y₁(x), Y₂(x)) ∈ D_k
where {D_k} is a set of non-overlapping domains in ℝ² defined by threshold values. The spatial structure of G(x) is governed by the covariance functions of Y₁ and Y₂. This provides a flexible prior model for complex spatial arrangements and contact rules.

3.3 Module B: Conditional Grade Model (GMM Formalism)

Conditional on lithology G(x) = g_k, the grade vector is modeled as a finite mixture of multivariate Gaussian distributions:
C(x) | G(x)=g_k ~ Σ_{i=1}^{M_k} π_{k,i} * N(μ_{k,i}, Σ_{k,i})
where:

· M_k is the number of mixture components for lithology k.
· π_{k,i} ∈ [0, 1] are mixture weights with Σ_i π_{k,i} = 1.
· μ_{k,i} ∈ ℝ^D is the mean vector for component i.
· Σ_{k,i} ∈ ℝ^{D×D} is the positive-definite covariance matrix for component i.

The parameters θ_k = {π_{k,i}, μ_{k,i}, Σ_{k,i}} for all i, k are the core learned representations of the geochemical signature, capable of expressing multimodality and element correlations.

3.4 Module C: Joint Spatial Simulation (Theoretical Algorithm)

The generation of a coupled geological realization (G^(r), C^(r)) proceeds via a conceptual sequential co-simulation:

1. Simulate the latent Gaussian fields Y₁^(r)(x), Y₂^(r)(x) over a discrete grid, conditional on any transformed hard data.
2. Apply the truncation rule to obtain the lithology realization G^(r)(x).
3. For each location x:
   a. Identify its lithology g_k = G^(r)(x).
   b. Sample a component index z from the categorical distribution defined by weights {π_{k,1}, ..., π_{k,M_k}}.
   c. Draw a grade vector C^(r)(x) from the multivariate Gaussian N(μ_{k,z}, Σ_{k,z}), optionally conditioned on nearby grade data via kriging within the same component.

This algorithm defines a valid sampler from the joint prior P(G, C) specified by Modules A and B.

3.5 Module D: Multivariate Price Process (OU Formalism)

The log-price process is modeled as a multivariate Ornstein-Uhlenbeck (MOU) process:
d ln(P_t) = Θ (μ - ln(P_t)) dt + Σ dW_t
where:

· ln(P_t), μ ∈ ℝ^D are the log-price and long-term equilibrium vectors.
· Θ ∈ ℝ^{D×D} is a positive-definite mean-reversion matrix.
· Σ ∈ ℝ^{D×D} is the Cholesky factor of the instantaneous covariance matrix.
· W_t is a D-dimensional standard Wiener process.

This defines a continuous-time, mean-reverting, and correlated stochastic process for prices. A discrete-time path P^(r)_t for valuation is generated via the Euler-Maruyama discretization of this SDE.

3.6 The Integrated Valuation Functional & Sensitivity Analysis

The valuation functional Φ maps a full realization to a scalar NPV:
NPV^(r) = Φ( G^(r), C^(r), P^(r) ) = Σ_{t=0}^{T} [ Q_t^(r) • P_t^(r) - Cost_t ] / (1+δ)^t
where Q_t^(r) is the vector of recoverable quantities of each REE, derived via a mining function Ψ applied to (G^(r), C^(r)): Q_t^(r) = Ψ( G^(r), C^(r), t ).

The ensemble {NPV^(r)} approximates the distribution P_V. The global sensitivity analysis is then a mathematical investigation of P_V. The first-order Sobol' index for an input group X_u (e.g., all parameters of the price process) is:
S_u = [ V( E[NPV | X_u] ) ] / V(NPV)
This index, estimable via Monte Carlo methods on the integrated framework, quantitatively reveals which stochastic module—geological or economic—contributes most to the variance in project value, providing a theoretical basis for optimal risk mitigation.

---

Appendices

Appendix A: Mathematical Derivations and Algorithms

A.1 Complete Derivation of the EM Algorithm for Gaussian Mixture Models (Module B)

Let  \mathcal{D} = \{ \mathbf{c}_1, \mathbf{c}_2, ..., \mathbf{c}_N \}  be a set of  D -dimensional grade vectors for a specific lithology  g_k . We assume the data is generated from a mixture of  M  multivariate Gaussian distributions. The complete-data likelihood, including latent component labels  z_j \in \{1, ..., M\} , is:

P(\mathcal{D}, \mathbf{Z} | \boldsymbol{\theta}) = \prod_{j=1}^{N} \prod_{i=1}^{M} \left[ \pi_i \mathcal{N}(\mathbf{c}_j | \boldsymbol{\mu}_i, \boldsymbol{\Sigma}_i) \right]^{\mathbb{I}(z_j = i)}

where  \boldsymbol{\theta} = \{ \pi_i, \boldsymbol{\mu}_i, \boldsymbol{\Sigma}_i \}_{i=1}^M , and  \mathbb{I}(\cdot)  is the indicator function.

The E-step computes the posterior responsibility of component  i  for data point  j , using the current parameter estimates  \boldsymbol{\theta}^{(t)} :

\gamma_{ji}^{(t)} = \mathbb{E}[\mathbb{I}(z_j = i) | \mathbf{c}_j, \boldsymbol{\theta}^{(t)}] = \frac{\pi_i^{(t)} \mathcal{N}(\mathbf{c}_j | \boldsymbol{\mu}_i^{(t)}, \boldsymbol{\Sigma}_i^{(t)})}{\sum_{m=1}^{M} \pi_m^{(t)} \mathcal{N}(\mathbf{c}_j | \boldsymbol{\mu}_m^{(t)}, \boldsymbol{\Sigma}_m^{(t)})}

The M-step maximizes the expected complete-data log-likelihood,  Q(\boldsymbol{\theta}, \boldsymbol{\theta}^{(t)}) = \mathbb{E}_{\mathbf{Z}|\mathcal{D}, \boldsymbol{\theta}^{(t)}}[\log P(\mathcal{D}, \mathbf{Z} | \boldsymbol{\theta})] , yielding closed-form updates:

\begin{aligned}
N_i^{(t+1)} &= \sum_{j=1}^N \gamma_{ji}^{(t)} \\
\pi_i^{(t+1)} &= \frac{N_i^{(t+1)}}{N} \\
\boldsymbol{\mu}_i^{(t+1)} &= \frac{1}{N_i^{(t+1)}} \sum_{j=1}^N \gamma_{ji}^{(t)} \mathbf{c}_j \\
\boldsymbol{\Sigma}_i^{(t+1)} &= \frac{1}{N_i^{(t+1)}} \sum_{j=1}^N \gamma_{ji}^{(t)} (\mathbf{c}_j - \boldsymbol{\mu}_i^{(t+1)})(\mathbf{c}_j - \boldsymbol{\mu}_i^{(t+1)})^\top
\end{aligned}

Iteration of the E- and M-steps is guaranteed to increase the log-likelihood  \log P(\mathcal{D} | \boldsymbol{\theta})  monotonically until convergence to a local maximum.

A.2 Implementation Algorithm for Plurigaussian Simulation (Module A)

Inputs:

1. A grid  \Omega  discretizing the spatial domain.
2. Lithology rule map: Truncation thresholds defining domains  D_k  in the Gaussian space.
3. Covariance models  C_1(\mathbf{h}), C_2(\mathbf{h})  for the two underlying Gaussian fields  Y_1, Y_2 .
4. Hard data: Known lithologies at specific locations, transformed to Gaussian constraints.

Algorithm:

1. Conditioning: Solve kriging systems to condition the Gaussian fields  Y_1, Y_2  to any hard data (e.g., known lithology at a point implies a constraint on the possible  (Y_1, Y_2)  values).
2. Sequential Simulation: Visit all grid nodes in a random order. For each node  \mathbf{x}_0 :
   a. Use simple kriging with covariance  C_1  to estimate the mean and variance of  Y_1(\mathbf{x}_0)  given all previously simulated nodes and hard data.
   b. Draw a random value  y_1^*  from the conditional Gaussian distribution.
   c. Repeat steps (a-b) for  Y_2(\mathbf{x}_0) , accounting for any cross-correlation if the fields are not independent.
   d. Apply the truncation rule: Assign lithology  g_k  to  \mathbf{x}_0  if  (y_1^*, y_2^*) \in D_k .
3. Post-processing: Repeat the simulation with different random paths to generate multiple realizations  \{ G^{(r)}(\mathbf{x}) \} .

A.3 Discretization of the Multivariate Ornstein-Uhlenbeck Process (Module D)

The continuous-time MOU process is given by:

d \ln(\mathbf{P}_t) = \boldsymbol{\Theta} (\boldsymbol{\mu} - \ln(\mathbf{P}_t)) dt + \boldsymbol{\Sigma} d\mathbf{W}_t

For computational simulation, we discretize time into intervals  \Delta t . The exact discrete-time solution, conditional on  \ln(\mathbf{P}_t) , is:

\ln(\mathbf{P}_{t+\Delta t}) = \boldsymbol{\mu} + e^{-\boldsymbol{\Theta} \Delta t}(\ln(\mathbf{P}_t) - \boldsymbol{\mu}) + \boldsymbol{\xi}

where  \boldsymbol{\xi}  is a multivariate normal random vector with mean  \mathbf{0}  and covariance matrix  \boldsymbol{\Gamma}  given by:

\boldsymbol{\Gamma} = \int_0^{\Delta t} e^{-\boldsymbol{\Theta} s} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^\top (e^{-\boldsymbol{\Theta} s})^\top ds

If  \boldsymbol{\Theta}  is diagonal, this simplifies. A common practical implementation uses the Euler-Maruyama approximation for a small  \Delta t :

\ln(\mathbf{P}_{t+\Delta t}) = \ln(\mathbf{P}_t) + \boldsymbol{\Theta} (\boldsymbol{\mu} - \ln(\mathbf{P}_t)) \Delta t + \boldsymbol{\Sigma} \sqrt{\Delta t} \boldsymbol{\epsilon}_t

where  \boldsymbol{\epsilon}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) . This allows straightforward generation of correlated price paths  \{ \mathbf{P}_t^{(r)} \} .

A.4 Sobol' Indices for Global Sensitivity Analysis (Chapter 3.6)

The Sobol' variance decomposition represents the total variance  V  of the model output  Y = f(\mathbf{X})  (e.g., NPV) as a sum of contributions from inputs  \mathbf{X} = (X_1, ..., X_d)  and their interactions:

V(Y) = \sum_{i=1}^d V_i + \sum_{1 \le i < j \le d} V_{ij} + ... + V_{12...d}

where  V_i = V[\mathbb{E}(Y|X_i)] ,  V_{ij} = V[\mathbb{E}(Y|X_i, X_j)] - V_i - V_j , etc.

The first-order Sobol' index  S_i  measures the main effect of  X_i :

S_i = \frac{V_i}{V(Y)}

The total-order Sobol' index  S_{T_i}  measures the total contribution of  X_i , including all its interactions:

S_{T_i} = 1 - \frac{V[\mathbb{E}(Y|\mathbf{X}_{\sim i})]}{V(Y)}

where  \mathbf{X}_{\sim i}  denotes all input variables except  X_i .

These indices are typically estimated via (quasi-)Monte Carlo methods. A common efficient approach is the Saltelli sampler, which requires  N \times (2d + 2)  model evaluations to compute all first-order and total-effect indices, where  N  is a base sample size (e.g., 1,000-10,000).

---

Appendix B: Implementation Blueprint and Software Considerations

B.1 Proposed Modular Software Architecture

The GEO-ECON framework is designed for implementation in a high-level scientific programming language, preferably Python, due to its extensive ecosystem. The architecture should be modular, mirroring the theoretical chapters:

· Module A (PGS) & Module B (GMM): Can be implemented using a combination of gstools (for variogram modeling and random field generation) and scikit-learn (for Gaussian Mixture Model fitting). Custom code is required to implement the lithology truncation rule and the sequential cosimulation algorithm that draws from lithology-specific GMMs.
· Module D (MOU): Can be implemented using numpy for linear algebra and stochastic number generation. The statsmodels library may assist in the initial calibration of the OU process parameters from historical price data.
· Valuation Functional (Φ) & Integration: The DCF engine and the master Monte Carlo loop orchestrating the modules can be implemented in pure numpy/pandas. For performance-critical sections, numba can be used for just-in-time compilation.
· Sensitivity Analysis (Sobol'): The SALib (Sensitivity Analysis Library in Python) provides robust, efficient implementations of the Saltelli sampler and index calculators.

Data Flow: The implementation should enforce a clear data pipeline:

1. Input Layer: Configuration files (JSON/YAML) defining geological rules, economic parameters, and simulation controls.
2. Simulation Core: Modules A, B, and D run independently or in a coordinated loop, passing realizations via standardized in-memory data structures (e.g., xarray DataArrays for gridded geology, pandas DataFrames for price paths).
3. Analysis Layer: Functions to compute NPV, aggregate results across the ensemble, and calculate risk metrics and Sobol' indices.
4. Visualization & Output: Routines (using matplotlib, plotly, or pyvista) to generate figures of realizations, NPV distributions, and sensitivity tornado plots. Results should be saved in standardized formats (NetCDF/HDF5 for grids, CSV/Parquet for tables).

B.2 Conceptual Workflow for a Single Monte Carlo Realization

The following pseudo-code illustrates the integrated workflow for generating a single realization r and its corresponding NPV_r:

```python
# Pseudo-code for Integrated GEO-ECON Realization
function generate_realization(r, parameters):
    # 1. Simulate Geology
    G_r, Y1_r, Y2_r = plurigaussian_simulate(parameters.grid, parameters.cov_model, parameters.rules, seed=r)
    
    # 2. Simulate Grades conditioned on Geology
    C_r = empty_grid(parameters.grid)
    for each grid cell x in random_path:
        lithology = G_r[x]
        gmm_params = parameters.gmm_models[lithology] # Contains π, μ, Σ for this rock type
        # Sample component z, then grade from N(μ_z, Σ_z), with spatial conditioning
        C_r[x] = conditional_gmm_draw(gmm_params, neighbors(C_r), Y1_r[x], Y2_r[x])
    
    # 3. Simulate Price Path
    P_r = simulate_mou_prices(parameters.mou_mu, parameters.mou_theta, parameters.mou_sigma, 
                               T=parameters.mine_life, dt=parameters.dt, seed=r+offset)
    
    # 4. Apply Mining & Recovery Model (Ψ) to get production schedule Q_r
    Q_r = apply_mining_schedule(G_r, C_r, parameters.mining_plan, parameters.recovery_factors)
    
    # 5. Compute NPV via DCF (Φ)
    NPV_r = 0
    for t in range(parameters.mine_life):
        revenue = dot(Q_r[t, :], P_r[t, :]) # Vector dot product: REE quantities * prices
        cost = parameters.cost_model(Q_r[t, :], t)
        cashflow = (revenue - cost) / ((1 + parameters.discount_rate) ** t)
        NPV_r += cashflow
    
    return NPV_r, G_r, C_r, P_r # Optional: return full realization data
```

B.3 Notes on Computational Performance and Parallelization

The framework is embarrassingly parallel at the level of Monte Carlo realizations. Each realization r is independent. A production implementation must leverage this:

· Use Python's multiprocessing or concurrent.futures modules for CPU parallelism on a single machine.
· For larger ensembles (>10,000 realizations) or very high-resolution grids, consider distributed computing frameworks like dask or ray, or implement the core simulation loops in a compiled language (C++, Rust) called from Python.
· Memory management is critical for storing multiple realizations of 3D grids. Consider an out-of-core strategy where realizations are processed in batches and only aggregate statistics (like NPV samples) are kept in memory, or use memory-mapped array storage (e.g., via zarr).

