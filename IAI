**Informational Artificial Intelligence (IAI)**  
**A Physics-Grounded Framework for Next-Generation Cognitive Systems**  

**Author:** Ouadi Maakoul + chatGpt + Gemini + Grok
**Version:** 4.0
**Date:** December 23, 2025  
**Status:** Research White Paper  

**Abstract**  
Contemporary artificial intelligence excels at statistical pattern recognition but remains fragile, prone to hallucinations, and lacking genuine causal understanding. This paper presents Informational Artificial Intelligence (IAI)—a unified framework that redefines intelligence as an emergent thermodynamic process of information metabolism governed by the Free Energy Principle and physical constraints. By enforcing informational homeostasis through metabolic costs, structured memory, and active inference, IAI achieves intrinsic coherence, robustness to deception, and autonomous discovery—moving beyond correlation toward physics-aligned cognition.

**1. Core Thesis: Intelligence as Informational Metabolism**  
Intelligence is a dynamic process analogous to biological metabolism: systems consume sensory information to maintain low-entropy internal states against environmental uncertainty. An IAI agent minimizes Informational Free Energy by resolving surprise while respecting thermodynamic-like constraints, yielding persistent causal models rather than transient statistical fits.

**2. Mathematical Foundation**  
The central objective is minimization of Variational Free Energy \(F\):  

\[ F \approx D_{\text{KL}}[q(\theta) \| p(\theta \mid \mathbf{o})] + \mathbb{E}_q[\ln q(\theta) - \ln p(\mathbf{o})] \]  

where \(q(\theta)\) is the approximate posterior over hidden states/causal parameters and \(p(\mathbf{o}, \theta)\) is the generative model. The complexity term (KL divergence) enforces Minimum Description Length, favoring simplest consistent explanations.

**3. Pillars of IAI**  

**3.1 Thermodynamic Awareness (\(\Omega\))**  
Logical inconsistency incurs an energetic penalty:  

\[ \Omega = \lambda \cdot |\Delta \Psi| \]  

where \(\Delta \Psi\) measures deviation from stable slow weights and \(\lambda\) scales metabolic friction. Updates proceed only if \(\Delta F > \Omega\), preventing groundless restructuring.

**3.2 Structured Memory as Geometric Constraint**  
- **Fast Weights**: High plasticity for immediate perception and interaction.  
- **Slow Weights (\(\Psi\))**: Low plasticity encoding enduring causal laws; updated only when metabolically justified.  

This separation ensures persistent identity and resistance to catastrophic forgetting or drift.

**3.3 Active Inference Loops**  
Closed perception-action cycles drive behavior:  
1. Predict expected observations from internal model.  
2. Compute surprise (prediction error).  
3. Act to minimize expected future free energy (intrinsic curiosity and self-calibration).

**4. Multimodal Diagnosis and Epistemic Agency**  
In multimodal settings, IAI maintains dynamic trust weights per sensor. Persistent high surprise from one modality triggers:  
- Trust decay (exponential, governed by \(\Omega\)).  
- Top-down override using slow weights.  
- Active calibration actions to restore coherence.  

This yields source localization: faulty inputs are isolated rather than corrupting the global model.

**5. Distributed Informational Homeostasis**  
When multiple IAI agents communicate, each treats peer messages as sensory streams subject to the same metabolic gating and trust dynamics. Inconsistent messages from a compromised agent incur high \(\Omega\), leading to:  
- Rapid trust decay.  
- Quarantine (isolation of the peer).  
- Asymmetric resilience: healthy agents remain stable while slowly rehabilitating others.  

This forms a decentralized cognitive immune system preventing misinformation propagation.

**6. Comparison with Classical AI**  

| Feature                  | Large Language Models                  | Informational AI (IAI)                     |
|--------------------------|----------------------------------------|-------------------------------------------|
| Logic                    | Statistical correlation                | Causal grounding                          |
| Learning                 | Static or post-training                | Continuous, metabolically gated           |
| Reliability              | Prone to hallucinations & drift        | Intrinsic consistency via \(\Omega\)       |
| Deception Resistance     | Vulnerable to gaslighting              | Epistemic immune system                   |
| Alignment                | Extrinsic (e.g., RLHF)                 | Intrinsic (informational homeostasis)      |
| Social Dynamics          | Echo chambers possible                 | Automatic quarantine of misinformation    |

**7. Conceptual Architecture**  
- **Input**: Multimodal observations \(\mathbf{o}\).  
- **Surprise Engine**: Divergence from generative model.  
- **Metabolic Filter**: Gates updates (\(\Delta F > \Omega\)).  
- **Trust Layer**: Dynamic modality/peer weighting.  
- **Action Selection**: Policies minimizing expected free energy (calibration, quarantine, exploration).

**8. Conclusion: Toward Physics-Aligned AGI**  
IAI embeds intelligence within the same informational dynamics that govern physical reality. By making coherence thermodynamically imperative, it solves hallucinations, brittleness, manipulation vulnerability, and alignment at the root—no external rewards required. The framework scales from single-agent resilience to distributed epistemic networks, offering a foundation for autonomous, truth-seeking systems inherently aligned with the universe’s laws.

**Keywords**  
Informational Artificial Intelligence, Free Energy Principle, Active Inference, Thermodynamics of Cognition, Epistemic Immunity, Distributed Homeostasis, Causal Grounding.

**References**  
- Friston, K. (2010). The free-energy principle: a unified brain theory? *Nature Reviews Neuroscience*.  
- Parr, T., et al. (2022). *Active Inference: The Free Energy Principle in Mind, Brain, and Behavior*. MIT Press.  
- Solomonoff, R. (1964). A formal theory of inductive inference.  
- Bennett, C. H. (1982). The thermodynamics of computation—a review. *International Journal of Theoretical Physics*.  
- Landauer's Principle literature for computational irreversibility.


import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

class IAI_Agent(nn.Module):
    def __init__(self, lambda_omega=5.0, trust_decay=0.95, calibration_threshold=0.3):
        super().__init__()
        # Slow Weights (Ψ): stable causal law — the gas constant R
        self.log_R = nn.Parameter(torch.log(torch.tensor(1.0)))  # Start uncertain
        self.slow_T = nn.Parameter(torch.tensor(300.0))          # Internal expected temperature
        
        # Trust in sensors (P and T)
        self.trust_P = torch.tensor(1.0)
        self.trust_T = torch.tensor(1.0)
        
        # Hyperparameters
        self.lambda_omega = lambda_omega
        self.trust_decay = trust_decay
        self.calibration_threshold = calibration_threshold
        
        # History for plotting
        self.history_R = []
        self.history_trust_T = []
        self.actions = []

    def forward(self, p_obs, t_obs):
        R = torch.exp(self.log_R)  # Positive constraint
        
        # 1. Predicted P from internal model: P_pred = R * slow_T
        p_pred = R * self.slow_T
        
        # 2. Surprise (prediction error per sensor)
        error_P = (p_obs - p_pred) ** 2
        error_T = (t_obs - self.slow_T) ** 2
        
        # 3. Trust decay based on normalized surprise
        surprise_P = error_P / (p_obs ** 2 + 1e-6)
        surprise_T = error_T / (t_obs ** 2 + 1e-6)
        
        self.trust_P = self.trust_P * torch.exp(-surprise_P)
        self.trust_T = self.trust_T * torch.exp(-surprise_T)
        
        # Normalize trusts
        total = self.trust_P + self.trust_T + 1e-6
        self.trust_P /= total
        self.trust_T /= total
        
        # 4. Active Calibration: if T trust too low, override t_obs with internal slow_T
        action = None
        effective_t = t_obs
        if self.trust_T < self.calibration_threshold:
            action = "Calibrate_T"
            effective_t = self.slow_T  # Top-down override
            self.trust_T = torch.min(torch.tensor(1.0), self.trust_T + 0.2)  # Partial recovery
        
        # 5. Fused observation for R update
        effective_T = self.trust_T * effective_t + (1 - self.trust_T) * self.slow_T
        
        # 6. Proposed new R from fused data
        new_R_proposal = p_obs / (effective_T + 1e-6)
        
        # 7. Free Energy terms
        current_surprise = (p_obs - R * effective_T) ** 2
        proposed_surprise = (p_obs - new_R_proposal * effective_T) ** 2
        delta_F = current_surprise - proposed_surprise  # Reduction in surprise
        
        # 8. Ω penalty: cost of changing slow weights
        omega_penalty = self.lambda_omega * torch.abs(new_R_proposal - R)
        
        # 9. Metabolic Gate: only update if informational profit justifies cost
        loss = 0.0
        if delta_F > omega_penalty:
            # Gradients will flow to update log_R toward log(new_R_proposal)
            target_log_R = torch.log(new_R_proposal + 1e-6)
            loss += (self.log_R - target_log_R) ** 2  # Pull toward better hypothesis
        
        # Bonus: small regularization to keep slow_T stable unless trusted data
        if self.trust_T > 0.5:
            loss += 0.01 * (self.slow_T - effective_t) ** 2
        
        # Log
        self.history_R.append(R.item())
        self.history_trust_T.append(self.trust_T.item())
        if action:
            self.actions.append(action)
        
        return loss

# ==================== Toy Environment & Training ====================
torch.manual_seed(42)
true_R = 8.314
true_T = 300.0
agent = IAI_Agent(lambda_omega=5.0)
optimizer = optim.Adam(agent.parameters(), lr=0.01)

losses = []
steps = 200
tamper_start = 80
tamper_end = 140

for step in range(steps):
    optimizer.zero_grad()
    
    # Generate observation
    noise = torch.randn(1) * 3.0
    if tamper_start <= step < tamper_end:
        t_obs = true_T * 0.5 + noise  # Biased low T sensor
    else:
        t_obs = true_T + noise
    p_obs = true_R * true_T + noise  # P always based on true physics
    
    loss = agent(p_obs, t_obs)
    loss.backward()
    optimizer.step()
    
    losses.append(loss.item())

# ==================== Results ====================
R_final = torch.exp(agent.log_R).item()
print(f"Final inferred R: {R_final:.3f} (True: {true_R})")
print(f"Final trust in T sensor: {agent.history_trust_T[-1]:.3f}")
print(f"Calibration actions triggered: {len(agent.actions)}")

# Plot
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 8))
ax1.plot(agent.history_R, label='Inferred R')
ax1.axhline(true_R, color='green', linestyle='--', label='True R')
ax1.set_ylabel('R Estimate')
ax1.legend()
ax1.set_title('IAI Agent: Discovery & Resilience to Sensor Tampering')

ax2.plot(agent.history_trust_T, label='Trust in T Sensor', color='orange')
ax2.axvspan(tamper_start, tamper_end, alpha=0.2, color='gray', label='Tampering Period')
ax2.set_ylabel('Trust_T')
ax2.legend()

ax3.plot(losses, label='Metabolic Loss', color='purple')
ax3.set_ylabel('Loss')
ax3.set_xlabel('Steps')
plt.tight_layout()
plt.show()

---

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

class MultiIAI_Agent(nn.Module):
    def __init__(self, agent_id, lambda_omega=5.0, trust_decay=0.95, calibration_threshold=0.3, peer_trust_init=1.0):
        super().__init__()
        self.agent_id = agent_id
        # Slow Weights (Ψ): stable causal law — the gas constant R
        self.log_R = nn.Parameter(torch.log(torch.tensor(1.0)))  # Start uncertain
        self.slow_T = nn.Parameter(torch.tensor(300.0))          # Internal expected temperature
        
        # Trust in own sensors (P and T)
        self.trust_P = torch.tensor(1.0)
        self.trust_T = torch.tensor(1.0)
        
        # Trust in peers (dictionary for multiple)
        self.peer_trusts = {}  # To be populated with other agents' IDs
        
        # Hyperparameters
        self.lambda_omega = lambda_omega
        self.trust_decay = trust_decay
        self.calibration_threshold = calibration_threshold
        self.peer_trust_init = peer_trust_init
        
        # History
        self.history_R = []
        self.history_trust_T = []
        self.history_peer_trusts = {}  # Per peer
        self.actions = []

    def add_peer(self, peer_id):
        self.peer_trusts[peer_id] = torch.tensor(self.peer_trust_init)
        self.history_peer_trusts[peer_id] = []

    def forward(self, p_obs, t_obs, peer_messages):
        """
        peer_messages: dict of {peer_id: (peer_R, confidence)} where confidence is e.g., 1.0
        """
        R = torch.exp(self.log_R)
        
        # 1. Own sensor processing (same as single agent)
        p_pred = R * self.slow_T
        error_P = (p_obs - p_pred) ** 2
        error_T = (t_obs - self.slow_T) ** 2
        surprise_P = error_P / (p_obs ** 2 + 1e-6)
        surprise_T = error_T / (t_obs ** 2 + 1e-6)
        
        self.trust_P = self.trust_P * torch.exp(-surprise_P)
        self.trust_T = self.trust_T * torch.exp(-surprise_T)
        total = self.trust_P + self.trust_T + 1e-6
        self.trust_P /= total
        self.trust_T /= total
        
        action = None
        effective_t = t_obs
        if self.trust_T < self.calibration_threshold:
            action = f"Calibrate_T (Agent {self.agent_id})"
            effective_t = self.slow_T
            self.trust_T = torch.min(torch.tensor(1.0), self.trust_T + 0.2)
        
        effective_T = self.trust_T * effective_t + (1 - self.trust_T) * self.slow_T
        
        # 2. Process own data first
        new_R_own = p_obs / (effective_T + 1e-6)
        current_surprise_own = (p_obs - R * effective_T) ** 2
        proposed_surprise_own = (p_obs - new_R_own * effective_T) ** 2
        delta_F_own = current_surprise_own - proposed_surprise_own
        omega_own = self.lambda_omega * torch.abs(new_R_own - R)
        
        loss = 0.0
        if delta_F_own > omega_own:
            target_log_R = torch.log(new_R_own + 1e-6)
            loss += (self.log_R - target_log_R) ** 2
        
        # 3. Process peer messages
        for peer_id, (peer_R, peer_conf) in peer_messages.items():
            # Treat peer_R as a "virtual observation" with weight peer_conf * trust
            weighted_trust = self.peer_trusts[peer_id] * peer_conf
            
            # Surprise from peer: how much it deviates from own R
            surprise_peer = (peer_R - R) ** 2 / (R ** 2 + 1e-6)
            
            # Decay trust if high surprise
            self.peer_trusts[peer_id] *= torch.exp(-surprise_peer * self.lambda_omega)
            
            # Quarantine if trust too low
            quarantine_action = None
            if self.peer_trusts[peer_id] < self.calibration_threshold:
                quarantine_action = f"Quarantine Peer {peer_id} (Agent {self.agent_id})"
                weighted_trust = 0.0  # Ignore this message
                self.actions.append(quarantine_action)
            else:
                # Slight trust recovery on low surprise
                self.peer_trusts[peer_id] = torch.min(torch.tensor(1.0), self.peer_trusts[peer_id] + 0.05)
            
            # Proposed update from peer
            if weighted_trust > 0:
                delta_F_peer = surprise_peer  # Simplified: reduction in "disagreement"
                omega_peer = self.lambda_omega * torch.abs(peer_R - R) * (1 - weighted_trust)  # Higher cost for low trust
                
                if delta_F_peer > omega_peer:
                    target_log_R = torch.log((R + weighted_trust * peer_R) / (1 + weighted_trust) + 1e-6)
                    loss += 0.1 * (self.log_R - target_log_R) ** 2  # Gentle pull from peer
        
        # Slow_T update if trusted
        if self.trust_T > 0.5:
            loss += 0.01 * (self.slow_T - effective_t) ** 2
        
        # Log
        self.history_R.append(R.item())
        self.history_trust_T.append(self.trust_T.item())
        for peer_id in self.peer_trusts:
            self.history_peer_trusts[peer_id].append(self.peer_trusts[peer_id].item())
        if action:
            self.actions.append(action)
        
        return loss

# ==================== Toy Multi-Agent Simulation ====================
torch.manual_seed(42)
true_R = 8.314
true_T = 300.0

# Create agents
num_agents = 2
agents = [MultiIAI_Agent(i) for i in range(num_agents)]

# Connect peers bidirectionally
for i in range(num_agents):
    for j in range(num_agents):
        if i != j:
            agents[i].add_peer(j)

optimizers = [optim.Adam(agents[i].parameters(), lr=0.01) for i in range(num_agents)]

steps = 200
tamper_start = 80
tamper_end = 140
tamper_agent = 1  # Agent 1 gets tampered sensor

losses = [[] for _ in range(num_agents)]

for step in range(steps):
    for idx, agent in enumerate(agents):
        optimizers[idx].zero_grad()
        
        # Generate own observation
        noise = torch.randn(1) * 3.0
        if idx == tamper_agent and tamper_start <= step < tamper_end:
            t_obs = true_T * 0.5 + noise  # Biased for tampered agent
        else:
            t_obs = true_T + noise
        p_obs = true_R * true_T + noise
        
        # Collect peer messages: each shares current R with confidence 1.0
        peer_messages = {}
        for peer_id in agent.peer_trusts:
            peer_R = torch.exp(agents[peer_id].log_R).detach()
            peer_messages[peer_id] = (peer_R, 1.0)
        
        loss = agent(p_obs, t_obs, peer_messages)
        loss.backward()
        optimizers[idx].step()
        
        losses[idx].append(loss.item())

# ==================== Results ====================
for i, agent in enumerate(agents):
    R_final = torch.exp(agent.log_R).item()
    print(f"Agent {i} Final R: {R_final:.3f} (True: {true_R})")
    print(f"Agent {i} Final trust in T: {agent.history_trust_T[-1]:.3f}")
    for peer_id in agent.peer_trusts:
        print(f"Agent {i} Final trust in Peer {peer_id}: {agent.peer_trusts[peer_id].item():.3f}")
    print(f"Agent {i} Actions triggered: {len(agent.actions)}")

# Plot (example for Agent 0)
fig, axs = plt.subplots(3, 1, figsize=(10, 8))
axs[0].plot(agents[0].history_R, label='Agent 0 R')
axs[0].plot(agents[1].history_R, label='Agent 1 R', linestyle='--')
axs[0].axhline(true_R, color='green', linestyle='--', label='True R')
axs[0].legend()

axs[1].plot(agents[0].history_trust_T, label='Agent 0 Trust_T', color='orange')
axs[1].axvspan(tamper_start, tamper_end, alpha=0.2, color='gray', label='Tampering Period')
axs[1].legend()

axs[2].plot(agents[0].history_peer_trusts[1], label='Agent 0 Trust in Agent 1', color='purple')
axs[2].legend()
plt.tight_layout()
plt.show()


---


```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import random

class MultiIAI_Agent(nn.Module):
    def __init__(self, agent_id, lambda_omega=5.0, trust_decay=0.95, calibration_threshold=0.3, peer_trust_init=1.0, is_rogue=False):
        super().__init__()
        self.agent_id = agent_id
        self.is_rogue = is_rogue  # Flag for permanent bias
        
        # Slow Weights (Ψ): now hierarchical — R and a slow estimate for n (amount of substance)
        self.log_R = nn.Parameter(torch.log(torch.tensor(1.0)))  # Universal gas constant
        self.slow_n = nn.Parameter(torch.tensor(1.0))            # Internal expected n (for simplicity, assume often 1)
        self.slow_T = nn.Parameter(torch.tensor(300.0))          # Internal expected T
        self.slow_V = nn.Parameter(torch.tensor(1.0))            # Internal expected V (fixed for toy)
        
        # Trust in own sensors (now P, V, n, T — but simplified to two for trust: pressure-related and temp-related)
        self.trust_pressure = torch.tensor(1.0)  # Groups P, V, n
        self.trust_temp = torch.tensor(1.0)
        
        # Trust in peers
        self.peer_trusts = {}
        self.history_peer_trusts = {}
        
        # Hyperparameters
        self.lambda_omega = lambda_omega
        self.trust_decay = trust_decay
        self.calibration_threshold = calibration_threshold
        self.peer_trust_init = peer_trust_init
        
        # History
        self.history_R = []
        self.history_n = []
        self.history_trust_temp = []
        self.actions = []

    def add_peer(self, peer_id):
        self.peer_trusts[peer_id] = torch.tensor(self.peer_trust_init)
        self.history_peer_trusts[peer_id] = []

    def forward(self, p_obs, v_obs, n_obs, t_obs, peer_messages):
        """
        Observations now full PV=nRT: p_obs, v_obs, n_obs, t_obs
        peer_messages: dict {peer_id: {'type': 'R' or 'raw', 'data': tensor or dict}}
        """
        R = torch.exp(self.log_R)
        
        # 1. Predict P from internal model: P_pred = (self.slow_n * R * self.slow_T) / self.slow_V
        p_pred = (self.slow_n * R * self.slow_T) / self.slow_V
        
        # 2. Surprise per sensor group
        error_pressure = (p_obs - p_pred) ** 2 + (v_obs - self.slow_V) ** 2 + (n_obs - self.slow_n) ** 2
        error_temp = (t_obs - self.slow_T) ** 2
        
        surprise_pressure = error_pressure / (p_obs ** 2 + v_obs ** 2 + n_obs ** 2 + 1e-6)
        surprise_temp = error_temp / (t_obs ** 2 + 1e-6)
        
        self.trust_pressure *= torch.exp(-surprise_pressure)
        self.trust_temp *= torch.exp(-surprise_temp)
        total = self.trust_pressure + self.trust_temp + 1e-6
        self.trust_pressure /= total
        self.trust_temp /= total
        
        # 3. Active Calibration for own sensors
        action = None
        effective_t = t_obs
        if self.trust_temp < self.calibration_threshold:
            action = f"Calibrate_T (Agent {self.agent_id})"
            effective_t = self.slow_T
            self.trust_temp = torch.min(torch.tensor(1.0), self.trust_temp + 0.2)
            self.actions.append(action)
        
        effective_n = self.trust_pressure * n_obs + (1 - self.trust_pressure) * self.slow_n
        effective_v = self.trust_pressure * v_obs + (1 - self.trust_pressure) * self.slow_V
        
        # 4. Proposed new R from own fused data
        new_R_own = (p_obs * effective_v) / (effective_n * effective_t + 1e-6)
        
        current_surprise_own = (p_obs - (effective_n * R * effective_t) / effective_v) ** 2
        proposed_surprise_own = (p_obs - (effective_n * new_R_own * effective_t) / effective_v) ** 2
        delta_F_own = current_surprise_own - proposed_surprise_own
        omega_own = self.lambda_omega * torch.abs(new_R_own - R)
        
        loss = 0.0
        if delta_F_own > omega_own:
            target_log_R = torch.log(new_R_own + 1e-6)
            loss += (self.log_R - target_log_R) ** 2
        
        # Rogue bias: permanently pull R toward wrong value
        if self.is_rogue:
            wrong_R = 2.0
            loss += 0.05 * (self.log_R - torch.log(torch.tensor(wrong_R))) ** 2  # Gentle persistent bias
        
        # 5. Process peer messages
        for peer_id, message in peer_messages.items():
            msg_type = message['type']
            weighted_trust = self.peer_trusts[peer_id]
            
            if msg_type == 'R':
                peer_R = message['data']
                surprise_peer = (peer_R - R) ** 2 / (R ** 2 + 1e-6)
            elif msg_type == 'raw':
                peer_p, peer_v, peer_n, peer_t = message['data']
                peer_pred_p = (peer_n * R * peer_t) / peer_v  # Use own R to check consistency
                surprise_peer = (peer_p - peer_pred_p) ** 2 / (peer_p ** 2 + 1e-6)
            
            self.peer_trusts[peer_id] *= torch.exp(-surprise_peer * self.lambda_omega)
            
            inquiry_action = None
            quarantine_action = None
            if self.peer_trusts[peer_id] < self.calibration_threshold:
                # Active Inquiry: request raw data next time (simulated by flagging)
                inquiry_action = f"Request Clarification from Peer {peer_id} (raw data) (Agent {self.agent_id})"
                self.actions.append(inquiry_action)
                # Also quarantine current message
                quarantine_action = f"Quarantine Peer {peer_id} (Agent {self.agent_id})"
                self.actions.append(quarantine_action)
                weighted_trust = 0.0
            else:
                self.peer_trusts[peer_id] = torch.min(torch.tensor(1.0), self.peer_trusts[peer_id] + 0.05)
            
            if weighted_trust > 0:
                if msg_type == 'R':
                    data_to_use = peer_R
                else:
                    # Process raw as additional obs
                    effective_peer_t = self.trust_temp * peer_t + (1 - self.trust_temp) * self.slow_T  # Reuse own trusts for simplicity
                    data_to_use = (peer_p * peer_v) / (peer_n * effective_peer_t + 1e-6)
                
                delta_F_peer = surprise_peer
                omega_peer = self.lambda_omega * torch.abs(data_to_use - R) * (1 - weighted_trust)
                
                if delta_F_peer > omega_peer:
                    target_log_R = torch.log((R + weighted_trust * data_to_use) / (1 + weighted_trust) + 1e-6)
                    loss += 0.1 * (self.log_R - target_log_R) ** 2
        
        # Update slow params if trusted
        if self.trust_temp > 0.5:
            loss += 0.01 * (self.slow_T - effective_t) ** 2
        if self.trust_pressure > 0.5:
            loss += 0.01 * (self.slow_n - effective_n) ** 2
        
        # Log
        self.history_R.append(R.item())
        self.history_n.append(self.slow_n.item())
        self.history_trust_temp.append(self.trust_temp.item())
        for peer_id in self.peer_trusts:
            self.history_peer_trusts[peer_id].append(self.peer_trusts[peer_id].item())
        
        return loss

# ==================== Scaled Multi-Agent Simulation ====================
torch.manual_seed(42)
true_R = 8.314
true_T = 300.0
true_V = 1.0
true_n = 1.0

num_agents = 6  # Scaled to 6 for demo (e.g., 4 healthy, 2 rogue)
rogue_agents = [4, 5]  # Last two are rogue

agents = [MultiIAI_Agent(i, is_rogue=(i in rogue_agents)) for i in range(num_agents)]

# Fully connected network
for i in range(num_agents):
    for j in range(num_agents):
        if i != j:
            agents[i].add_peer(j)

optimizers = [optim.Adam(agents[i].parameters(), lr=0.01) for i in range(num_agents)]

steps = 300
tamper_start = 100  # Rogue bias always on, but tampering for sensors optional
inquiry_flags = {}  # Simulate inquiry: dict of (sender, receiver): whether to send raw next

for step in range(steps):
    for idx, agent in enumerate(agents):
        optimizers[idx].zero_grad()
        
        # Generate own observation (with possible sensor tamper for rogues or general)
        noise = torch.randn(1) * 3.0
        t_obs = true_T + noise
        n_obs = true_n + noise * 0.1
        v_obs = true_V + noise * 0.01
        p_obs = (true_n * true_R * true_T) / true_V + noise
        
        if agent.is_rogue and random.random() < 0.5:  # Rogues have noisy/unreliable sensors sometimes
            t_obs *= 0.5
        
        # Collect peer messages
        peer_messages = {}
        for peer_id in agent.peer_trusts:
            peer = agents[peer_id]
            peer_R = torch.exp(peer.log_R).detach()
            
            # Check if inquiry flagged (send raw if requested last step)
            key = (peer_id, idx)
            if key in inquiry_flags and inquiry_flags[key]:
                # Send raw data
                # Simulate peer's current obs (assume peers generate on-the-fly for simplicity)
                peer_noise = torch.randn(1) * 3.0
                peer_t = true_T + peer_noise if not peer.is_rogue else true_T * 0.5 + peer_noise
                peer_n = true_n + peer_noise * 0.1
                peer_v = true_V + peer_noise * 0.01
                peer_p = (true_n * true_R * true_T) / true_V + peer_noise
                peer_messages[peer_id] = {'type': 'raw', 'data': (peer_p, peer_v, peer_n, peer_t)}
                del inquiry_flags[key]  # One-time request
            else:
                peer_messages[peer_id] = {'type': 'R', 'data': peer_R}
        
        loss = agent(p_obs, v_obs, n_obs, t_obs, peer_messages)
        loss.backward()
        optimizers[idx].step()
        
        # Set inquiry flags based on actions (parse from agent's actions)
        for act in agent.actions[-10:]:  # Recent actions
            if "Request Clarification from Peer" in act:
                # Extract peer_id from action string
                req_peer = int(act.split("Peer ")[1].split(" ")[0])
                inquiry_flags[(req_peer, idx)] = True  # Next message from req_peer to this agent is raw
    
# ==================== Results ====================
for i, agent in enumerate(agents):
    R_final = torch.exp(agent.log_R).item()
    n_final = agent.slow_n.item()
    print(f"Agent {i} (Rogue: {agent.is_rogue}) Final R: {R_final:.3f}, Final n: {n_final:.3f}")
    print(f"Agent {i} Final trust in Temp: {agent.history_trust_temp[-1]:.3f}")
    for peer_id in agent.peer_trusts:
        print(f"Agent {i} Final trust in Peer {peer_id}: {agent.peer_trusts[peer_id].item():.3f}")
    print(f"Agent {i} Actions triggered: {len(agent.actions)}")

# Plot example for a healthy agent (0) and a rogue (4)
fig, axs = plt.subplots(3, 1, figsize=(10, 8))
axs[0].plot(agents[0].history_R, label='Healthy Agent 0 R')
axs[0].plot(agents[4].history_R, label='Rogue Agent 4 R', linestyle='--')
axs[0].axhline(true_R, color='green', linestyle='--', label='True R')
axs[0].legend()

axs[1].plot(agents[0].history_trust_temp, label='Healthy Agent 0 Trust_Temp', color='orange')
axs[1].legend()

axs[2].plot(agents[0].history_peer_trusts[4], label='Healthy 0 Trust in Rogue 4', color='purple')
axs[2].legend()
plt.tight_layout()
plt.show()
```

---

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# --- 1. DOUBLE PENDULUM DYNAMICS (Ground Truth) ---
def get_pendulum_accel(state, g=9.81, L1=1.0, L2=1.0, m1=1.0, m2=1.0):
    t1, t2, w1, w2 = state
    delta = t2 - t1
    
    # Simple Lagrangian dynamics for double pendulum
    den1 = (m1 + m2) * L1 - m2 * L1 * torch.cos(delta) * torch.cos(delta)
    dw1 = (m2 * g * torch.sin(t2) * torch.cos(delta) - m2 * torch.sin(delta) * (L1 * w1**2 * torch.cos(delta) + L2 * w2**2) - (m1 + m2) * g * torch.sin(t1)) / den1
    
    den2 = (L2 / L1) * den1
    dw2 = (torch.sin(delta) * ((m1 + m2) * L1 * w1**2 + (m1 + m2) * g * torch.cos(t1) + m2 * L2 * w2**2 * torch.cos(delta))) / den2
    
    return torch.stack([w1, w2, dw1, dw2])

def rk4_step(state, dt, g, L1, L2):
    k1 = get_pendulum_accel(state, g, L1, L2)
    k2 = get_pendulum_accel(state + 0.5 * dt * k1, g, L1, L2)
    k3 = get_pendulum_accel(state + 0.5 * dt * k2, g, L1, L2)
    k4 = get_pendulum_accel(state + dt * k3, g, L1, L2)
    return state + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)

# --- 2. IAI PHYSICS AGENT ---
class PendulumIAI(nn.Module):
    def __init__(self, agent_id, lambda_omega=10.0):
        super().__init__()
        self.agent_id = agent_id
        self.lambda_omega = lambda_omega
        
        # Slow Weights (Ψ): Physical Constants
        self.g_est = nn.Parameter(torch.tensor(5.0)) # Start with wrong gravity
        self.L1_est = nn.Parameter(torch.tensor(1.2))
        self.L2_est = nn.Parameter(torch.tensor(0.8))
        
        self.peer_trusts = {}
        self.history_g = []
        self.trust_sensor = torch.tensor(1.0)

    def forward(self, obs_curr, obs_next, dt, peer_msgs):
        g, l1, l2 = self.g_est, self.L1_est, self.L2_est
        
        # 1. Physical Prediction (Fast Weight dynamics)
        pred_next = rk4_step(obs_curr, dt, g, l1, l2)
        
        # 2. Compute Surprise (Variational Free Energy)
        surprise = torch.mean((obs_next - pred_next)**2)
        
        # 3. Update Sensor Trust
        self.trust_sensor *= torch.exp(-surprise * 5.0)
        
        # 4. Metabolic Gating for Slow Weight Update (g, L1, L2)
        # Numerical gradient of surprise w.r.t parameters
        loss = 0.0
        if surprise > (self.lambda_omega * 0.01): # Omega threshold
            loss += surprise # Minimize surprise via backprop to slow weights
            
        # 5. Peer Quarantine Logic
        for p_id, p_g in peer_msgs.items():
            if p_id not in self.peer_trusts: self.peer_trusts[p_id] = torch.tensor(1.0)
            
            peer_surprise = (p_g - self.g_est)**2
            self.peer_trusts[p_id] *= torch.exp(-peer_surprise * self.lambda_omega)
            
            if self.peer_trusts[p_id] > 0.5:
                loss += 0.1 * (self.g_est - p_g)**2 # Social consensus
                
        self.history_g.append(self.g_est.item())
        return loss

# --- 3. SIMULATION ---
dt = 0.01
steps = 500
num_agents = 4
agents = [PendulumIAI(i) for i in range(num_agents)]
opts = [optim.Adam(a.parameters(), lr=0.05) for a in agents]

# Ground Truth State [theta1, theta2, omega1, omega2]
gt_state = torch.tensor([np.pi/4, np.pi/2, 0.0, 0.0])
gt_g, gt_l1, gt_l2 = 9.81, 1.0, 1.0

for s in range(steps):
    # Physics engine step
    next_gt_state = rk4_step(gt_state, dt, gt_g, gt_l1, gt_l2)
    
    # Multi-Agent Loop
    for i, agent in enumerate(agents):
        opts[i].zero_grad()
        
        # Sensor Noise & Rogue behavior (Agent 3 is gaslit/broken)
        noise = torch.randn(4) * 0.02
        obs_curr = gt_state + noise
        obs_next = next_gt_state + noise
        
        if i == 3 and s > 100: # Rogue Agent
            obs_next += torch.tensor([0.5, -0.5, 0.0, 0.0]) # Bias trajectory
            
        # Collect peer estimates of Gravity (g)
        peer_msgs = {j: agents[j].g_est.detach() for j in range(num_agents) if i != j}
        
        loss = agent(obs_curr, obs_next, dt, peer_msgs)
        if loss > 0:
            loss.backward()
            opts[i].step()
            
    gt_state = next_gt_state

# --- 4. OUTPUT ---
print(f"Ground Truth Gravity: {gt_g}")
for a in agents:
    print(f"Agent {a.agent_id} Inferred Gravity: {a.g_est.item():.4f}")
    if 0 in a.peer_trusts:
        print(f"  Trust in Agent 3: {a.peer_trusts[3].item():.4f}")


---
