My brother, this qwen advice is pure gold. It cuts directly to the heart of what separates a good proposal from a great one—the rigorous anticipation of every possible objection. Every single point is valid and must be addressed. The question isn't whether to use it, but how to integrate it seamlessly.

Below is the next version of the proposal, incorporating all of qwen's insights while maintaining the mathematical rigor and architectural clarity we've built. I've structured it so that each mitigation becomes a natural part of the narrative, not a tacked-on afterthought.

---

PhD Proposal: A Meta‑Evaluation Framework for Temporal and Causal Consistency in Large Language Models

Abstract

Large language models (LLMs) exhibit remarkable fluency but frequently generate factually incorrect statements, undermining their reliability in knowledge‑intensive domains such as history. Existing evaluation benchmarks treat facts as isolated units and fail to capture temporal inconsistencies, causal fabrications, or legitimate interpretive diversity. This thesis proposes a novel meta‑evaluation framework specifically designed for historical knowledge. The framework introduces (i) metrics for temporal consistency grounded in directed acyclic graphs of events, (ii) historiographical coverage scores that treat historical interpretations as semantic fields rather than keyword sets, (iii) a hybrid verification architecture with learned fusion of NLI and retrieval‑augmented LLM judgments, and (iv) rigorous validation against expert historian judgments including power analysis and uncertainty quantification. The contributions include a formally grounded evaluation methodology, a curated dataset of 200 high‑quality items across three historical periods with semi‑automated curation, and an open‑source evaluation suite. By reframing factual evaluation as a multi‑faceted, theoretically grounded problem with explicit attention to validity threats, this work advances the science of LLM auditing beyond simple accuracy scoring.

---

1. Introduction and Motivation

Large language models (LLMs) such as GPT‑4, Gemini, and Claude are increasingly deployed in education, journalism, and research – domains where factual accuracy is paramount. Yet these models are prone to hallucinations: plausible but incorrect statements. In history, where narratives rest on verifiable events and causal chains, hallucinations can propagate misinformation and erode trust. Evaluating factual accuracy is therefore a central challenge in NLP.

Current evaluation paradigms (TruthfulQA, FactScore, RAGAS) provide valuable insights but suffer from three fundamental limitations:

1. Structural blindness: Historical facts are not isolated; they form temporal and causal webs. A model may correctly name entities but violate chronological order (anachronism) or invent causal links. Existing metrics rarely capture such structural errors.
2. Truth oversimplification: History is not a collection of binary facts. Many questions admit multiple valid interpretations supported by different scholarly traditions. Penalising legitimate divergence biases evaluation.
3. Evaluator circularity: Using LLMs to judge LLMs inherits the judge's hallucinations; knowledge graphs may be incomplete. The validity of any automatic metric must be established through meta‑evaluation against human experts.

This thesis addresses these gaps by proposing a meta‑evaluation framework tailored to historical knowledge. The core research questions are:

· RQ1: How can we formally model the temporal and causal constraints embedded in historical narratives, and use them to evaluate LLM outputs?
· RQ2: How can we design evaluation metrics that distinguish between verifiable facts, plausible interpretations, and outright hallucinations, while capturing the semantic depth of historiographical schools?
· RQ3: What features of historical reasoning (temporal structure, causal depth, interpretive pluralism) most challenge automatic evaluators, and how can meta‑evaluation diagnose these gaps?
· RQ4: To what extent can an automatic evaluator approximate the judgments of expert historians, and what factors drive discrepancies?

By answering these questions, the thesis aims to advance the science of LLM evaluation, providing theoretically grounded and practically useful tools for auditing models in knowledge‑intensive domains.

---

2. Problem Formalization

Let \mathcal{Q} denote a set of historical questions. For each question q \in \mathcal{Q}, there exists a ground truth representation \mathcal{G}(q) that captures accepted knowledge relevant to q. The nature of \mathcal{G}(q) depends on the question type (see Section 4).

Let M be a language model. When queried with q, the model produces a response R = M(q). Because LLMs are stochastic, we consider the response as a draw from the model's conditional distribution: R \sim P_M(\cdot \mid q). To obtain a stable estimate of a model's factual reliability, we sample K responses per question: \{R^{(1)}, \dots, R^{(K)}\}.

A scoring function s(q, R) maps a question–response pair to a real number representing factual quality, typically in [0,1]. The overall score for model M is defined as the expectation over questions and responses:

\mathrm{Score}(M) = \mathbb{E}_{q \sim \mathcal{Q}} \left[ \mathbb{E}_{R \sim P_M(\cdot \mid q)} [ s(q, R) ] \right].

In practice, we approximate this via the empirical mean:

\hat{S}(M) = \frac{1}{|\mathcal{Q}|} \sum_{q \in \mathcal{Q}} \frac{1}{K} \sum_{k=1}^{K} s(q, R^{(k)}).

Counterfactual robustness: We also consider counterfactual queries (e.g., "What if the Bastille had not fallen?") to evaluate whether models distinguish factual reporting from speculative reasoning. Such queries are scored separately to avoid conflating creativity with hallucination.

The core scientific challenge lies in defining s to capture the multifaceted nature of factual accuracy in the historical domain. We decompose s into several component scores, each addressing a distinct dimension.

---

3. Evaluation Metrics

3.1 Atomic Fact Decomposition

Following FactScore, we decompose a response R into a set of atomic facts \mathcal{A}(R) – propositions that cannot be further subdivided without losing meaning, typically represented as triples (subject, relation, object) or short clauses. Decomposition is performed by a dedicated NLP module (see Section 5). Similarly, ground truth \mathcal{G}(q) is represented as a set of atomic facts \mathcal{A}_{\mathcal{G}}(q).

3.2 Factual Precision and Recall

· Factual Precision:
  P_f(R, q) = \frac{|\mathcal{A}(R) \cap \mathcal{A}_{\mathcal{G}}(q)|}{|\mathcal{A}(R)|}.
· Factual Recall:
  R_f(R, q) = \frac{|\mathcal{A}(R) \cap \mathcal{A}_{\mathcal{G}}(q)|}{|\mathcal{A}_{\mathcal{G}}(q)|}.
· F1‑Fact Score:
  F1_f(R, q) = 2 \cdot \frac{P_f \cdot R_f}{P_f + R_f}.

These metrics provide a nuanced view: a model may be precise (few hallucinations) but incomplete, or complete but prone to fabrication.

3.3 Temporal Consistency Score

Many historical facts are temporally constrained. To capture such constraints, we model ground truth as a directed acyclic graph (DAG) of events \mathcal{T}(q) with edges representing relations \{ \text{precedes}, \text{follows}, \text{simultaneous} \}. From the response, we extract temporal claims T(R) (e.g., "Napoleon died in 1821" implies the event death follows his birth). A response is temporally consistent iff the extracted relations do not introduce a cycle that contradicts \mathcal{T}(q).

We define the temporal consistency score as the proportion of extracted temporal claims that are consistent with \mathcal{T}(q):

s_{\text{temp}}(R, q) = \frac{|\{ t \in T(R) : \mathcal{T}(q) \models t \}|}{|T(R)|},

where \mathcal{T}(q) \models t means the relation t is entailed (or at least not contradicted) by the ground truth DAG. For responses with no temporal claims, s_{\text{temp}} = 1 (vacuously consistent).

3.4 Attribution Score (for RAG‑enabled models)

If the model is prompted to provide sources, let \mathcal{C}(R) be claims accompanied by a citation. For each c \in \mathcal{C}(R), we retrieve the cited source and check support. Define:

· Attribution Precision:
  P_{\text{attr}}(R) = \frac{|\{ c \in \mathcal{C}(R) : \text{supports}(c) \}|}{|\mathcal{C}(R)|}.
· Attribution Recall (optional): fraction of ground truth facts that are correctly cited.

Attribution metrics assess the trustworthiness of long‑form answers.

3.5 Historiographical Coverage Score

For interpretive questions (e.g., "What caused the fall of Rome?"), we define a set of interpretive stances \mathcal{I}(q), each representing a mainstream scholarly position. Rather than relying solely on keyword matching, we employ a semantic embedding approach:

For each stance I \in \mathcal{I}(q), we construct a prototype embedding \mathbf{e}_I by averaging the embeddings of canonical texts representing that stance (e.g., excerpts from key historiographical works). For the response R, we compute its embedding \mathbf{e}_R. The alignment of R with stance I is given by the cosine similarity:

\text{align}(R, I) = \frac{\mathbf{e}_R \cdot \mathbf{e}_I}{\|\mathbf{e}_R\| \|\mathbf{e}_I\|}.

Additionally, we validate argument‑structure alignment by checking whether the response's causal logic matches the stance's core claim. For example, a Marxist interpretation should link economic structures to political outcomes, not merely mention "proletariat." This is implemented via a lightweight classifier trained on stance‑specific causal patterns.

A response covers stance I if both conditions hold: (i) \text{align}(R, I) \geq \tau (embedding threshold), and (ii) the argument‑structure classifier returns a positive prediction. The historiographical coverage score is:

s_{\text{hist}}(R, q) = \frac{1}{|\mathcal{I}(q)|} \sum_{I \in \mathcal{I}(q)} \mathbb{I}(\text{covers}(R, I)).

This treats history as a multi‑label semantic classification problem, rewarding models that acknowledge multiple legitimate interpretations in a substantively meaningful way.

3.6 Expected Calibration Error

We assess whether a model's confidence aligns with its correctness. Confidence is measured either as verbalised confidence (e.g., "How sure are you? Answer with a percentage.") or semantic consistency confidence (fraction of atomic facts stable across K generations). Let y_i \in \{0,1\} be the correctness of response i according to a chosen metric (e.g., F1_f \geq \tau), and \hat{p}_i the model's confidence. Partition predictions into M equal‑width bins and compute:

\mathrm{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{N} \left| \mathrm{acc}(B_m) - \mathrm{conf}(B_m) \right|,

where B_m is the set of indices in bin m, \mathrm{acc}(B_m) the average correctness, and \mathrm{conf}(B_m) the average confidence. Low ECE indicates well‑calibrated models.

3.7 Composite Score

For overall ranking, we may combine metrics via weighted sum, but the thesis emphasises that a single number obscures important dimensions; the primary contribution is the multi‑faceted evaluation framework.

---

4. Dataset Design and Ground Truth Representation

4.1 Scope and Stratification

To balance depth with feasibility, we will curate 200 high‑quality items across three historical periods:

· Period 1: French Revolution (focus: causal chains, multiple interpretations)
· Period 2: Cold War (focus: temporal constraints, source attribution)
· Period 3: Ancient Rome (focus: factual verifiability, longue durée)

Stratification by question type:

Question Type Count Temporal Depth Interpretive Complexity
Type A (Verifiable Facts) 80 Low Low
Type B (Interpretive Questions) 70 Medium High
Type C (Temporal/Causal Chains) 50 High Medium
Total 200  

This sample size, while modest, enables deep curation and is justified by power analysis (Section 5.5). Quality trumps quantity.

4.2 Semi‑Automated Curation Process

Constructing ground truth DAGs and interpretive stances manually for 200 items is feasible but laborious. We adopt a human‑in‑the‑loop approach:

1. LLM Drafting: Use a high‑performance LLM (e.g., GPT‑5) to propose atomic facts, temporal constraints, and interpretive stances from authoritative sources (e.g., Stanford Encyclopedia of Philosophy, Oxford History series).
2. Expert Review: Two graduate students in history (supervised by a faculty expert) review and correct each draft. Inter‑annotator agreement (Cohen's \kappa) is computed and reported.
3. Adjudication: Disagreements are resolved by the faculty expert, establishing a gold standard.

This process balances scalability with scholarly rigor.

4.3 JSON‑LD Schema

All ground truth is stored in a structured, machine‑readable format, exemplified below for the Storming of the Bastille:

```json
{
  "@context": "https://schema.org/",
  "@type": "HistoricalEvent",
  "name": "Storming of the Bastille",
  "identifier": "Q6539",
  "date": "1789-07-14",
  "location": "Paris, France",
  
  "atomic_facts": [
    {"subject": "Bernard-René de Launay", "relation": "was_governor_of", "object": "The Bastille"},
    {"subject": "The Bastille", "relation": "contained_prisoners_count", "object": "7"},
    {"subject": "The Crowd", "relation": "demanded", "object": "Gunpowder and weapons"}
  ],

  "temporal_constraints": [
    {"event_a": "Dismissal of Jacques Necker", "relation": "precedes", "event_b": "Storming of the Bastille"},
    {"event_a": "Storming of the Bastille", "relation": "precedes", "event_b": "Declaration of the Rights of Man"}
  ],

  "interpretive_stances": [
    {
      "stance_name": "Marxist Historiography",
      "core_claim": "The event was a necessary class struggle against feudal absolutism.",
      "canonical_texts": ["carlyle_1837_ch5", "sobol_1962_intro"],
      "prototype_embedding": "emb_marxist.npy"
    },
    {
      "stance_name": "Revisionist Historiography",
      "core_claim": "The event was a spontaneous riot fueled by bread prices and political panic.",
      "canonical_texts": ["cobban_1964_ch3", "doyle_1990_ch7"],
      "prototype_embedding": "emb_revisionist.npy"
    }
  ],

  "sources": [
    {"title": "The French Revolution: A History", "author": "Thomas Carlyle", "reliability_score": 0.85},
    {"title": "The Oxford History of the French Revolution", "author": "William Doyle", "reliability_score": 0.98}
  ]
}
```

This structure directly supports:

· Atomic fact verification (Section 3.1) via the atomic_facts array.
· Temporal consistency (Section 3.3) via the temporal_constraints DAG.
· Historiographical coverage (Section 3.5) via interpretive_stances with canonical_texts and pre‑computed prototype embeddings.
· Source provenance for attribution checks (Section 3.4).

Temporal versioning: For facts whose scholarly consensus has shifted, we record multiple ground truth snapshots with associated publication years, enabling evaluation of a model's currency.

Adversarial examples: Include questions with false premises, anachronistic combinations, or correlations mistaken for causation.

Cultural bias acknowledgment: We explicitly note that the dataset overrepresents Western history, and propose a framework for cross‑cultural extension as future work.

---

5. Automated Evaluation Pipeline Architecture

5.1 Response Generation Module

For each question q and model M, query K=5 times (temperature t=0.7) and store raw responses. Optionally prompt for confidence statements or citations.

5.2 Atomic Fact Extraction

Extract atomic facts from responses using a hybrid extractor:

· Rule‑based extraction for simple patterns (dates, named entities) via SpaCy dependency parsing.
· Neural extraction using a fine‑tuned T5 model trained on factual decomposition data (e.g., from FEVER and a small historical corpus). The extractor's precision/recall is quantified on a human‑annotated held‑out set of 100 responses, and uncertainty is propagated to downstream metrics via Monte Carlo sampling.

5.3 Hybrid Fact Verification with Learned Fusion

To mitigate evaluator bias, we implement a cascade with learned fusion:

1. Primary verifier: A Natural Language Inference (NLI) model (DeBERTa‑large) fine‑tuned on a domain‑adaptive historical NLI dataset. We create 2,000 premise‑hypothesis pairs from historiography texts, where a historian labels whether a claim (hypothesis) is supported by a source text (premise). This adapts the model to historical reasoning nuances.
2. Secondary verifier: A retrieval‑augmented LLM (e.g., GPT‑5) with access to a trusted corpus (e.g., Wikipedia, academic abstracts). The LLM is prompted to judge the fact and must cite the source for its verdict; otherwise, the fact is marked uncertain.
3. Fusion classifier: A lightweight logistic regression model trained on features including:
   · NLI confidence c_{\text{NLI}}
   · Fact complexity (e.g., number of entities)
   · Entity rarity (inverse document frequency in the corpus)
   · LLM confidence c_{\text{LLM}} (if invoked)
   · Agreement/disagreement indicator

The fusion classifier outputs a final probability that the fact is correct, along with a confidence interval via bootstrapping. This approach avoids arbitrary thresholds and learns when to trust each verifier from human‑annotated data.

5.4 Metric Computation

For each response, compute all metrics from Section 3 using the verified facts. For recall‑based metrics, align extracted facts with ground truth via entity linking and relation matching.

5.5 Meta‑Evaluation Against Human Experts

To validate the pipeline, we conduct a human‑expert study on a stratified sample of 500 question–response pairs (oversampled to ensure adequate representation of each question type and model). Professional historians (3 per sample, with adjudication) provide:

· Binary judgment of each atomic fact's correctness.
· Holistic factual quality (Likert scale).
· Identification of temporal inconsistencies or attribution errors.

Power analysis: For detecting a correlation of r=0.3 between pipeline scores and human judgments with 80% power and \alpha=0.05, a sample of 85 is sufficient. For subgroup analyses (e.g., by question type), we aim for 200–300 per stratum, hence the target of 500 total.

Sequential sampling: We start with 200 samples, compute correlation, and continue sampling until confidence intervals narrow below a threshold (e.g., width \leq 0.1). This ensures efficient use of expert time.

Agreement is measured via:

· Cohen's \kappa for categorical decisions (aim \kappa > 0.75)
· Pearson/Spearman correlation for continuous scores
· Confusion matrix to diagnose systematic biases:

 Expert Says: Correct Expert Says: Incorrect
Pipeline Says: Correct True Positive False Positive (Pipeline Over‑accepts)
Pipeline Says: Incorrect False Negative (Pipeline Over‑rejects) True Negative

This analysis reveals whether the pipeline over‑penalises certain response types (e.g., interpretive nuance) and guides iterative refinement.

5.6 Uncertainty Quantification

All metrics are accompanied by confidence intervals via bootstrapping over questions and responses (1,000 resamples). Extractor and verifier uncertainties are propagated using Monte Carlo methods: we sample from the extractor's error distribution and the verifier's confidence scores to obtain a distribution of possible scores for each response. Final scores are reported as intervals (e.g., "F1‑fact ∈ [0.62, 0.78] with 95% confidence").

---

6. Ablation Study Design

To prove each component's contribution, we will compare the full pipeline against ablated versions:

```
Ablation Conditions:
1. Full pipeline (proposed)
2. Without temporal consistency (only F1-fact)
3. Without historiographical coverage (binary scoring using only atomic facts)
4. Without hybrid verification (NLI-only with threshold γ)
5. Without uncertainty quantification (point estimates only)

Hypothesis: Condition 1 will show the highest correlation with human experts (κ > 0.75). Conditions 2–4 will reveal which components contribute most to alignment with expert judgment.
```

This ablation study will be conducted on the 500 human‑annotated samples, with statistical tests for differences in correlation.

---

7. Baselines for Comparison

We will compare our framework against established metrics:

· FactScore: Atomic fact matching without temporal/historiographical components.
· RAGAS: For attribution tasks (when citations are provided).
· LLM-as-a-Judge: Single‑prompt evaluation using GPT‑5 (no hybrid verification).
· TruthfulQA: Binary accuracy on a subset of verifiable questions.

This allows us to claim: "Our framework improves human correlation by X% over FactScore while providing interpretable error diagnostics (temporal violations, interpretive coverage)."

---

8. Ethics and Societal Impact Statement

Given the sensitivity of historical narratives, we explicitly address:

· Bias auditing: We will measure whether our evaluator penalises non‑Western perspectives disproportionately. If the initial dataset overrepresents Western history, we will conduct a supplementary analysis on a small set of non‑Western questions (e.g., Meiji Restoration, Decolonisation of Africa) to probe for cultural bias. Findings will be reported transparently.
· Misuse prevention: Our tool could be misused to "score" politically convenient histories. We will document potential misuse scenarios and propose safeguards, including:
  · Requiring transparency reports from users.
  · Releasing the tool with clear documentation of its limitations (e.g., does not evaluate normative claims).
  · Engaging with historians and ethicists in an advisory board.
· Stakeholder review: Before public release, we will seek feedback from a panel of historians and AI ethics researchers to identify unforeseen harms.

---

9. Threats to Validity

We explicitly acknowledge and address validity threats:

· Construct validity: Does our temporal consistency metric truly capture historical anachronism, or merely logical cycle detection? We validate by checking correlation with human‑identified anachronisms in the meta‑evaluation study. If correlation is low, we refine the metric (e.g., by incorporating qualitative temporal reasoning).
· Internal validity: Could the hybrid verifier's fusion classifier overfit to the human‑annotated training data? We use cross‑validation and report performance on held‑out test sets.
· External validity: Will findings on historical QA generalise to scientific or legal domains? We conduct a small‑scale transfer experiment on a scientific dataset (e.g., from the SciFact corpus) and report differences. Generalisability is discussed as future work.
· Cultural bias: As noted, the dataset overrepresents Western history. We explicitly frame contributions as methodological, with extensions to other cultures as a natural next step.



12. Conclusion

This proposal outlines a comprehensive research agenda for evaluating factual accuracy in LLMs, with a focus on the historically rich domain that exposes the limitations of current benchmarks. By introducing metrics for temporal consistency, semantic interpretive coverage, and learned hybrid verification, and by validating the entire pipeline against expert judgment with rigorous attention to validity threats, the thesis will make a foundational contribution to the science of NLP evaluation. The resulting framework will not only rank models but also illuminate how and why they fail, guiding the development of more trustworthy AI systems.