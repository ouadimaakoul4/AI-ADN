Adaptive Capsule-Based Artificial Intelligence:

Brain-Inspired Plasticity, Self-Generation, and Emergent Distributed Cognition


Abstract

Current artificial intelligence systems predominantly rely on static architectures that lack the capacity for dynamic self-repair, functional redistribution, and emergent distributed intelligence. In contrast, the human brain exhibits remarkable plasticity—neurons reorganise, new connections form, and functions can be reassigned even after significant structural damage. This thesis proposes a novel AI architecture inspired by neural plasticity and neurogenesis, where functional units called Capsules output activity vectors encoding both the presence and properties of features. These capsules are interconnected by adaptive links called Highways whose weights evolve according to biologically inspired plasticity rules (Hebbian, covariance, BCM, STDP) under the control of a central Nucleus (digital DNA) that monitors network activity and generates new capsules when underutilisation or functional gaps are detected. The system is extended to multiple nuclei, formalised as a Hamiltonian system with proven energy bounds, enabling the study of emergent distributed cognition quantified by causal emergence metrics. The architecture is implemented in Rust for performance and memory safety, with SQLite for state persistence and event-sourced logging. Through systematic experiments on continual learning benchmarks (CORe50, Split-CIFAR-100) and real-world inspired simulations (drone networks with Poisson failures), the project aims to demonstrate autonomous adaptation, functional recovery, and emergent behaviours. Expected contributions include a formal framework for self-generating neural architectures, a Hamiltonian stability theorem for multi-nuclei systems, a causal emergence quantification of distributed intelligence, and an open-source implementation serving as a testbed for adaptive AI research.

---

1. Introduction and Motivation

1.1 The Plasticity of the Human Brain

The human brain is a marvel of adaptive engineering. Cases of hemispherectomy—where an entire cerebral hemisphere is removed to treat severe epilepsy—show that individuals can retain near-normal cognitive function, with the remaining hemisphere reorganising to take over lost functions [1]. This phenomenon, known as plasticity, operates at multiple scales: synaptic connections strengthen or weaken (Hebbian plasticity), new synapses form (synaptogenesis), and entire cortical maps can reassign functions (map plasticity). Additionally, adult neurogenesis—the birth of new neurons—occurs in regions like the hippocampus, contributing to learning and memory [2].

1.2 Limitations of Current AI Systems

Contemporary AI models, including deep neural networks, are typically trained on fixed architectures. Once training is complete, the network structure remains static. This rigidity poses several problems:

· Lack of resilience: Damage to a node or connection often catastrophically degrades performance.
· No online adaptation: They cannot reorganise in response to new tasks or environmental changes without retraining.
· No structural growth: They cannot generate new computational units to handle increased complexity or novel input modalities.
· Limited interpretability: The distributed representations are difficult to analyse in terms of functional specialisation.

While some research explores evolving neural networks (e.g., NEAT [3]) and modular architectures (e.g., mixture of experts [4]), these approaches typically operate offline or lack the online, lifelong structural plasticity observed in biology.

1.3 A Brain-Inspired Approach: Capsules, Highways, and Nuclei

This thesis proposes an architecture that explicitly models three key biological concepts:

· Capsules as functional processing units (analogous to neurons or cortical columns) that output activity vectors encoding both the presence and properties of features. This vector representation, inspired by capsule networks [13], enables rich part-whole relationships and dynamic routing by agreement.
· Highways as adaptive connections (analogous to synapses) whose weights change according to plasticity rules. Highways transmit vectors and are modulated by coupling coefficients that evolve through iterative routing.
· Nuclei as digital DNA (analogous to genetic regulatory networks) that monitor network activity and generate new capsules or highways when underutilisation or damage is detected. The Nucleus uses an entropy-based trigger to distinguish uninformative noise from low activity.

The architecture is designed to exhibit online structural plasticity: the network can grow, prune, and reorganise its topology in response to experience and perturbation.

1.4 Why Rust and SQLite?

· Rust offers memory safety without garbage collection, making it ideal for simulating large, dynamic networks with real-time performance requirements. Its strong type system and concurrency primitives will facilitate parallel simulation of multiple nuclei via message passing or atomic reference counting. The nalgebra crate provides efficient linear algebra for capsule vector operations, and petgraph enables robust graph management.
· SQLite provides lightweight, file-based persistence. To avoid performance bottlenecks, an event-sourced logging approach is adopted: only significant events (capsule creation, pruning, weight shifts >10%) and periodic checkpoints are recorded, rather than logging every activation.

1.5 Research Vision

This research aims to create a foundational platform for studying artificial distributed intelligence—a system where cognitive functions emerge from the interactions of many simple, self-organising units. By grounding the architecture in rigorous mathematics (Hamiltonian dynamics, causal emergence), the project will provide both theoretical insights and practical tools for building resilient, adaptive AI systems. Potential applications include resilient robotics, adaptive IoT networks, and new paradigms for artificial general intelligence (AGI).

---

2. Literature Review

2.1 Neural Plasticity Models

· Hebbian learning [5]: "Neurons that fire together, wire together." Mathematically, $\Delta w_{ij} = \eta \nu_i \nu_j$. Extensions include the covariance rule [6] and BCM theory [7], which incorporate sliding thresholds to prevent unbounded growth.
· Spike-Timing Dependent Plasticity (STDP) [8]: Weights are modified based on the precise timing of pre- and post-synaptic spikes, with potentiation for pre-before-post and depression for post-before-pre.
· Homeostatic plasticity [9]: Mechanisms that keep total synaptic input to a neuron within a stable range, preventing runaway excitation or silencing.
· Structural plasticity: Models where synapses and neurons can be created or removed [10]. The SynaptoGen model [11] provides a probabilistic framework linking gene expression to synaptic connectivity.

2.2 Evolving and Generative Neural Networks

· NEAT (NeuroEvolution of Augmenting Topologies) [3]: Evolves network topology and weights using genetic algorithms, starting from minimal structures and complexifying over generations.
· HyperNEAT [12]: Uses compositional pattern-producing networks to generate large-scale neural structures with regular patterns.
· Capsule Networks [13]: Introduced by Hinton et al., capsules output vectors representing entity properties, with dynamic routing by agreement. This provides a natural representation for part-whole hierarchies and has inspired the vector-based capsules in this work.

2.3 Modular and Multi-Agent Systems

· Mixture of Experts [4]: Gating networks route inputs to specialised sub-networks, allowing conditional computation.
· Multi-agent reinforcement learning [14]: Agents learn to coordinate in shared environments, sometimes developing emergent communication protocols [15].
· Hamiltonian Neural Networks [16]: Incorporate physical principles to learn conservative dynamics, providing inspiration for the energy-based formulation of multi-nuclei systems.

2.4 Resilience and Robustness in Networks

· Graph-theoretic metrics: Redundancy, modularity, centrality [17] quantify structural robustness.
· Percolation theory: Resilience to random vs. targeted attacks [18].
· Functional resilience: Measuring system performance under perturbation [19].

2.5 Causal Emergence and Distributed Intelligence

· Causal emergence framework [20, 21]: Formalises when macro-scale variables have causal power not present at the micro-scale. This provides a quantitative measure for emergence in multi-nuclei systems via the $\Delta C$ metric.
· Integrated Information Theory [22]: Offers a related approach to quantifying consciousness, but causal emergence is more directly applicable to distributed AI systems.

2.6 Gaps Addressed by This Research

While the above fields provide building blocks, no existing work combines:

· Online structural plasticity with vector-based capsule routing and dynamic agreement.
· Hamiltonian formalisation of multi-nuclei systems with proven energy bounds.
· Causal emergence quantification of distributed intelligence.
· Persistent, event-sourced logging for reproducible experimentation.

This thesis aims to fill these gaps by proposing a unified architecture and implementing it in a high-performance language.

---

3. Research Questions

The project is guided by the following research questions:

1. RQ1 (Detection and Generation): How can an AI system, using only local activity metrics and a global Nucleus, detect underutilised functional niches and generate new capsules with appropriate connectivity to improve task performance? Specifically, can entropy-based measures (e.g., high variance with low task correlation) serve as a more robust trigger than simple activity thresholds?
2. RQ2 (Plasticity Rules and Dynamic Routing): How do different mathematical formulations of plasticity (Hebbian, covariance, BCM, STDP) interact with dynamic routing by agreement to affect the network's ability to redistribute functions after damage and learn new associations? A sub-question addresses homeostatic plasticity: how can the Nucleus implement synaptic scaling to maintain stable total input energy per capsule, and what is its impact on long-term resilience?
3. RQ3 (Multi-Nuclei Emergence and Hamiltonian Stability): Can multiple nuclei, each governing a subset of capsules, coordinate via inter-nuclei highways to exhibit emergent problem-solving behaviours not present in a single-nucleus system? How can the system be formalised as a Hamiltonian dynamical system, and under what conditions can we prove that homeostatic mechanisms keep the total energy bounded?
4. RQ4 (Resilience and Causal Emergence Quantification): What quantitative metrics best capture the adaptive capacity of a self-generating capsule network? In addition to resilience index and adaptation latency, how can we define a resource efficiency score that balances performance gain against computational cost of structural changes? Furthermore, can the causal emergence $\Delta C$ metric quantify the degree of distributed intelligence in multi-nuclei systems?

---

4. Objectives

To answer these questions, the project will:

1. Define the formal mathematical framework for vector-based capsules, dynamic routing, plasticity rules, genetic synaptogenesis, and entropy-based generation triggers.
2. Implement a Rust prototype with capsules as vector processors using nalgebra, graph management with petgraph, and dynamic routing by agreement.
3. Integrate SQLite persistence with event-sourced logging to enable experiment repeatability and longitudinal analysis.
4. Develop algorithms for automatic capsule generation based on activity monitoring, entropy measures, and genetic rules (SynaptoGen).
5. Extend the system to multiple nuclei, formalising them as a Hamiltonian system and proving an energy bound theorem.
6. Implement advanced plasticity rules including homeostatic scaling and compare their effects.
7. Design and run systematic experiments using continual learning benchmarks (CORe50, Split-CIFAR-100 via the avalanche library) and real-world inspired simulations (drone networks with Poisson failures).
8. Quantify emergence using the causal emergence $\Delta C$ metric.
9. Optimise the implementation for performance and scalability, targeting <1s per 1000-step simulation.
10. Release the code as open-source (crates.io) to foster further research.

---

5. Methodology

The research will proceed in seven phases, each building on the previous. For each phase, we describe the conceptual goal, mathematical formalisation, architectural design, implementation approach, and evaluation criteria.

Phase 1 — Conceptual Foundations & Architecture with Vector Capsules

Goal: Define the core components as vector-processing units with dynamic routing, and create an initial Rust prototype using modern libraries.

5.1.1 Mathematical Formalisation

Capsules: A capsule $c$ is defined by:

· An input vector $\mathbf{x} \in \mathbb{R}^n$
· A weight matrix $\mathbf{W}_c \in \mathbb{R}^{d \times n}$ (transforms input to capsule's own pose space)
· An activation function $f: \mathbb{R}^d \to \mathbb{R}^d$ (typically the squashing function: $\mathbf{v} = \frac{\|\mathbf{s}\|^2}{1+\|\mathbf{s}\|^2} \frac{\mathbf{s}}{\|\mathbf{s}\|}$)
· An output vector $\mathbf{u}_c = f(\mathbf{W}_c \mathbf{x})$

For a capsule $j$ receiving predictions from multiple lower-level capsules $i$, the prediction vector from $i$ to $j$ is:
\hat{\mathbf{u}}_{j|i} = \mathbf{W}_{ij} \mathbf{u}_i


where $\mathbf{W}_{ij} \in \mathbb{R}^{d_j \times d_i}$ is a learned transformation matrix.

Dynamic Routing: The coupling coefficient $c_{ij}$ between capsule $i$ and $j$ is updated iteratively:

1. Initialize log priors $b_{ij} = 0$.
2. For each routing iteration:
   c_{ij} = \frac{\exp(b_{ij})}{\sum_k \exp(b_{ik})}
   
   \mathbf{s}_j = \sum_i c_{ij} \hat{\mathbf{u}}_{j|i}
   
   \mathbf{v}_j = f(\mathbf{s}_j)
   
   b_{ij} \leftarrow b_{ij} + \mathbf{v}_j \cdot \hat{\mathbf{u}}_{j|i}
3. After $T$ iterations, $\mathbf{v}_j$ is the final output of capsule $j$.

Highways: A highway $h_{ij}$ connects capsule $i$ to capsule $j$ and stores the transformation matrix $\mathbf{W}_{ij}$ (or a scalar weight if capsules share the same dimensionality). Plasticity rules (Phase 5) will modify these matrices over time.

Nucleus (Digital DNA): As before, but now the Nucleus also monitors the agreement between capsules during routing to detect underperforming capsules.

5.1.2 Architectural Design and Implementation

The Rust implementation will use:

· nalgebra for matrix/vector operations.
· petgraph to represent the capsule graph and perform topological sorts for forward passes (ensuring DAG processing order).
· A Nucleus struct containing a DiGraph from petgraph with node weights being Capsule and edge weights being Highway.

```rust
use nalgebra as na;
use petgraph::graph::{DiGraph, NodeIndex};
use std::collections::HashMap;

type CapsuleId = NodeIndex;

struct Capsule {
    id: CapsuleId,
    weights: na::DMatrix<f64>,  // transformation from input to capsule space
    activation: fn(na::DVector<f64>) -> na::DVector<f64>,
    output: na::DVector<f64>,
    activity: f64,  // norm of output
}

struct Highway {
    from: CapsuleId,
    to: CapsuleId,
    transform: na::DMatrix<f64>,  // transformation matrix W_ij
}

struct Nucleus {
    graph: DiGraph<Capsule, Highway>,
    next_id: usize,
    activity_history: HashMap<CapsuleId, VecDeque<f64>>,
    // ... other fields
}
```

The forward pass will:

1. Topologically sort the graph.
2. For each capsule in order, compute its output using inputs from incoming highways (sum of predictions).
3. After all capsules processed, perform dynamic routing iterations (repeating steps 2-3 for $T$ iterations).

5.1.3 Evaluation for Phase 1

· Verify that capsules correctly compute vector outputs and dynamic routing converges.
· Test on a simple part-whole task (e.g., MNIST digit recognition) to ensure representational power.
· Benchmark simulation speed: target <1ms per capsule per step.

Phase 2 — SQLite Integration with Event-Sourced Logging

Goal: Enable saving and loading the network state with efficient logging.

5.2.1 Mathematical Formalisation

No new mathematics; focus on data persistence.

5.2.2 Implementation

As in Version 3, but now logging only major events:

· Capsule creation/deletion
· Highway creation/deletion
· Weight matrix changes exceeding a threshold (e.g., Frobenius norm change >10%)
· Periodic snapshots (every 1000 steps)

The rusqlite crate will be used with bincode for serialising matrices.

5.2.3 Evaluation

· Verify save/load integrity.
· Measure database size after long simulations; ensure it remains manageable.

Phase 3 — Automatic Capsule Generation with Genetic Control and Entropy-Based Triggers

Goal: Implement the Nucleus's ability to detect underused or missing areas and generate new capsules using a genetic model, with refined detection criteria.

5.3.1 Mathematical Formalisation

As in Version 3, but now the entropy-based trigger considers the capsule's output vector variance and correlation with task target (if supervised). We also incorporate the SynaptoGen model for probabilistic connectivity.

5.3.2 Implementation

Extend Nucleus with gene matrix and interaction matrix. When generating a new capsule:

· Inherit and mutate parent's gene expression.
· Use interaction matrix to compute connection probabilities to existing capsules.
· Create highways probabilistically with initial random transformation matrices.

5.3.3 Evaluation

· Simulate a task requiring two distinct pathways; measure time until missing pathway is generated.
· Compare entropy trigger vs. simple activity threshold.

Phase 4 — Multiple Nuclei as a Hamiltonian System

Goal: Scale to multiple nuclei, formalise as a Hamiltonian system, and prove energy bounds.

4.1 Mathematical Formalisation

Let the global state of the system be described by a set of generalized coordinates $\mathbf{q}_i$ for each capsule $i$ (e.g., its output vector $\mathbf{u}_i$) and conjugate momenta $\mathbf{p}_i$ (which could represent internal states or learning dynamics). Define the Hamiltonian:
H(\{\mathbf{q}_i, \mathbf{p}_i\}) = \sum_i \left( \frac{1}{2} \|\mathbf{p}_i\|^2 + V_i(\mathbf{q}_i) \right) + \sum_{(i,j) \in E} \Phi_{ij}(\mathbf{q}_i, \mathbf{q}_j)


where $V_i$ is the internal potential of capsule $i$ (e.g., from its activation function) and $\Phi_{ij}$ is the interaction potential via the highway (e.g., agreement energy).

The dynamics follow Hamilton's equations:
\dot{\mathbf{q}}_i = \frac{\partial H}{\partial \mathbf{p}_i}, \quad \dot{\mathbf{p}}_i = -\frac{\partial H}{\partial \mathbf{q}_i}

Homeostatic plasticity can be modelled as a damping term that keeps $\|\mathbf{p}_i\|$ bounded.

Theorem (Energy Boundedness): Under homeostatic scaling that caps the total incoming weight to each capsule and with Lipschitz continuous potentials, the total energy $H$ remains bounded for all time. Proof sketch: show that $\dot{H} \leq 0$ due to homeostatic dissipation, or that energy growth is compensated by pruning.

4.2 Implementation

Use tokio or rayon for parallel execution of nuclei. Inter-nuclei communication via message passing (channels) or shared-memory with Arc<RwLock>. Global highways store transformation matrices and are updated with a slower learning rate.

4.3 Evaluation

· Verify energy remains bounded in simulations.
· Measure mutual information $I(N_1; N_2)$ between nucleus activities as a coordination metric.

Phase 5 — Advanced Plasticity Rules Including Homeostatic Plasticity

Goal: Implement and compare multiple plasticity rules, including homeostatic scaling.

5.1 Mathematical Formalisation

We extend the plasticity rules from Version 3 to matrix-valued weights. For Hebbian learning on a transformation matrix $\mathbf{W}_{ij}$, a simple rule is:
\Delta \mathbf{W}_{ij} = \eta \, \mathbf{u}_j \mathbf{u}_i^\top - \gamma \mathbf{W}_{ij}


where $\mathbf{u}_i$ is the output of the pre-synaptic capsule and $\mathbf{u}_j$ the post-synaptic output (after routing). For STDP, we use trace-based approximations.

Homeostatic scaling adjusts all incoming weights to a capsule to keep the total input norm within a target range:
\mathbf{W}_{ij} \leftarrow \alpha \mathbf{W}_{ij} \quad \text{for all } i, \text{ with } \alpha = \frac{T}{\sum_i \|\mathbf{W}_{ij} \mathbf{u}_i\|}

5.2 Implementation

Plasticity rules are implemented as traits, applied after each forward pass (or after routing iterations). Homeostatic scaling is applied globally after all updates.

5.3 Evaluation

· Compare rules on associative memory tasks and recovery from damage.
· Measure the effect of homeostatic scaling on long-term stability.

Phase 6 — Systematic Evaluation with Benchmarks and Real-World Simulations

Goal: Quantify resilience, adaptability, and emergence using standard benchmarks and realistic simulations.

6.1 Experimental Scenarios

1. Continual Learning Benchmarks: Use the avalanche library to test on CORe50 and Split-CIFAR-100. After training on a sequence of tasks, remove 50% of capsules and measure the Resilience Index (area under performance curve post-damage). Compare against NEAT and static capsule network baselines.
2. Drone Network Simulation: Simulate a fleet of delivery drones using the desim discrete-event simulator. Each drone is controlled by a nucleus. Inject Poisson-distributed communication failures. Measure task completion rate and mutual information between nuclei as a coordination metric.
3. Targeted Attacks: Remove capsules with highest centrality; compare to random deletion.
4. Resource Efficiency: Track the number of structural changes and compute resource efficiency score.

6.2 Metrics

· Resilience Index: $R = \int_{t_0}^{t_1} P(t) dt$
· Adaptation Latency: Time to return to 90% baseline.
· Functional Redundancy: Number of disjoint pathways (by ablation).
· Graph Metrics: $L$, $C$, $Q$.
· Resource Efficiency: $\frac{\Delta \text{Performance}}{\text{Number of structural changes}}$.
· Causal Emergence: $\Delta C$ computed via the Causal Emergence Framework [20].

6.3 Implementation

A test harness in Rust will run multiple trials, log metrics, and output CSV for analysis in Python. Statistical tests (ANOVA, t-tests) will compare conditions.

Phase 7 — Advanced Perspectives and Causal Emergence

Goal: Explore unsupervised learning inside capsules, real-time interfaces, and apply the causal emergence framework to quantify distributed intelligence.

7.1 Causal Emergence Quantification

Apply the Causal Emergence Framework (CFF) [20] to multi-nuclei systems:

· Define micro-scale: individual capsule states.
· Define macro-scale: nucleus-level variables (e.g., average activity, centroid of capsule outputs).
· Compute the causal contribution $\Delta C$ of the macro-scale. A positive $\Delta C$ indicates that the macro-scale has causal power not reducible to the micro-scale, i.e., genuine emergence.

7.2 Internal Learning

Experiment with capsules containing small neural networks (e.g., MLPs) that learn via local gradients (using candle or tch-rs in Rust). This would make each capsule a mini-agent.

7.3 Real-Time Visualization

Develop a simple visualisation using egui or a web-based frontend to display the graph and activity levels in real time.

---

6. Expected Contributions

1. Formal Framework: A mathematical characterisation of capsule-based structural plasticity integrating vector representations, dynamic routing, Hebbian learning, genetic synaptogenesis, entropy-based generation, and Hamiltonian multi-nuclei dynamics.
2. Hamiltonian Stability Theorem: Proof that under homeostatic scaling, the total energy of a multi-nuclei system remains bounded, ensuring long-term stability.
3. Causal Emergence Quantification: Application of the Causal Emergence Framework to measure distributed intelligence in multi-nuclei systems, providing a quantitative metric for emergence.
4. Open-Source Implementation: A modular, performant Rust library with SQLite persistence and event-sourced logging, enabling reproducible research.
5. Empirical Insights: Quantitative results on how different plasticity rules, generation strategies, and network topologies affect resilience, adaptability, and emergence.
6. Resilience Metrics: A suite of metrics including resilience index, adaptation latency, functional redundancy, and resource efficiency for evaluating online adaptive systems.

---

7. Comparison with Existing Approaches

Feature Standard DNN Evolving Networks (NEAT) Capsule Networks Proposed Adaptive Capsules
Growth Mechanism Static Genetic (Generational) Static Activity-Triggered (Online)
Unit Logic Scalar (Neuron) Scalar (Neuron) Vector (Capsule) Vector (Capsule) with dynamic routing
Persistence Weights File Genome File Weights File Relational DB + Event Log
Plasticity Backpropagation Evolutionary Backpropagation Hebbian + Homeostatic + STDP variants
Resilience Metrics Limited Generational Limited Online metrics + Causal emergence
Multi-Scale Formalisation None None None Hamiltonian + Causal emergence

---

8. Conclusion

This thesis proposes a novel, brain-inspired architecture for adaptive AI that combines the representational power of capsule networks with the structural plasticity of biological neural systems. 

By grounding the design in rigorous mathematics—vector capsules, dynamic routing, Hamiltonian dynamics, and causal emergence—and implementing it in Rust with SQLite persistence, the project will produce both theoretical insights and practical tools for studying distributed, resilient intelligence. 

The incremental roadmap ensures steady progress and early validation, while the advanced phases explore emergent phenomena and causal quantification. This work has the potential to influence future AI systems that must operate reliably in dynamic, unpredictable environments.


Appendices: Mathematical Foundations and Proofs

Appendix A: Capsule Vector Mathematics and Dynamic Routing

A.1 Vector Representation of Capsules

Let a capsule $c$ be defined by a tuple $(\mathbf{W}_c, f, \mathbf{u}_c)$ where:

· $\mathbf{W}_c \in \mathbb{R}^{d \times n}$ is a weight matrix transforming input to capsule's pose space
· $f: \mathbb{R}^d \to \mathbb{R}^d$ is the squashing activation function
· $\mathbf{u}_c \in \mathbb{R}^d$ is the output vector

For an input vector $\mathbf{x} \in \mathbb{R}^n$, the capsule computes:
\mathbf{s}_c = \mathbf{W}_c \mathbf{x}


\mathbf{u}_c = f(\mathbf{s}_c) = \frac{\|\mathbf{s}_c\|^2}{1 + \|\mathbf{s}_c\|^2} \frac{\mathbf{s}_c}{\|\mathbf{s}_c\|}

Property A.1 (Length Invariance): The squashing function preserves the direction of $\mathbf{s}_c$ while normalizing its magnitude to the interval $(0,1)$. Specifically, $\|\mathbf{u}_c\| \in (0,1)$ and $\frac{\mathbf{u}_c}{\|\mathbf{u}_c\|} = \frac{\mathbf{s}_c}{\|\mathbf{s}_c\|}$.

A.2 Prediction Vectors and Transformation Matrices

For a connection from capsule $i$ (lower level) to capsule $j$ (higher level), the prediction vector is:
\hat{\mathbf{u}}_{j|i} = \mathbf{W}_{ij} \mathbf{u}_i


where $\mathbf{W}_{ij} \in \mathbb{R}^{d_j \times d_i}$ is a learned transformation matrix encoding spatial and part-whole relationships.

A.3 Dynamic Routing by Agreement

The routing mechanism iteratively refines coupling coefficients. Let $b_{ij}$ be log prior probabilities that capsule $i$ should be coupled to capsule $j$.

Algorithm A.1 (Dynamic Routing):
For $r = 1$ to $T$ iterations:

1. Softmax: $c_{ij} = \frac{\exp(b_{ij})}{\sum_k \exp(b_{ik})}$
2. Weighted Sum: $\mathbf{s}_j = \sum_i c_{ij} \hat{\mathbf{u}}_{j|i}$
3. Squash: $\mathbf{v}_j = f(\mathbf{s}_j)$
4. Agreement Update: $b_{ij} \leftarrow b_{ij} + \mathbf{v}_j \cdot \hat{\mathbf{u}}_{j|i}$

Theorem A.1 (Convergence of Dynamic Routing) : The dynamic routing algorithm minimizes a concave objective function and converges to a fixed point. The objective function can be expressed as:
\mathcal{L} = \sum_j \|\mathbf{v}_j\|^2 - \frac{1}{2}\sum_{i,j} c_{ij}^2\|\hat{\mathbf{u}}_{j|i}\|^2


subject to $\sum_j c_{ij} = 1$ for all $i$.

Proof Sketch: The algorithm performs coordinate ascent on this concave function under linear constraints. Each iteration increases the agreement between $\mathbf{v}_j$ and $\hat{\mathbf{u}}_{j|i}$, leading to monotonic improvement until convergence. [Full proof follows optimization theory for concave functions with linear constraints.]

---

Appendix B: Entropy-Adjusted Dynamic Routing

B.1 Information-Theoretic Motivation

Following recent advances , we incorporate entropy regularization to improve routing under uncertainty.

Definition B.1 (Routing Entropy): For capsule $i$, the entropy of its routing distribution is:
H_i = -\sum_j c_{ij} \log c_{ij}

Definition B.2 (Entropy-Adjusted Objective):
\mathcal{L}_{EADR} = \mathcal{L} - \lambda \sum_i H_i


where $\lambda > 0$ is a regularization parameter.

B.2 Modified Update Rule

The entropy-regularized routing update becomes:
b_{ij} \leftarrow b_{ij} + \mathbf{v}_j \cdot \hat{\mathbf{u}}_{j|i} + \lambda(1 + \log c_{ij})

Proposition B.1 (Convergence with Entropy Regularization): The entropy-adjusted algorithm converges to a unique fixed point for sufficiently small $\lambda$.

Proof: The entropy term adds strong concavity to the objective, ensuring a unique global maximum.

---

Appendix C: Genetic Synaptogenesis Model (SynaptoGen)

C.1 The Connectome Model Foundation

Following , we formalize the relationship between gene expression and synaptic connectivity.

Let:

· $N$ = number of neurons (capsules)
· $G$ = number of genes involved in synaptogenesis
· $\mathbf{X} \in \mathbb{R}^{N \times G}$ = gene expression matrix (each row is a neuron's expression profile)
· $\mathbf{O} \in \mathbb{R}^{G \times G}$ = genetic rule matrix (interaction probabilities between proteins)

The Connectome Model (CM) approximates the adjacency matrix $\mathbf{B} \in \mathbb{Z}^{N \times N}$ as:
\mathbf{B} \approx \mathbf{X} \mathbf{O} \mathbf{X}^T

C.2 Probabilistic Interpretation for Synaptic Multiplicity

Definition C.1: For a pair of neurons with expression vectors $\mathbf{x}, \mathbf{y} \in \mathbb{R}^G$, let $\mathcal{B}^{ij}$ be a binomial random variable representing the contribution of gene pair $(i,j)$ to synapse formation:
\mathcal{B}^{ij} \sim \text{Binomial}(n_{ij}, p_{ij})


where $n_{ij} = x_i y_j$ is the number of independent attempts, and $p_{ij} = O_{ij}$ is the success probability.

Proposition C.1 (Expected Synapse Count) : 
\mathbb{E}[\mathcal{B}] = \mathbf{x}^T \mathbf{O} \mathbf{y}

Proof: From probability theory, $\mathbb{E}[\mathcal{B}^{ij}] = n_{ij} p_{ij}$. By linearity of expectation:
\mathbb{E}[\mathcal{B}] = \sum_{i,j} \mathbb{E}[\mathcal{B}^{ij}] = \sum_{i,j} n_{ij} p_{ij}


But $\mathbf{x}^T \mathbf{O} \mathbf{y} = \sum_i x_i \sum_j O_{ij} y_j = \sum_{i,j} x_i y_j O_{ij} = \sum_{i,j} n_{ij} p_{ij}$. ∎

C.3 Extension to Synaptic Conductance

For modeling synaptic strength, we consider neurotransmitter-receptor interactions.

Let:

· $L$ = number of neurotransmitter types
· $M$ = number of receptor types
· $\mathbf{q} \in [0,1]^L$ = probability distribution of neurotransmitter release from pre-synaptic neuron
· $\mathbf{r} \in [0,1]^M$ = probability distribution of receptor expression on post-synaptic neuron
· $\mathbf{A} \in \mathbb{R}^{L \times M}$ = conductance matrix for neurotransmitter-receptor pairs

Proposition C.2 (Expected Synaptic Conductance): The expected conductance between neurons $u$ and $v$ is:
\mathbb{E}[G_{uv}] = \mathbf{q}_u^T \mathbf{A} \mathbf{r}_v

Proof: Similar to Proposition C.1, treating each neurotransmitter-receptor pair as independent contributions.

C.4 Gradient-Based Optimization

The differentiability of $\mathbb{E}[\mathcal{B}] = \mathbf{x}^T \mathbf{O} \mathbf{y}$ enables gradient-based learning:

\frac{\partial \mathbb{E}[\mathcal{B}]}{\partial \mathbf{x}} = \mathbf{O} \mathbf{y}


\frac{\partial \mathbb{E}[\mathcal{B}]}{\partial \mathbf{y}} = \mathbf{O}^T \mathbf{x}


\frac{\partial \mathbb{E}[\mathcal{B}]}{\partial \mathbf{O}} = \mathbf{x} \mathbf{y}^T

---

Appendix D: Plasticity Rules

D.1 Hebbian Learning

Definition D.1 (Classical Hebbian Rule):
\frac{dw_{ij}}{dt} = \eta \nu_i \nu_j - \gamma w_{ij}


where $\nu_i, \nu_j$ are pre- and post-synaptic activities, $\eta$ is learning rate, and $\gamma$ is decay constant.

Proposition D.1 (Fixed Points): The Hebbian rule has fixed points at $w_{ij}^* = \frac{\eta}{\gamma} \nu_i \nu_j$.

D.2 Covariance Rule

Definition D.2 (Sejnowski's Covariance Rule):
\frac{dw_{ij}}{dt} = \eta (\nu_i - \langle \nu_i \rangle)(\nu_j - \langle \nu_j \rangle)

This rule produces Long-Term Potentiation (LTP) when activity is correlated above baseline, and Long-Term Depression (LTD) when correlated below baseline.

D.3 BCM Theory

Definition D.3 (Bienenstock-Cooper-Munro Rule) :
\frac{dw_{ij}}{dt} = \eta \nu_i \nu_j (\nu_j - \theta_j)


where $\theta_j$ is a sliding threshold:
\frac{d\theta_j}{dt} = \frac{1}{\tau}(\nu_j^2 - \theta_j)

Theorem D.1 (BCM Stability): The BCM rule with sliding threshold ensures that average synaptic weights remain bounded and selective.

D.4 Spike-Timing Dependent Plasticity (STDP)

Definition D.4 (Pair-Based STDP): For a pre-synaptic spike at time $t_{\text{pre}}$ and post-synaptic spike at $t_{\text{post}}$, the weight change is:
\Delta w = \begin{cases} A_+ \exp(-\Delta t / \tau_+) & \text{if } \Delta t = t_{\text{post}} - t_{\text{pre}} > 0 \\ -A_- \exp(\Delta t / \tau_-) & \text{if } \Delta t < 0 \end{cases}

Definition D.5 (Trace-Based STDP for Rate-Coded Networks):
\Delta w = A_+ \text{tr}_{\text{pre}} \nu_{\text{post}} - A_- \text{tr}_{\text{post}} \nu_{\text{pre}}


where traces decay as $\frac{d\text{tr}}{dt} = -\frac{\text{tr}}{\tau} + \nu$.

D.5 Reconciliation of STDP and BCM

Theorem D.2 (Bush et al. 2010) : Under specific conditions on the STDP learning window and spike-pair interactions, the average effect of STDP approximates the BCM rule. The relative scale of weights depends on the asymmetry of the learning window.

---

Appendix E: Hamiltonian Formulation of Multi-Nuclei Systems

E.1 Hamiltonian Dynamics

Let the global state be described by generalized coordinates $\mathbf{q}_i$ (capsule output vectors) and conjugate momenta $\mathbf{p}_i$ (internal state variables).

Definition E.1 (System Hamiltonian):
H(\{\mathbf{q}_i, \mathbf{p}_i\}) = \sum_i \left( \frac{1}{2} \|\mathbf{p}_i\|^2 + V_i(\mathbf{q}_i) \right) + \sum_{(i,j) \in E} \Phi_{ij}(\mathbf{q}_i, \mathbf{q}_j)


where:

· $V_i$ is the internal potential of capsule $i$ (derived from its activation function)
· $\Phi_{ij}$ is the interaction potential via the highway (agreement energy)

Definition E.2 (Hamilton's Equations):
\dot{\mathbf{q}}_i = \frac{\partial H}{\partial \mathbf{p}_i} = \mathbf{p}_i


\dot{\mathbf{p}}_i = -\frac{\partial H}{\partial \mathbf{q}_i} = -\nabla V_i(\mathbf{q}_i) - \sum_{j \in \mathcal{N}(i)} \nabla_{\mathbf{q}_i} \Phi_{ij}(\mathbf{q}_i, \mathbf{q}_j)

E.2 Port-Hamiltonian Formulation with Dissipation

Following , we incorporate dissipation:

Definition E.3 (Port-Hamiltonian System):
\begin{pmatrix} \dot{\mathbf{q}} \\ \dot{\mathbf{p}} \end{pmatrix} = \left( \begin{pmatrix} 0 & \mathbf{I} \\ -\mathbf{I} & -\mathbf{D}(\mathbf{q},\mathbf{p}) \end{pmatrix} \right) \begin{pmatrix} \nabla_{\mathbf{q}} H \\ \nabla_{\mathbf{p}} H \end{pmatrix} + \begin{pmatrix} 0 \\ \mathbf{G}(\mathbf{q}) \end{pmatrix} \mathbf{u}


where $\mathbf{D}(\mathbf{q},\mathbf{p})$ is a positive semidefinite damping matrix representing homeostatic plasticity, and $\mathbf{u}$ represents external inputs.

E.3 Energy Boundedness Theorem

Theorem E.1 (Hamiltonian Stability with Homeostatic Scaling): Under homeostatic plasticity that caps total incoming weight to each capsule, and with Lipschitz continuous potentials $V_i$ and $\Phi_{ij}$, the total energy $H(t)$ remains bounded for all time.

Proof: The time derivative of $H$ along system trajectories is:
\dot{H} = \sum_i \left( \nabla_{\mathbf{q}_i}H \cdot \dot{\mathbf{q}}_i + \nabla_{\mathbf{p}_i}H \cdot \dot{\mathbf{p}}_i \right)

Substituting Hamilton's equations with damping:
\dot{H} = \sum_i \left( \mathbf{p}_i \cdot \mathbf{p}_i + (-\nabla V_i - \sum_j \nabla \Phi_{ij}) \cdot (-\nabla V_i - \sum_j \nabla \Phi_{ij} - \mathbf{D} \mathbf{p}_i) \right)

Simplifying:
\dot{H} = -\sum_i \mathbf{p}_i^T \mathbf{D} \mathbf{p}_i \leq 0

The damping matrix $\mathbf{D}$ is positive semidefinite due to homeostatic mechanisms that prevent unbounded growth. Therefore, $H$ is non-increasing. Since $H$ is bounded below (potentials are bounded due to squashing function), $H(t)$ remains bounded for all time.

Furthermore, by Barbalat's lemma, $\dot{H} \to 0$, implying $\mathbf{D}^{1/2} \mathbf{p}_i \to 0$, i.e., the system converges to a set where dissipation vanishes. ∎

E.4 Synchronization Energy

Following , we define synchronization energy for coupled nuclei:

Definition E.4: For two nuclei with states $(\mathbf{q}_1, \mathbf{p}_1)$ and $(\mathbf{q}_2, \mathbf{p}_2)$, the synchronization energy is:
E_{\text{sync}} = \frac{1}{2}\|\mathbf{q}_1 - \mathbf{q}_2\|^2 + \frac{1}{2}\|\mathbf{p}_1 - \mathbf{p}_2\|^2

Corollary E.1: Under the conditions of Theorem E.1, $E_{\text{sync}}$ decays asymptotically to zero if the coupling $\Phi_{12}$ is sufficiently strong.

---

Appendix F: Causal Emergence Framework

F.1 Effective Information

Following Hoel et al. , we define effective information (EI) for a system with transition probability matrix $T$:

Definition F.1 (Effective Information):
\text{EI}(T) = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n T_{ij} \log_2 \left( \frac{T_{ij}}{\frac{1}{n} \sum_{k=1}^n T_{kj}} \right)


where $n$ is the number of system states.

EI measures the degree of determinism and degeneracy in the system's causal structure.

F.2 Causal Emergence

Definition F.2 (Causal Emergence): For a micro-scale system $S$ and a macro-scale coarse-graining $\phi$, the causal emergence is:
\Delta C = \text{EI}(M_\phi) - \text{EI}(S)


where $M_\phi$ is the macro-scale transition matrix induced by $\phi$.

Theorem F.1 (Macro Can Beat Micro) : There exist systems and coarse-grainings for which $\Delta C > 0$, meaning the macro-scale has greater causal power than the micro-scale.

Proof Sketch: Construct examples where micro-scale has high degeneracy (many micro-states lead to same outcome) but coarse-graining eliminates degeneracy, increasing determinism enough to overcome the reduction in state space size.

F.3 Application to Multi-Nuclei Systems

For a system with multiple nuclei, define:

· Micro-scale: individual capsule states $\{\mathbf{u}_c\}$
· Macro-scale: nucleus-level variables (e.g., centroid $\bar{\mathbf{u}}_k = \frac{1}{|C_k|}\sum_{c \in C_k} \mathbf{u}_c$)

Proposition F.1 (Emergence in Multi-Nuclei Systems): If capsules within a nucleus are highly synchronized, the macro-scale nucleus representation may have higher EI than the micro-scale capsule representation.

Proof: Synchronization creates degeneracy at micro-scale (many capsule configurations map to same macro-state), which is eliminated at macro-scale, potentially increasing EI.

F.4 Quantifying Distributed Intelligence

Definition F.3 (Distributed Intelligence Metric):
\mathcal{D} = \sum_{k=1}^K \Delta C_k + I(N_1; N_2; \ldots; N_K)


where $\Delta C_k$ is the causal emergence for nucleus $k$, and $I$ is the multi-information measuring statistical dependence between nuclei.

A high $\mathcal{D}$ indicates both that nuclei are internally coherent (emergence) and that they coordinate meaningfully (mutual information).

---

Appendix G: Resource Efficiency Metric

Definition G.1 (Resource Efficiency): For a time interval $[t_0, t_1]$, the resource efficiency is:
\mathcal{R} = \frac{P(t_1) - P(t_0)}{\Delta_{\text{struct}} + \epsilon}


where $P(t)$ is task performance, and $\Delta_{\text{struct}}$ is the number of structural changes (capsule additions/deletions, significant weight modifications).

Proposition G.1 (Bounded Efficiency): For a system satisfying Theorem E.1, $\mathcal{R}$ remains bounded as long as performance improvements are bounded.

---

References for Appendices

[1] Babaeian, B., & Yamakou, M. E. (2025). Asymptotic stability proof and port-Hamiltonian physics-informed neural network approach to chaotic synchronization. arXiv:2511.04809. 

[2] Hoel, E. P., Albantakis, L., & Tononi, G. (2013). Quantifying causal emergence shows that macro can beat micro. PNAS, 110(49), 19790-19795. 

[3] Pham, H., et al. (2025). EADR: Entropy Adjusted Dynamic Routing Capsule Networks. 59th Annual Conference on Information Sciences and Systems (CISS). 

[4] Boccato, T., Ferrante, M., & Toschi, N. (2024). A differentiable model for optimizing the genetic drivers of synaptogenesis. eLife (reviewed preprint). 

[5] Bush, D., Philippides, A., Husbands, P., & O'Shea, M. (2010). Reconciling the STDP and BCM models of synaptic plasticity in a spiking recurrent neural network. Neural Computation, 22(8), 2059-2085. 

[6] Yamakou, M. E. (2020). Chaotic synchronization of memristive neurons: Lyapunov function versus Hamilton function. Nonlinear Dynamics. 

[7] Yang, Y., et al. (2025). The Convergence of Dynamic Routing between Capsules. arXiv:2501.06240. 

