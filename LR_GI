Local Rationality, Global Instability: Formalizing and Mitigating Incentive-Driven Collapse in Multi-Agent Stochastic Games

Author: Ouadi Maakoul
Date: February 25, 2026

---

Abstract

In open multi-agent ecosystems, individually rational agents optimizing local reward functions often precipitate systemic collapse. This thesis formalizes the condition $\text{Alignment}_\text{local} \not\Rightarrow \text{Stability}_\text{global}$ within the framework of general-sum stochastic games. We identify five structural mechanisms driving this divergence—incentive misalignment, information asymmetry, power concentration, collusion, and dynamical chaos—and prove that instability occurs iff the game lacks potential structure and the learning dynamics violate Lyapunov stability. We propose the Stability Stack, a layered intervention comprising: (i) intrinsic reward shaping to approximate potential games; (ii) Lyapunov-constrained policy optimization to bound chaotic dynamics; and (iii) extrinsic regulatory mechanisms (dynamic taxation, information symmetry enforcement) to curb power drift and collusion. We prove sufficient conditions for global stability, provide concrete examples, and characterize the residual open challenges of observability, regulator capture, and computational tractability.

---

Table of Contents

1. Introduction
2. Preliminaries and Formal Model
3. Mechanisms of Instability
4. The Stability Stack: Layered Solutions
5. Theoretical Guarantees
6. Open Challenges and Future Work
7. Conclusion
   Bibliography

---

Chapter 1: Introduction

1.1 Motivation

On May 6, 2010, the U.S. stock market experienced a flash crash that saw the Dow Jones Industrial Average plunge nearly 1,000 points in minutes before recovering. The event was later attributed to a large automated sell order interacting with high-frequency trading algorithms—each algorithm acting rationally according to its own objectives, yet collectively causing a systemic breakdown. This is but one example of a pervasive phenomenon: in systems composed of autonomous, incentive-driven agents, local rationality can lead to global instability.

Similar patterns appear in communication networks (congestion collapse), cloud computing (resource contention), decentralized finance (liquidity crises), and multi-agent reinforcement learning (reward hacking). In each case, agents are locally aligned with their individual tasks, yet the emergent global dynamics degrade a system-level measure of welfare, stability, or sustainability.

This thesis addresses the fundamental question: When does local rationality imply global instability? We provide a rigorous mathematical framework to analyze this phenomenon and propose a structured set of interventions to restore stability without sacrificing individual agency.

1.2 Problem Statement

We consider a system of $N$ adaptive agents interacting in a stochastic environment. Each agent maximizes its own discounted cumulative reward. We assume that rewards are locally aligned with some intended task, i.e., each agent's reward function approximates a task-specific utility. However, despite local alignment, the joint behavior of agents may degrade a global stability functional $\Phi$ beyond acceptable bounds.

Formally, we define emergent incentive instability as the existence of a Nash equilibrium $\pi^*$ of the multi-agent game such that:

1. Each agent's policy is individually optimal: $\pi_i^* = \arg\max_{\pi_i} J_i(\pi_i, \pi_{-i}^*)$.
2. The long-run average of $\Phi$ exceeds a critical threshold: $\limsup_{T\to\infty} \frac{1}{T}\sum_{t=0}^T \Phi(S^t) > C_{\text{critical}}$.

Our goal is to characterize conditions under which such instability arises and to design mechanisms that guarantee global stability while preserving local rationality.

1.3 Contributions

The main contributions of this thesis are:

1. A Formal Framework: We provide a precise mathematical model of multi-agent stochastic games with local alignment and global stability functionals, along with definitions of five key instability mechanisms.
2. Structural Analysis: We prove necessary and sufficient conditions for the emergence of instability, linking it to the absence of potential game structure and the presence of positive Lyapunov exponents in learning dynamics.
3. The Stability Stack: We propose a three-layer architecture (intrinsic, dynamic, extrinsic) that combines reward shaping, Lyapunov-constrained learning, and regulatory mechanisms to enforce global stability.
4. Theoretical Guarantees: We prove sufficient conditions for stability and characterize the trade-offs involved in each layer.
5. Open Challenges: We identify fundamental limitations—observability, regulator capture, computability—that must be addressed for real-world deployment.

1.4 Outline

Chapter 2 introduces the formal model, including stochastic games, Nash equilibrium, and stability functionals, with a concrete example. Chapter 3 dissects the five mechanisms that drive instability, each with its own subsection, formal definitions, and propositions. Chapter 4 presents the Stability Stack, detailing each layer with mathematical rigor and proofs. Chapter 5 provides theoretical guarantees, including a complete proof of the sufficiency theorem. Chapter 6 discusses open challenges in depth. Chapter 7 concludes with broader implications.

---

Chapter 2: Preliminaries and Formal Model

2.1 Stochastic Games

A stochastic game (Markov game) is defined by the tuple

\mathcal{G} = \langle \mathcal{A}, \mathcal{S}, \{ \mathcal{A}_i \}_{i=1}^N, P, \{ R_i \}_{i=1}^N, \gamma \rangle,

where:

· $\mathcal{A} = \{1, \dots, N\}$ is the set of agents.
· $\mathcal{S}$ is a finite or compact state space.
· $\mathcal{A}_i$ is the action space of agent $i$; let $\mathcal{A} = \times_{i=1}^N \mathcal{A}_i$ denote the joint action space.
· $P: \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})$ is the transition probability kernel; $\Delta(\mathcal{S})$ denotes the set of probability distributions over $\mathcal{S}$.
· $R_i: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ is the bounded reward function for agent $i$.
· $\gamma \in (0,1)$ is the discount factor.

Time is discrete: $t = 0,1,2,\dots$. At each time $t$, the environment is in state $S^t$. Each agent $i$ selects an action $a_i^t$ according to a policy $\pi_i: \mathcal{S} \to \Delta(\mathcal{A}_i)$. The joint action $a^t = (a_1^t, \dots, a_N^t)$ is taken, and the next state $S^{t+1}$ is drawn from $P(\cdot \mid S^t, a^t)$. Agent $i$ receives reward $R_i(S^t, a^t)$.

2.2 Policies and Value Functions

A stationary policy for agent $i$ is a mapping $\pi_i: \mathcal{S} \to \Delta(\mathcal{A}_i)$. The set of all such policies is denoted $\Pi_i$. A joint policy $\pi = (\pi_1, \dots, \pi_N) \in \Pi = \times_i \Pi_i$ induces a Markov chain over $\mathcal{S} \times \mathcal{A}$.

The value function for agent $i$ under joint policy $\pi$ is

V_i^\pi(s) = \mathbb{E}^\pi\left[ \sum_{t=0}^\infty \gamma^t R_i(S^t, a^t) \mid S^0 = s \right],

where $\mathbb{E}^\pi$ denotes expectation with respect to the stochastic process induced by $\pi$ and $P$.

The objective of agent $i$ is to maximize its expected discounted return from an initial state distribution $\mu_0$:

J_i(\pi) = \mathbb{E}_{S^0 \sim \mu_0}[V_i^\pi(S^0)].

2.3 Nash Equilibrium

A joint policy $\pi^*$ is a Nash equilibrium if for every agent $i$ and every alternative policy $\pi_i' \in \Pi_i$,

J_i(\pi_i^*, \pi_{-i}^*) \ge J_i(\pi_i', \pi_{-i}^*),

where $\pi_{-i}^*$ denotes the policies of all agents except $i$.

2.4 Local Alignment Assumption

We assume that each reward function can be decomposed as

R_i(s,a) = U_i(s,a) + \epsilon_i(s,a),

where $U_i$ represents the intended task utility (e.g., aligned with a system designer's goal) and $\epsilon_i$ is bounded mis-specification noise. We require that $|\epsilon_i(s,a)| \le \varepsilon$ for all $s,a$, with $\varepsilon$ small relative to the scale of $U_i$. This captures the idea that individually, each agent's objective is "close" to a well-intentioned task.

2.5 Global Stability Functional

To quantify system-level health, we introduce a global functional $\Phi: \mathcal{S} \to \mathbb{R}$. Examples include:

· Total welfare: $\Phi(s) = \sum_i U_i(s, \pi(s))$.
· Resource balance: $\Phi(s) = \text{Herfindahl index}^{-1}$.
· Information symmetry: $\Phi(s) = -\sum_{i\neq j} D_{KL}(b_i \| b_j)$.
· System entropy: $\Phi(s) = H(s)$.

We say the system is stable if the long-run average of $\Phi$ is bounded below a critical level (or above, depending on orientation). For definiteness, assume higher $\Phi$ is better. Then stability requires

\liminf_{T\to\infty} \frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge C_{\text{critical}}.

2.6 Emergent Instability Definition

Definition 2.1 (Emergent Incentive Instability). Given a stochastic game $\mathcal{G}$ with local alignment ($R_i = U_i + \epsilon_i$), a global functional $\Phi$, and a critical threshold $C_{\text{critical}}$, we say the system exhibits emergent incentive instability if there exists a Nash equilibrium $\pi^*$ such that:

1. (Individual Optimality) For all $i$, $\pi_i^* = \arg\max_{\pi_i} J_i(\pi_i, \pi_{-i}^*)$.
2. (Global Degradation) Under $\pi^*$, the induced process satisfies

\liminf_{T\to\infty} \frac{1}{T} \sum_{t=0}^T \Phi(S^t) < C_{\text{critical}}.

Example 2.1 (Tragedy of the Commons). Consider $N$ agents sharing a common resource pool. Each agent can extract either sustainably or overexploit. Individually, overexploitation yields higher immediate reward, but if all overexploit, the resource collapses. This is a stochastic game where local rationality leads to global degradation. We will return to this example throughout.

---

Chapter 3: Mechanisms of Instability

We identify five fundamental mechanisms that cause local rationality to diverge from global stability. Each is formalized with definitions, propositions, and examples.

3.1 Incentive Misalignment

Let $W = \sum_{i=1}^N \alpha_i J_i$ be a weighted global welfare function (e.g., $\alpha_i = 1/N$). Define the incentive divergence operator for agent $i$ as

\Delta_i(\pi) = \nabla_{\pi_i} J_i(\pi) - \lambda \nabla_{\pi_i} W(\pi),

where $\lambda > 0$ is a scaling factor. If $\Delta_i \neq 0$, then agent $i$'s local gradient does not align with the gradient of global welfare.

Proposition 3.1 (Incentive Alignment in Potential Games). If the game is an exact potential game with potential $\Psi = W$, then $\Delta_i = 0$ for all $i$ (up to scaling). Conversely, if $\Delta_i \neq 0$ for some $i$, the game cannot be an exact potential game with potential proportional to $W$.

Proof. Suppose the game is an exact potential game with potential $\Psi$. Then by definition, for any agent $i$ and any two policies $\pi_i, \pi_i'$,

J_i(\pi_i, \pi_{-i}) - J_i(\pi_i', \pi_{-i}) = \Psi(\pi_i, \pi_{-i}) - \Psi(\pi_i', \pi_{-i}).

Taking the limit as $\pi_i' \to \pi_i$ in the direction of a small perturbation, we obtain the gradient equality:

\nabla_{\pi_i} J_i = \nabla_{\pi_i} \Psi.

If $\Psi = \lambda W$ for some $\lambda > 0$, then $\nabla_{\pi_i} J_i = \lambda \nabla_{\pi_i} W$, so $\Delta_i = \nabla_{\pi_i} J_i - \lambda \nabla_{\pi_i} W = 0$ for all $i$.

Conversely, suppose there exists an agent $i$ such that $\Delta_i \neq 0$, i.e., $\nabla_{\pi_i} J_i \neq \lambda \nabla_{\pi_i} W$ for any $\lambda$ (or for the specific $\lambda$ under consideration). If the game were an exact potential game with potential proportional to $W$, then there would exist some $\lambda$ such that $\nabla_{\pi_i} J_i = \lambda \nabla_{\pi_i} W$ for all $i$, contradicting $\Delta_i \neq 0$. Hence no such potential exists. ∎

Example 3.1 (Non-potential game). Consider a two-player, two-action game with payoffs: (C,C): (2,2), (C,D): (0,3), (D,C): (3,0), (D,D): (1,1). This is a Prisoner's Dilemma, which is not a potential game because there is no function $\Psi$ satisfying the difference condition. Here, individual rationality (choosing D) leads to the inefficient (D,D) outcome.

3.2 Information Asymmetry

Let $b_i^t \in \Delta(\mathcal{S})$ denote agent $i$'s belief about the current state, updated via Bayesian inference from private observations. Define the information asymmetry measure at time $t$ as

\mathcal{I}^t = \sum_{i \neq j} D_{KL}(b_i^t \| b_j^t).

Proposition 3.2. If agents have common knowledge of the state (i.e., $b_i^t = b_j^t$ for all $i,j$), then $\mathcal{I}^t = 0$. Conversely, if $\mathcal{I}^t > 0$, there exist states where agents disagree, enabling strategic deception.

Proof. The first statement is immediate. For the second, if $\mathcal{I}^t > 0$, then some $D_{KL}(b_i^t \| b_j^t) > 0$, so $b_i^t \neq b_j^t$. In such a situation, agent $i$ can take actions that agent $j$ does not anticipate, potentially harming $j$ without immediate detection. ∎

Example 3.2 (Signaling game). In a market, a seller may have private information about product quality. Buyers form beliefs based on price. The seller can signal quality through price, potentially leading to inefficient equilibria where high-quality goods are underpriced to signal.

3.3 Power-Seeking Drift

Let $R_i^t$ denote a measure of resources or influence held by agent $i$ at time $t$. Define total resources $R_{\text{total}}^t = \sum_i R_i^t$. The Herfindahl index (concentration) is

H^t = \sum_{i=1}^N \left( \frac{R_i^t}{R_{\text{total}}^t} \right)^2.

$H^t$ ranges from $1/N$ (equal distribution) to $1$ (monopoly). Power-seeking drift occurs when $\lim_{t\to\infty} H^t \to 1$.

Proposition 3.3. In a stochastic game where agents can accumulate resources and rewards are increasing in own resources, Nash equilibria tend to concentrate resources if there are increasing returns to scale. Formally, if the marginal benefit of an extra unit of resource is increasing in current resources, then the only stable equilibria are monopolistic.

Proof sketch. This is a standard result in growth models with strategic interactions. Under convex returns, the agent with a slight advantage invests more, widening the gap. ∎

Example 3.3 (Wealth concentration). In algorithmic trading, firms with faster algorithms capture more profits, enabling them to invest in even faster algorithms, leading to a winner-take-all market.

3.4 Collusion

A coalition $C \subseteq \mathcal{A}$ can coordinate their actions. Let $J_C = \sum_{i \in C} J_i$. Collusion is incentive-compatible at a Nash equilibrium $\pi^*$ if there exists a joint deviation $\tilde{\pi}_C$ such that

J_C(\tilde{\pi}_C, \pi_{-C}^*) > \sum_{i \in C} J_i(\pi^*),

and the deviation is undetectable or unpenalized by the environment or other agents.

Proposition 3.4. In any game, if there exists a coalition $C$ with a profitable undetectable deviation, then $\pi^*$ is not a strong Nash equilibrium (i.e., not coalition-proof). The existence of such coalitions can lead to outcomes worse for non-members.

Proof. By definition, a strong Nash equilibrium requires no coalitional deviation. The inequality shows such a deviation exists, so $\pi^*$ is not strong. ∎

Example 3.4 (Price-fixing cartel). Firms in an oligopoly may collude to set high prices, increasing their joint profits at the expense of consumers. If detection is unlikely, the cartel is stable.

3.5 Dynamical Chaos

Consider gradient ascent learning:

\pi_i^{t+1} = \pi_i^t + \eta \nabla_{\pi_i} J_i(\pi^t).

This defines a discrete-time dynamical system. It is chaotic if it has sensitive dependence on initial conditions and a positive maximum Lyapunov exponent $\lambda_{\max} > 0$.

Proposition 3.5. In a zero-sum game, the gradient dynamics are Hamiltonian and can exhibit chaos. In general-sum games, chaos can arise even in simple settings, preventing convergence to any equilibrium.

Proof sketch. See (Balduzzi et al., 2018) for examples in differentiable games. ∎

Example 3.5 (Rock-paper-scissors). The cyclic nature of rock-paper-scissors can lead to oscillatory or chaotic dynamics under gradient-based learning.

---

Chapter 4: The Stability Stack: Layered Solutions

We propose a three-layer architecture—the Stability Stack—to systematically address the mechanisms of instability.

4.1 Layer 1: Intrinsic Structure – Potential Game Transformation

Goal: Align local incentives with global welfare by making the game a potential game with potential $\Psi \approx \Phi$.

4.1.1 Potential Games

Recall that a game is an exact potential game if there exists $\Psi: \Pi \to \mathbb{R}$ such that for all $i$ and $\pi_i, \pi_i'$,

J_i(\pi_i, \pi_{-i}) - J_i(\pi_i', \pi_{-i}) = \Psi(\pi_i, \pi_{-i}) - \Psi(\pi_i', \pi_{-i}).

4.1.2 Reward Shaping

We modify each agent's reward by adding a shaping term $\Phi(s') - \Phi(s)$ (scaled appropriately). Specifically, define

R_i'(s,a,s') = R_i(s,a) + \gamma \Phi(s') - \Phi(s),

where $\Phi$ is the global functional. This shaping is potential-based and preserves Nash equilibria.

Theorem 4.1 (Equilibrium Preservation under Reward Shaping). If all agents receive the same shaping term $\gamma \Phi(s') - \Phi(s)$, then for any joint policy $\pi$, the difference $\tilde{J}_i(\pi) - J_i(\pi)$ is independent of $\pi_i$ (given a fixed initial state distribution). Consequently, the set of Nash equilibria of the original game is identical to that of the shaped game.

Proof. Let $\tilde{R}_i(s,a,s') = R_i(s,a) + \gamma \Phi(s') - \Phi(s)$. For any joint policy $\pi$, the expected discounted return under $\tilde{R}_i$ is

\tilde{J}_i(\pi) = \mathbb{E}^\pi\left[ \sum_{t=0}^\infty \gamma^t R_i(S^t, a^t) \right] + \mathbb{E}^\pi\left[ \sum_{t=0}^\infty \gamma^t (\gamma \Phi(S^{t+1}) - \Phi(S^t)) \right].

The second term telescopes:

\sum_{t=0}^\infty \gamma^t (\gamma \Phi(S^{t+1}) - \Phi(S^t)) = \lim_{T\to\infty} \left( \gamma^{T+1} \Phi(S^{T+1}) - \Phi(S^0) \right).

Assuming $\Phi$ is bounded, the limit term vanishes as $T\to\infty$, so the sum equals $-\Phi(S^0)$. Hence

\tilde{J}_i(\pi) = J_i(\pi) - \mathbb{E}^\pi[\Phi(S^0)].

The initial state distribution $\mu_0$ is fixed and independent of the policies. Therefore $\mathbb{E}^\pi[\Phi(S^0)] = \mathbb{E}_{S^0 \sim \mu_0}[\Phi(S^0)]$ is a constant $C$ that does not depend on $\pi$. Thus

\tilde{J}_i(\pi) = J_i(\pi) - C.

Now consider any agent $i$ and any two policies $\pi_i, \pi_i'$. Holding $\pi_{-i}$ fixed,

\tilde{J}_i(\pi_i, \pi_{-i}) - \tilde{J}_i(\pi_i', \pi_{-i}) = (J_i(\pi_i, \pi_{-i}) - C) - (J_i(\pi_i', \pi_{-i}) - C) = J_i(\pi_i, \pi_{-i}) - J_i(\pi_i', \pi_{-i}).

Thus the payoff differences are unchanged, so best responses are identical. Hence $\pi^*$ is a Nash equilibrium in the original game if and only if it is a Nash equilibrium in the shaped game. ∎

4.1.3 Approximate Potential Games

Exact potential may be unattainable. We relax to $\delta$-potential games.

Definition 4.1 ($\delta$-Potential Game). A game is a $\delta$-potential game if there exists $\Psi$ such that for all $i$ and $\pi_i, \pi_i'$,

| (J_i(\pi_i, \pi_{-i}) - J_i(\pi_i', \pi_{-i})) - (\Psi(\pi_i, \pi_{-i}) - \Psi(\pi_i', \pi_{-i})) | \le \delta.

Proposition 4.1 (Approximate Alignment). If the game is a $\delta$-potential game with potential $\Psi = \mathbb{E}[\sum_{t=0}^\infty \gamma^t \Phi(S^t)]$, then any Nash equilibrium $\pi^*$ satisfies

\Phi(\pi^*) \ge \max_{\pi} \Phi(\pi) - \frac{2\delta}{1-\gamma},

where $\Phi(\pi) = \mathbb{E}^\pi[\sum_{t=0}^\infty \gamma^t \Phi(S^t)]$.

Proof. This result follows from standard properties of approximate potential games; see e.g., [Candogan et al., 2013] for a detailed proof. The intuition is that at a Nash equilibrium, no agent can gain more than $\delta$ by deviating, which implies that the potential cannot be far from its maximum. The factor $1/(1-\gamma)$ arises because the potential is a discounted sum, and the stage-game deviations accumulate over time. ∎

4.2 Layer 2: Learning Dynamics – Lyapunov-Constrained Policy Optimization

Goal: Ensure that during learning, the global functional $\Phi$ does not degrade uncontrollably.

4.2.1 Control Lyapunov Function

Define $V(s) = K - \Phi(s)$ where $K \ge \sup \Phi$ (e.g., $K = \sup_s \Phi(s)$). Then $V \ge 0$, and decreasing $V$ increases $\Phi$.

We impose that each policy update $\delta\pi$ satisfies

\mathbb{E}_{S^{t+1} \sim P(\cdot|S^t, a^t)}[V(S^{t+1})] \le (1-\alpha) V(S^t),

for some $\alpha \in (0,1)$. This is a Lyapunov condition ensuring $V$ contracts in expectation.

4.2.2 Constrained Gradient Update

Let $g = (\nabla_{\pi_1} J_1, \dots, \nabla_{\pi_N} J_N)$ be the joint gradient. We solve

\begin{aligned}
\min_{\delta\pi} &\quad \|\delta\pi - \eta g\|^2 \\
\text{s.t.} &\quad \mathbb{E}_{a \sim \pi + \delta\pi, s' \sim P(\cdot|S^t,a)}[V(s')] \le (1-\alpha) V(S^t).
\end{aligned}

If the constraint is infeasible, we set $\delta\pi = 0$ (or take a smaller step). This is a quadratic program that can be solved efficiently when the number of parameters is moderate.

Theorem 4.2 (Stability of Constrained Dynamics). If at each step the policy update satisfies the Lyapunov constraint, then $V(S^t)$ converges almost surely to $0$, i.e., $\Phi(S^t) \to K$, the maximum possible value. Consequently,

\liminf_{T\to\infty} \frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge K.

Proof. Define $\mathcal{F}^t$ as the sigma-algebra generated by the history up to time $t$. The constraint gives

\mathbb{E}[V(S^{t+1}) \mid \mathcal{F}^t] \le (1-\alpha) V(S^t).

Since $V \ge 0$, the process $\{V(S^t)\}$ is a nonnegative supermartingale with respect to $\mathcal{F}^t$. By the supermartingale convergence theorem (Doob, 1953), $V(S^t)$ converges almost surely to a finite random variable $V_\infty$. Moreover, taking expectations,

\mathbb{E}[V(S^{t+1})] \le (1-\alpha) \mathbb{E}[V(S^t)] \le (1-\alpha)^{t+1} \mathbb{E}[V(S^0)].

Hence $\mathbb{E}[V(S^t)] \to 0$. By Fatou's lemma,

\mathbb{E}[V_\infty] \le \liminf_{t\to\infty} \mathbb{E}[V(S^t)] = 0,

so $V_\infty = 0$ almost surely. Therefore $\Phi(S^t) = K - V(S^t) \to K$ almost surely.

Now, since $\Phi(S^t) \to K$ a.s., for any $\epsilon > 0$, there exists $T_0$ such that for all $t \ge T_0$, $\Phi(S^t) \ge K - \epsilon$. Then for $T > T_0$,

\frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge \frac{1}{T} \left( \sum_{t=0}^{T_0-1} \Phi(S^t) + (T - T_0 + 1)(K - \epsilon) \right).

Taking $\liminf$ as $T \to \infty$, the first term vanishes, yielding

\liminf_{T\to\infty} \frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge K - \epsilon.

Since $\epsilon$ is arbitrary, the liminf is at least $K$. ∎

4.3 Layer 3: Extrinsic Regulation – Taxation and Information Symmetry

Goal: Address power concentration, information asymmetry, and collusion.

4.3.1 Anti-Concentration Taxation

Modify rewards with a convex tax on resource share:

R_i^{\text{tax}}(s,a) = R_i(s,a) - \mu \left( \frac{R_i}{R_{\text{total}}} \right)^k, \quad k > 1, \mu > 0.

Proposition 4.2 (Bounded Concentration). In any Nash equilibrium of the modified game, the Herfindahl index satisfies $H \le H_{\max} < 1$, where $H_{\max}$ depends on $\mu, k$ and the marginal benefits.

Proof. The tax modifies each agent's reward by subtracting $\mu (R_i/R_{\text{total}})^k$. At any Nash equilibrium, the marginal benefit of increasing $R_i$ must equal the marginal tax cost. Assuming the marginal benefit of resources is bounded above by $M$, we have for any agent $i$,

\mu k \frac{R_i^{k-1}}{R_{\text{total}}^k} \le M \quad \Rightarrow \quad \frac{R_i}{R_{\text{total}}} \le \left( \frac{M R_{\text{total}}^{k-1}}{\mu k} \right)^{1/(k-1)}.

Since $R_{\text{total}}$ is bounded by the total resources in the system, the right-hand side is a constant $c < 1$ (provided $\mu$ is large enough). Thus each agent's share is at most $c$. Choosing $\mu$ such that $c < 1/\sqrt{N}$ ensures $H \le N c^2 < 1$. Hence $H$ is bounded away from 1. ∎

4.3.2 Information Symmetry Enforcement

Introduce a public belief $b_{\text{pub}}^t$ (e.g., via a shared ledger) and penalize deviations:

\tilde{J}_i = J_i - \beta \, D_{KL}(b_i^t \| b_{\text{pub}}^t), \quad \beta > 0.

Proposition 4.3 (Vanishing Asymmetry). If $\beta > \max_i |\frac{\partial J_i}{\partial \text{deception}}|$, then at any Nash equilibrium, $b_i^t = b_{\text{pub}}^t$ for all $i$, so $\mathcal{I}^t = 0$.

Proof. The penalty term is minimized when $b_i^t = b_{\text{pub}}^t$. If the potential gain from deception is less than $\beta$ times the derivative of the KL divergence, deviating is suboptimal. More formally, suppose $b_i^t \neq b_{\text{pub}}^t$. Then the penalty is positive. The maximum possible gain from deception is bounded by $\max_i |\frac{\partial J_i}{\partial \text{deception}}|$. If $\beta$ exceeds this bound, the penalty outweighs any possible gain, so any equilibrium must have $b_i^t = b_{\text{pub}}^t$. ∎

4.3.3 Collusion Deterrence via Audits

Random audits detect collusion with probability $p_{\text{detect}}$. If detected, a penalty $P$ is imposed. Collusion is deterred if

p_{\text{detect}} \cdot P > \max_{C} \left( J_C(\text{deviation}) - \sum_{i\in C} J_i(\pi^*) \right).

By setting $P$ sufficiently high (or $p_{\text{detect}}$ close to 1), we can make any collusion unprofitable.

---

Chapter 5: Theoretical Guarantees

We now combine the layers to provide sufficient conditions for global stability and a necessary condition for instability.

5.1 Sufficient Conditions for Global Stability

Theorem 5.1 (Sufficient Conditions). Consider a stochastic game $\mathcal{G}$ with local alignment and a global functional $\Phi$. Suppose:

1. Potential Structure: The game is transformed (via Layer 1) into an exact potential game with potential $\Psi = \mathbb{E}[\sum_{t=0}^\infty \gamma^t \Phi(S^t)]$.
2. Lyapunov Constraint: The learning dynamics satisfy the Lyapunov condition of Layer 2 with $\alpha > 0$.
3. Bounded Concentration: The tax mechanism ensures $H^t \le H_{\max} < 1$ a.s.
4. Vanishing Asymmetry: The information penalty ensures $\mathcal{I}^t \to 0$ a.s.
5. Collusion Deterrence: The audit mechanism ensures no profitable collusion.

Then the system is globally stable: $\liminf_{T\to\infty} \frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge \sup \Phi$ almost surely.

Proof. We prove via a series of lemmas.

Lemma 5.1 (Potential Equilibria are Optimal). In an exact potential game with potential $\Psi$, every Nash equilibrium $\pi^*$ satisfies $\Psi(\pi^*) = \max_{\pi} \Psi(\pi)$ (if $\Psi$ has a unique maximum) or is a local maximum. Since $\Psi = \mathbb{E}[\sum \gamma^t \Phi]$, this implies that $\Phi$ at equilibrium is maximized in expectation.

Proof. In an exact potential game, the potential function $\Psi$ has the property that any unilateral deviation by an agent changes $\Psi$ by exactly the same amount as the agent's payoff. Therefore, at a Nash equilibrium, no agent can improve their payoff, which implies that no unilateral deviation can increase $\Psi$. Hence $\pi^*$ is a local maximum of $\Psi$ (or a global maximum if the potential is concave). ∎

Lemma 5.2 (Lyapunov Convergence). Under the Lyapunov constraint, $\Phi(S^t) \to \sup \Phi$ almost surely. (Proof given in Theorem 4.2.)

Lemma 5.3 (Concentration Bound). $H^t \le H_{\max} < 1$ prevents monopolistic collapse, ensuring that the limiting state has diverse agents. (Proof in Proposition 4.2.)

Lemma 5.4 (Symmetric Information). $\mathcal{I}^t \to 0$ ensures that in the limit, agents have common knowledge, preventing hidden externalities. (Proof in Proposition 4.3.)

Lemma 5.5 (No Collusion). Collusion is unprofitable, so any limit point is a strong Nash equilibrium.

Proof. Let $\pi^*$ be a limit point of the dynamics (e.g., a Nash equilibrium of the regulated game). Suppose there exists a coalition $C$ and a joint deviation $\tilde{\pi}_C$ such that $J_C(\tilde{\pi}_C, \pi_{-C}^*) > \sum_{i \in C} J_i(\pi^*)$. The audit mechanism detects this deviation with probability $p_{\text{detect}}$ and imposes a penalty $P$ on each member. The expected payoff for each member $i$ under deviation is $J_i(\tilde{\pi}_C, \pi_{-C}^*) - p_{\text{detect}} P$. If $p_{\text{detect}} P > J_i(\tilde{\pi}_C, \pi_{-C}^*) - J_i(\pi^*)$ for all $i$, then the deviation is not profitable. By assumption, $p_{\text{detect}} P$ is chosen larger than the maximum possible gain for any coalition, so no such deviation exists. Hence $\pi^*$ is a strong Nash equilibrium. ∎

Now, by Lemma 5.2, $\Phi(S^t) \to \sup \Phi$. Therefore, the Cesàro mean also converges to $\sup \Phi$. The other lemmas ensure that this limit is consistent with the game's structure and not undermined by concentration, asymmetry, or collusion. ∎

5.2 Instability Criterion

Theorem 5.2 (Instability Criterion). Local rationality implies global instability if and only if the following hold simultaneously:

1. Non-Potential Structure: The game is not a potential game (or not sufficiently close) with respect to $\Phi$.
2. Unstable Learning Dynamics: The learning dynamics have $\lambda_{\max} > 0$, preventing convergence.
3. Persistent Asymmetry: $\limsup_{t\to\infty} \mathcal{I}^t > 0$.
4. Unchecked Power Concentration: $\lim_{t\to\infty} H^t \to 1$.
5. Profitable Collusion: There exists a coalition with a profitable undetectable deviation.

Proof. The "if" direction: If all hold, then there exists a Nash equilibrium (or limit cycle) that degrades $\Phi$. Non-potential allows inefficient equilibria; chaos means the system may not settle; asymmetry and collusion allow hidden actions; concentration leads to fragility. Hence instability.

The "only if" direction: If any condition fails, then at least one stabilizing mechanism is active. For example, if the game is potential, then by Lemma 5.1, any equilibrium is near-optimal. If Lyapunov condition holds, convergence is guaranteed. If asymmetry vanishes, no hidden externalities. If concentration bounded, no monopoly. If collusion deterred, no coalitional deviations. Therefore, instability cannot occur. ∎

5.3 Corollaries

Corollary 5.1. In a zero-sum game, condition 1 (non-potential) holds generically, and condition 2 (chaos) often holds, so instability is likely unless regulated.

Corollary 5.2. Adding small noise $\epsilon_i$ can break potential structure, triggering instability if other conditions align.

---

Chapter 6: Open Challenges and Future Work

Despite the theoretical framework, several fundamental challenges remain.

6.1 The Measurement Problem

All layers rely on observability of $\Phi$, $R_i$, $b_i^t$, etc. In practice, global welfare may be unobservable or only partially observable. Agents may optimize proxy metrics, leading to specification gaming. Future work could explore robust proxies or inverse reinforcement learning to infer $\Phi$.

6.2 The Regulator Capture Problem

Layer 3 introduces a regulator (tax authority, audit mechanism) that itself becomes a strategic target. Agents may attempt to influence the regulator's parameters. Designing incentive-compatible regulators (e.g., using cryptographic randomness or mechanism design) is an open problem.

6.3 The Computability Problem

Finding Nash equilibria in general-sum stochastic games is PPAD-complete. Enforcing stability constraints adds computational burden. Scalable algorithms (e.g., deep RL with Lagrangian methods) need further development.

6.4 The Interdependency Problem

The three layers interact. For example, taxation may distort the potential game structure; Lyapunov constraints may slow learning, making collusion easier. Optimizing the joint system is a multi-objective challenge.

6.5 The Incentive to Circumvent

Any fixed regulatory mechanism will eventually be gamed. Adaptive, randomized, or dynamic regulation may resist manipulation but could introduce instability. This is a rich area for future research.

---

Chapter 7: Conclusion

This thesis has formalized the paradox of multi-agent ecosystems: local rationality does not imply global stability. We have shown that instability arises from a combination of structural factors—lack of potential game structure, information asymmetry, power concentration, collusion, and chaotic dynamics—and provided a rigorous mathematical framework to analyze them.

We proposed the Stability Stack, a three-layer intervention that addresses these factors at design time, learning time, and run time. Each layer comes with theoretical guarantees, and together they provide sufficient conditions for global stability. We also characterized the necessary conditions for instability, giving a criterion that can be used to diagnose risky systems.

Despite these advances, open challenges remain, particularly around observability, regulator capture, and computational tractability. Addressing these will be crucial for deploying stable multi-agent systems in the real world.

The core message is hopeful but demanding: Stability is possible, but it must be engineered. It does not emerge naturally from local rationality alone. By combining intrinsic alignment, constrained learning, and extrinsic regulation, we can create systems that are both individually rational and globally stable.

Appendices

Appendix A: Detailed Proofs of Main Theorems

A.1 Proof of Proposition 3.1 (Incentive Alignment)

Proposition 3.1. If the game is an exact potential game with potential $\Psi = W$, then $\Delta_i = 0$ for all $i$ (up to scaling). Conversely, if $\Delta_i \neq 0$ for some $i$, the game cannot be an exact potential game with potential proportional to $W$.

Detailed Proof.
Recall that a game is an exact potential game if there exists a function $\Psi: \Pi \to \mathbb{R}$ such that for every agent $i$ and every pair of policies $\pi_i, \pi_i'$,

J_i(\pi_i, \pi_{-i}) - J_i(\pi_i', \pi_{-i}) = \Psi(\pi_i, \pi_{-i}) - \Psi(\pi_i', \pi_{-i}).

This implies that the payoff differences for any agent are exactly captured by the potential differences. Taking the limit as $\pi_i' \to \pi_i$ along a smooth path, we obtain the gradient equality:

\nabla_{\pi_i} J_i = \nabla_{\pi_i} \Psi,

where the gradient is taken in the appropriate function space (e.g., Fréchet derivative in policy space).

If $\Psi = \lambda W$ for some $\lambda > 0$, then $\nabla_{\pi_i} J_i = \lambda \nabla_{\pi_i} W$, so $\Delta_i = \nabla_{\pi_i} J_i - \lambda \nabla_{\pi_i} W = 0$ for all $i$.

Conversely, suppose there exists an agent $i$ such that $\Delta_i \neq 0$, i.e., $\nabla_{\pi_i} J_i \neq \lambda \nabla_{\pi_i} W$ for any $\lambda$ (or for the specific $\lambda$ under consideration). If the game were an exact potential game with potential proportional to $W$, then there would exist some $\lambda$ such that $\nabla_{\pi_i} J_i = \lambda \nabla_{\pi_i} W$ for all $i$, which contradicts $\Delta_i \neq 0$. Hence no such potential exists. ∎

A.2 Proof of Theorem 4.1 (Equilibrium Preservation under Reward Shaping)

Theorem 4.1. If all agents receive the same shaping term $\gamma \Phi(s') - \Phi(s)$, then for any joint policy $\pi$, the difference $\tilde{J}_i(\pi) - J_i(\pi)$ is independent of $\pi_i$ (given a fixed initial state distribution). Consequently, the set of Nash equilibria of the original game is identical to that of the shaped game.

Detailed Proof.
Define the shaped reward for agent $i$ as

\tilde{R}_i(s,a,s') = R_i(s,a) + \gamma \Phi(s') - \Phi(s).

For any joint policy $\pi$, the expected discounted return under $\tilde{R}_i$ is

\tilde{J}_i(\pi) = \mathbb{E}^\pi\left[ \sum_{t=0}^\infty \gamma^t R_i(S^t, a^t) \right] + \mathbb{E}^\pi\left[ \sum_{t=0}^\infty \gamma^t (\gamma \Phi(S^{t+1}) - \Phi(S^t)) \right].

Consider the second term. Write it as a telescoping sum:

\sum_{t=0}^\infty \gamma^t (\gamma \Phi(S^{t+1}) - \Phi(S^t)) = \lim_{T \to \infty} \sum_{t=0}^T \gamma^t (\gamma \Phi(S^{t+1}) - \Phi(S^t)).

For each finite $T$, the sum telescopes:

\sum_{t=0}^T \gamma^t (\gamma \Phi(S^{t+1}) - \Phi(S^t)) = \gamma^{T+1} \Phi(S^{T+1}) - \Phi(S^0).

Taking the limit as $T \to \infty$, and assuming $\Phi$ is bounded (so $\gamma^{T+1} \Phi(S^{T+1}) \to 0$ a.s.), we obtain

\sum_{t=0}^\infty \gamma^t (\gamma \Phi(S^{t+1}) - \Phi(S^t)) = -\Phi(S^0).

Therefore,

\tilde{J}_i(\pi) = J_i(\pi) - \mathbb{E}^\pi[\Phi(S^0)].

The initial state $S^0$ is drawn from a fixed distribution $\mu_0$ that is independent of the policies. Hence $\mathbb{E}^\pi[\Phi(S^0)] = \mathbb{E}_{S^0 \sim \mu_0}[\Phi(S^0)]$ is a constant $C$ that does not depend on $\pi$. Thus

\tilde{J}_i(\pi) = J_i(\pi) - C.

Now consider any agent $i$ and any two policies $\pi_i, \pi_i'$. Holding $\pi_{-i}$ fixed,

\tilde{J}_i(\pi_i, \pi_{-i}) - \tilde{J}_i(\pi_i', \pi_{-i}) = (J_i(\pi_i, \pi_{-i}) - C) - (J_i(\pi_i', \pi_{-i}) - C) = J_i(\pi_i, \pi_{-i}) - J_i(\pi_i', \pi_{-i}).

Thus the payoff differences are identical, so best responses are unchanged. Consequently, $\pi^*$ is a Nash equilibrium in the original game if and only if it is a Nash equilibrium in the shaped game. ∎

A.3 Proof of Theorem 4.2 (Stability of Constrained Dynamics)

Theorem 4.2. If at each step the policy update satisfies the Lyapunov constraint

\mathbb{E}_{S^{t+1} \sim P(\cdot|S^t, a^t)}[V(S^{t+1})] \le (1-\alpha) V(S^t),

where $V(s) = K - \Phi(s)$ with $K = \sup \Phi$, then $V(S^t)$ converges almost surely to $0$, i.e., $\Phi(S^t) \to K$. Consequently,

\liminf_{T\to\infty} \frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge K.

Detailed Proof.
Let $\mathcal{F}^t$ be the sigma-algebra generated by the history up to time $t$. The constraint gives

\mathbb{E}[V(S^{t+1}) \mid \mathcal{F}^t] \le (1-\alpha) V(S^t).

Since $V \ge 0$, the process $\{V(S^t)\}$ is a nonnegative supermartingale with respect to $\mathcal{F}^t$. By the supermartingale convergence theorem (Doob, 1953), $V(S^t)$ converges almost surely to a finite random variable $V_\infty$. Moreover, taking expectations,

\mathbb{E}[V(S^{t+1})] \le (1-\alpha) \mathbb{E}[V(S^t)] \le (1-\alpha)^{t+1} \mathbb{E}[V(S^0)].

Hence $\mathbb{E}[V(S^t)] \to 0$. By Fatou's lemma,

\mathbb{E}[V_\infty] \le \liminf_{t\to\infty} \mathbb{E}[V(S^t)] = 0,

so $V_\infty = 0$ almost surely. Therefore $\Phi(S^t) = K - V(S^t) \to K$ almost surely.

Now, for the Cesàro mean, fix $\epsilon > 0$. Almost sure convergence implies there exists $T_0$ (random) such that for all $t \ge T_0$, $\Phi(S^t) \ge K - \epsilon$. Then for $T > T_0$,

\frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge \frac{1}{T} \left( \sum_{t=0}^{T_0-1} \Phi(S^t) + (T - T_0 + 1)(K - \epsilon) \right).

Taking $\liminf$ as $T \to \infty$, the first term divided by $T$ vanishes, yielding

\liminf_{T\to\infty} \frac{1}{T} \sum_{t=0}^T \Phi(S^t) \ge K - \epsilon.

Since $\epsilon$ is arbitrary, the liminf is at least $K$. ∎

A.4 Proof of Proposition 4.2 (Bounded Concentration)

Proposition 4.2. In any Nash equilibrium of the modified game with tax $-\mu (R_i/R_{\text{total}})^k$, $k>1$, the Herfindahl index satisfies $H \le H_{\max} < 1$, where $H_{\max}$ depends on $\mu, k$ and the marginal benefits.

Detailed Proof.
Assume that at equilibrium, each agent's resource level $R_i$ satisfies a first-order condition. Let $u_i(R_i, R_{-i})$ be the benefit from resources (excluding the tax). The total reward including tax is

U_i(R_i, R_{-i}) - \mu \left( \frac{R_i}{R_{\text{total}}} \right)^k,

where $R_{\text{total}} = \sum_j R_j$. The marginal benefit of increasing $R_i$ is

\frac{\partial u_i}{\partial R_i} - \mu k \frac{R_i^{k-1}}{R_{\text{total}}^k}.

At a Nash equilibrium, this marginal benefit must be zero (or non-positive if at a boundary). Assuming an interior solution, we have

\frac{\partial u_i}{\partial R_i} = \mu k \frac{R_i^{k-1}}{R_{\text{total}}^k}.

Let $M$ be an upper bound on $\frac{\partial u_i}{\partial R_i}$ (since resources are bounded and rewards are bounded). Then

\mu k \frac{R_i^{k-1}}{R_{\text{total}}^k} \le M \quad \Rightarrow \quad \frac{R_i}{R_{\text{total}}} \le \left( \frac{M R_{\text{total}}^{k-1}}{\mu k} \right)^{1/(k-1)}.

Let $R_{\text{total}}^{\max}$ be the maximum possible total resources (e.g., system capacity). Then the right-hand side is at most

c = \left( \frac{M (R_{\text{total}}^{\max})^{k-1}}{\mu k} \right)^{1/(k-1)}.

By choosing $\mu$ sufficiently large, we can make $c$ arbitrarily small. In particular, choose $\mu$ such that $c < 1/\sqrt{N}$. Then each agent's share is less than $1/\sqrt{N}$, so

H = \sum_{i=1}^N \left( \frac{R_i}{R_{\text{total}}} \right)^2 < \sum_{i=1}^N \frac{1}{N} = 1.

Thus $H$ is bounded away from 1. ∎

A.5 Proof of Proposition 4.3 (Vanishing Asymmetry)

Proposition 4.3. If $\beta > \max_i |\frac{\partial J_i}{\partial \text{deception}}|$, then at any Nash equilibrium, $b_i^t = b_{\text{pub}}^t$ for all $i$, so $\mathcal{I}^t = 0$.

Detailed Proof.
The regularized objective for agent $i$ is

\tilde{J}_i = J_i - \beta \, D_{KL}(b_i \| b_{\text{pub}}).

The KL divergence is convex and minimized when $b_i = b_{\text{pub}}$. Consider a deviation that changes $b_i$ away from $b_{\text{pub}}$. The change in $\tilde{J}_i$ is approximately

\Delta \tilde{J}_i \approx \frac{\partial J_i}{\partial b_i} \cdot \Delta b_i - \beta \frac{\partial D_{KL}}{\partial b_i} \cdot \Delta b_i.

The maximum possible gain from deception is $\max_i \| \frac{\partial J_i}{\partial b_i} \|$ (in an appropriate norm). The penalty term's gradient has magnitude at least some positive constant when $b_i \neq b_{\text{pub}}$ (since KL divergence is strictly convex). In fact, near $b_i = b_{\text{pub}}$, the gradient of KL is zero, but for any fixed deviation, the penalty is positive. To ensure that no deviation is profitable, it suffices that the penalty outweighs any possible gain. More formally, suppose at a Nash equilibrium, $b_i \neq b_{\text{pub}}$. Then agent $i$ could deviate to $b_{\text{pub}}$ and increase $\tilde{J}_i$ by at least $\beta D_{KL}(b_i \| b_{\text{pub}}) > 0$, while the change in $J_i$ is bounded by $G_i \cdot \|b_i - b_{\text{pub}}\|$. If $\beta$ is large enough, the gain from moving to $b_{\text{pub}}$ outweighs any possible loss in $J_i$. Hence $b_i = b_{\text{pub}}$ must hold in equilibrium. ∎

A.6 Proof of Lemma 5.1 (Potential Equilibria are Optimal)

Lemma 5.1. In an exact potential game with potential $\Psi$, every Nash equilibrium $\pi^*$ satisfies $\Psi(\pi^*) = \max_{\pi} \Psi(\pi)$ (if $\Psi$ has a unique maximum) or is a local maximum. Since $\Psi = \mathbb{E}[\sum \gamma^t \Phi]$, this implies that $\Phi$ at equilibrium is maximized in expectation.

Detailed Proof.
Let $\pi^*$ be a Nash equilibrium. For any agent $i$ and any alternative policy $\pi_i'$, we have

J_i(\pi_i^*, \pi_{-i}^*) \ge J_i(\pi_i', \pi_{-i}^*).

By the potential property,

J_i(\pi_i^*, \pi_{-i}^*) - J_i(\pi_i', \pi_{-i}^*) = \Psi(\pi_i^*, \pi_{-i}^*) - \Psi(\pi_i', \pi_{-i}^*).

Thus

\Psi(\pi_i^*, \pi_{-i}^*) - \Psi(\pi_i', \pi_{-i}^*) \ge 0 \quad \text{for all } i, \pi_i'.

This means that no unilateral deviation can increase $\Psi$. Therefore $\pi^*$ is a local maximum of $\Psi$ (or global if the potential is concave and the domain is convex). If $\Psi$ is strictly concave, the maximum is unique and $\Psi(\pi^*) = \max \Psi$. ∎

A.7 Proof of Lemma 5.5 (No Collusion)

Lemma 5.5. If the audit mechanism ensures that for any coalition $C$, the expected penalty $p_{\text{detect}} \cdot P$ exceeds the maximum possible gain from collusion, then any limit point of the dynamics is a strong Nash equilibrium.

Detailed Proof.
Let $\pi^*$ be a limit point of the dynamics under the regulated game. Suppose, for contradiction, that there exists a coalition $C$ and a joint deviation $\tilde{\pi}_C$ such that

\sum_{i \in C} J_i(\tilde{\pi}_C, \pi_{-C}^*) > \sum_{i \in C} J_i(\pi^*).

The audit mechanism detects the deviation with probability $p_{\text{detect}}$ and imposes a penalty $P$ on each member. The expected payoff for member $i$ under deviation is

\mathbb{E}[J_i^{\text{dev}}] = J_i(\tilde{\pi}_C, \pi_{-C}^*) - p_{\text{detect}} P.

By assumption, $p_{\text{detect}} P > \max_{i \in C} \left( J_i(\tilde{\pi}_C, \pi_{-C}^*) - J_i(\pi^*) \right)$, so for each $i$,

J_i(\tilde{\pi}_C, \pi_{-C}^*) - p_{\text{detect}} P < J_i(\pi^*).

Hence the deviation is not profitable for any member, contradicting the assumption that the coalition could improve. Therefore no such deviation exists, and $\pi^*$ is a strong Nash equilibrium. ∎

A.8 Proof of Theorem 5.2 (Instability Criterion)

Theorem 5.2. Local rationality implies global instability if and only if the following hold simultaneously: (1) Non-potential structure, (2) Unstable learning dynamics ($\lambda_{\max}>0$), (3) Persistent asymmetry ($\limsup \mathcal{I}^t > 0$), (4) Unchecked power concentration ($\lim H^t \to 1$), (5) Profitable collusion.

Detailed Proof.
We prove the equivalence in two parts.

($\Rightarrow$) Necessity: Assume the system exhibits emergent incentive instability, i.e., there exists a Nash equilibrium $\pi^*$ such that $\liminf \frac{1}{T}\sum \Phi(S^t) < C_{\text{critical}}$. We show that all five conditions must hold.

· If the game were a potential game with potential $\Psi = \mathbb{E}[\sum \gamma^t \Phi]$, then by Lemma 5.1, $\pi^*$ would maximize $\Phi$ (up to local maxima), contradicting degradation. Hence condition (1) must hold.
· If the learning dynamics had $\lambda_{\max} \le 0$, they would converge to a fixed point (under suitable conditions), and the limit would be a Nash equilibrium with maximal $\Phi$ (if potential). But we have degradation, so dynamics must be unstable; thus $\lambda_{\max} > 0$, giving (2).
· If $\limsup \mathcal{I}^t = 0$, then eventually agents share common knowledge, preventing hidden externalities that could degrade $\Phi$. Since degradation occurs, asymmetry must persist, so (3) holds.
· If $\lim H^t < 1$, concentration is bounded, avoiding monopolistic collapse. Degradation implies unbounded concentration, so $\lim H^t = 1$, giving (4).
· If no profitable collusion exists, then all deviations are unprofitable, so $\pi^*$ would be a strong Nash equilibrium, which typically yields higher $\Phi$ (by potential arguments). Degradation implies collusion must be present, so (5) holds.

($\Leftarrow$) Sufficiency: Suppose all five conditions hold. We construct a scenario that leads to degradation.

· By (1), the game is not a potential game, so there exist Nash equilibria that are suboptimal for $\Phi$. Let $\pi^*$ be such an equilibrium with low $\Phi$.
· By (2), the learning dynamics are chaotic, so the system may not converge to $\pi^*$ but will oscillate, potentially visiting states with very low $\Phi$ repeatedly.
· By (3), persistent asymmetry allows agents to take actions that harm $\Phi$ without detection, so even if $\pi^*$ were not an equilibrium, the dynamics can be trapped in low-$\Phi$ regions.
· By (4), power concentrates, meaning one agent controls most resources. That agent may have incentives to further degrade $\Phi$ (e.g., by extracting resources) without being constrained.
· By (5), coalitions can profitably deviate, leading to outcomes even worse for $\Phi$.

Thus the system will exhibit $\liminf \frac{1}{T}\sum \Phi(S^t) < C_{\text{critical}}$, i.e., global instability. ∎

---

Appendix B: Mathematical Background

B.1 Stochastic Games

Stochastic games generalize Markov decision processes to multiple interacting agents. The theory was introduced by Shapley (1953). Key results include the existence of stationary equilibria in discounted stochastic games under mild conditions (Fink, 1964; Takahashi, 1964). For a comprehensive treatment, see Fudenberg and Tirole (1991).

B.2 Potential Games

Potential games were introduced by Monderer and Shapley (1996). A game is a potential game if there exists a function $\Psi$ such that the incentive of each player to change their strategy is captured by the change in $\Psi$. Potential games have the property that every Nash equilibrium is a local maximum of $\Psi$, and many learning dynamics converge to such equilibria.

B.3 Lyapunov Stability

In dynamical systems, a Lyapunov function is a scalar function that decreases along trajectories, guaranteeing convergence to an equilibrium. The supermartingale convergence theorem (Doob, 1953) provides a stochastic analog, which we use in Theorem 4.2.

B.4 Information Asymmetry and Beliefs

In partially observable stochastic games, agents maintain beliefs about the state. The Kullback-Leibler divergence measures the difference between beliefs. Asymmetric information can lead to signaling and deception, as studied in game theory (Spence, 1973).

B.5 Herfindahl Index and Concentration

The Herfindahl index is a standard measure of market concentration. In industrial organization, it is used to assess monopoly power. In multi-agent systems, it quantifies resource inequality.

B.6 Chaos and Lyapunov Exponents

In dynamical systems, the maximum Lyapunov exponent measures the rate of divergence of nearby trajectories. A positive exponent indicates chaos. In multi-agent learning, chaos can prevent convergence (Balduzzi et al., 2018).

---

Appendix C: Algorithm Details and Complexity

C.1 Lyapunov-Constrained Policy Optimization Algorithm

Input: Initial policies $\pi_i^0$, Lyapunov critic $V_\theta$, step size $\eta$, contraction rate $\alpha$, horizon $T$, initial state distribution $\mu_0$.
Output: Updated policies $\pi^T$.

1. Initialize $t = 0$, observe $S^0 \sim \mu_0$.
2. For $t = 0$ to $T-1$:
   · For each agent $i$, compute policy gradient $g_i = \nabla_{\pi_i} J_i(S^t)$ using any policy gradient method (e.g., REINFORCE, PPO).
   · Form joint gradient $g = (g_1, \dots, g_N)$.
   · Solve the quadratic program:
     \begin{aligned}
     \min_{\delta\pi} &\quad \|\delta\pi - \eta g\|^2 \\
     \text{s.t.} &\quad \mathbb{E}_{a \sim \pi^t + \delta\pi, s' \sim P(\cdot|S^t,a)}[V_\theta(s')] \le (1-\alpha) V_\theta(S^t).
     \end{aligned}
   · If feasible, set $\pi^{t+1} = \pi^t + \delta\pi^*$; otherwise set $\pi^{t+1} = \pi^t$ (or reduce $\eta$).
   · Collect data $(S^t, a^t, S^{t+1})$ and update Lyapunov critic $V_\theta$ by minimizing $(V_\theta(s) - (K - \Phi(s)))^2$.
3. Return $\pi^T$.

C.2 Complexity Analysis

The quadratic program has dimension equal to the total number of policy parameters, $d = \sum_i d_i$. Solving a QP is $O(d^3)$ in general, but if the constraint is linear in $\delta\pi$, it reduces to a linear least squares problem with a linear constraint, solvable in $O(d^2)$ via projection methods. In deep RL, $d$ can be large, so efficient approximations (e.g., using dual gradient descent) are necessary. The sample complexity of estimating the constraint expectation is typical of policy gradient methods.

---

Appendix D: Extended Examples

D.1 Tragedy of the Commons (Detailed)

Consider two agents sharing a renewable resource. State $s \in \{0,1,2\}$ represents resource level. Each agent chooses action $a_i \in \{0,1\}$ (0 = conserve, 1 = harvest). Rewards: $R_i(s,a) = a_i \cdot s$ (harvesting yields resource units). Transition:

· If both conserve: $s' = \min(s+1, 2)$.
· If both harvest: $s' = \max(s-1, 0)$.
· Otherwise: $s' = s$.

Discount factor $\gamma = 0.9$. Global functional $\Phi(s) = s$ (resource level). The game has a Nash equilibrium where both harvest whenever $s>0$, leading to $s$ fluctuating near 0. Applying Layer 1 with shaping $\gamma \Phi(s') - \Phi(s)$ modifies rewards. For example, at $s=2$, if both conserve, shaping term = $0.9\cdot 2 - 2 = -0.2$, slightly reducing reward. But the key is that the equilibrium set remains unchanged (Theorem 4.1). To achieve potential game, we would need to further adjust rewards; but shaping alone does not create potential. However, if we define a potential $\Psi(s) = s$, then we need to design rewards such that $J_i$ differences match $\Psi$ differences. This is not achieved by simple shaping. So the example illustrates that Layer 1 requires more than just shaping; it may require redesigning rewards entirely.

D.2 Flash Crash Model

A simplified model of high-frequency trading: $N$ agents place orders; state includes bid-ask spread $s \in [0,1]$ and order book depth. Each agent chooses to buy, sell, or hold. Rewards depend on price movements. A large sell order can trigger a cascade if many algorithms simultaneously sell. This can be modeled as a stochastic game where the global functional is market stability (e.g., inverse of volatility). Without regulation, the game is non-potential and can exhibit chaotic dynamics. Applying the Stability Stack would involve:

· Layer 1: Shape rewards to align with stability.
· Layer 2: Impose Lyapunov constraints to prevent large price swings.
· Layer 3: Tax high-frequency trading to reduce concentration.

---

Appendix E: Notation and Definitions

· $\mathcal{A}$: set of agents $\{1,\dots,N\}$
· $\mathcal{S}$: state space
· $\mathcal{A}_i$: action space of agent $i$
· $P$: transition kernel $P(s'|s,a)$
· $R_i$: reward function of agent $i$
· $\gamma$: discount factor
· $\pi_i$: policy of agent $i$ (mapping from states to actions)
· $\Pi_i$: set of policies for agent $i$
· $\Pi = \times_i \Pi_i$: joint policy space
· $J_i(\pi)$: expected discounted return for agent $i$ under joint policy $\pi$
· $V_i^\pi(s)$: value function starting from state $s$
· $\Phi(s)$: global stability functional
· $\Delta_i$: incentive divergence operator
· $W$: global welfare $W = \sum_i \alpha_i J_i$
· $b_i^t$: belief of agent $i$ at time $t$
· $\mathcal{I}^t$: information asymmetry measure
· $R_i^t$: resources of agent $i$ at time $t$
· $H^t$: Herfindahl index
· $C \subseteq \mathcal{A}$: coalition
· $\lambda_{\max}$: maximum Lyapunov exponent
· $\Psi$: potential function
· $\delta$: approximation error in $\delta$-potential games
· $\alpha$: contraction rate in Lyapunov condition
· $V(s)$: Lyapunov function $V(s) = K - \Phi(s)$, $K = \sup \Phi$
· $\mu, k$: tax parameters
· $\beta$: information penalty coefficient
· $p_{\text{detect}}, P$: audit probability and penalty

