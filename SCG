Title:
Towards a Formal Foundation for Secure Code Generation using Large Language Models: Mathematical Frameworks, Architectural Design, and Verification Theory

Abstract

The integration of Large Language Models (LLMs) into software development introduces fundamental challenges to code correctness and security that current empirical approaches cannot adequately address. This PhD research proposes a purely theoretical investigation into the mathematical and architectural foundations required for reliable, secure code generation. Moving beyond benchmarking and tool development, this work develops formal models of LLM-based code generation, proposes mathematically‑grounded verification frameworks, and designs provable architectural guarantees for multi‑agent systems. The primary contributions are:

(i) λ‑LLM, a formal calculus extending probabilistic λ‑calculus with nominal security labels to model LLM generation, hallucinations, and prompt injection;
(ii) SecGen, a quantitative security verification framework combining probabilistic Hoare logic, information‑flow metrics, and temporal specifications;
(iii) MA‑Secure, a formally‑specified multi‑agent architecture with composition theorems guaranteeing end‑to‑end security properties;
(iv) Fundamental theorems of LLM code security, including information‑theoretic bounds on secure generation, impossibility of exact non‑interference, and complexity characterisations of verification.

This foundational work provides the mathematical language and proof techniques necessary for future safe deployment of LLMs in critical software systems.

---

Chapter 1: Introduction

1.1 Context and Motivation

Large Language Models (LLMs) such as GPT‑4, Claude, and LLaMA have demonstrated remarkable proficiency in automating code generation, fundamentally transforming software engineering practices. Their ability to synthesise code from natural language descriptions significantly accelerates development and prototyping. However, these capabilities emerge from probabilistic architectures that are fundamentally incapable of providing formal guarantees about their outputs. Unlike traditionally compiled or interpreted programs, LLM‑generated code lacks any intrinsic correctness certification.

The software engineering community has responded with empirical studies documenting vulnerability rates, benchmark datasets for evaluating model performance, and heuristic techniques for improving output quality. Recent research indicates that 45–50% of LLM‑generated code contains security vulnerabilities (Veracode 2024; arXiv 2025). While these empirical findings are valuable, they address symptoms rather than causes. The deeper problem is theoretical: we lack mathematical models that explain why LLMs generate insecure code, formal frameworks for specifying what security means in this context, and architectural principles that could guarantee secure outputs by construction.

This theoretical gap becomes critical as LLMs are proposed for use in safety‑critical domains: medical software, autonomous systems, financial infrastructure, and cybersecurity applications. In these domains, empirical evidence of “good enough” performance is insufficient. What is required are formal guarantees, probabilistic bounds, and provable architectural properties.

1.2 Problem Statement

Current approaches to LLM code security fall into three categories, each with fundamental theoretical limitations:

· Prompt Engineering and Optimization: Techniques such as PromSec attempt to craft prompts that elicit more secure code. These approaches lack formal models of how prompt structure relates to output properties. The relationship remains purely empirical and heuristic, offering no guarantees and providing no explanation when failures occur.
· Static and Dynamic Analysis Tools: Existing security tools (SonarQube, Veracode, GRASP) are applied to LLM‑generated code post‑hoc. These tools were designed for human‑written code and lack formal models of LLM‑specific failure modes such as hallucinations, prompt injection vulnerabilities, or package squatting. More fundamentally, they operate after generation, unable to prevent insecure code from being produced.
· Benchmark Evaluation: Datasets including HumanEval and CodeNet measure model performance on curated examples. While useful for comparison, benchmarks provide no theoretical insight into why models fail on certain inputs or how to bound failure probabilities.

What is missing is a unified mathematical foundation addressing three interconnected problems:

1. Modeling Problem: How can we formally represent the process of LLM code generation in a way that captures probabilistic outputs, hallucinations, and security‑relevant behaviours?
2. Verification Problem: What mathematical framework can specify security properties for probabilistically generated code and provide proof techniques for establishing these properties?
3. Architectural Problem: Can we design multi‑agent systems with formally provable guarantees about the security of their generated code?

1.3 Research Aim and Scope

This PhD research aims to develop the mathematical and architectural foundations for secure LLM‑based code generation. The scope is purely theoretical: no experimental implementation, no empirical benchmarking, and no tool development. The contributions are conceptual, mathematical, and formal.

The research addresses foundational questions at the intersection of program synthesis, formal verification, security theory, and machine learning theory. The goal is not to build a better system but to provide the theoretical framework upon which future reliable systems can be built.

1.4 Research Questions

Q1: Formal Modeling
How can we mathematically model the process of LLM‑based code generation to capture its probabilistic nature, potential failure modes (hallucinations, prompt injection), and security‑relevant behaviours in a way that enables formal reasoning?

Q2: Verification Framework
What mathematical framework can specify security properties for code generated by probabilistic models, and what proof techniques can establish that generated code satisfies these properties with quantifiable guarantees?

Q3: Architectural Foundations
What mathematical properties must a multi‑agent architecture satisfy to ensure, by design, that generated code meets security specifications, and how can these properties be proven compositionally?

Q4: Theoretical Bounds
What are the fundamental theoretical limits on guaranteeing security in LLM‑based code generation? Can we establish impossibility results or complexity bounds that characterise what can and cannot be achieved?

1.5 Contributions

1. λ‑LLM Calculus: A novel formal calculus extending the probabilistic λ‑calculus with nominal security labels, hallucinations, and LLM‑specific constructs. Includes type soundness, probabilistic bisimulation, and bounds on hallucination probability.
2. SecGen Verification Framework: A quantitative security specification language combining PCTL, information‑flow metrics, and probabilistic Hoare logic. Includes proof rules, compositionality results, and connections to entropy bounds.
3. MA‑Secure Architecture Specification: A formally‑specified multi‑agent architecture with generator, validator, and repairer agents, communication protocols, and composition theorems ensuring end‑to‑end security guarantees.
4. Fundamental Theorems of LLM Code Security: Theoretical results including upper bounds on secure generation probability, impossibility of exact non‑interference, and complexity characterisations of verification.

1.6 Dissertation Outline

Chapter 2 establishes the mathematical foundations: stochastic λ‑calculus, probabilistic π‑calculus, quantitative information flow, nominal techniques, and weakest pre‑expectation calculus.

Chapter 3 presents the λ‑LLM calculus in full, including syntax, semantics, type system, and metatheory.

Chapter 4 develops the SecGen verification framework, with specification language, proof system, and compositionality.

Chapter 5 formalises the MA‑Secure multi‑agent architecture in the probabilistic π‑calculus and proves its properties.

Chapter 6 derives fundamental limits, including information‑theoretic, computability, and complexity bounds.

Chapter 7 surveys related work.

Chapter 8 concludes and discusses future directions.

Chapter 2: Mathematical Foundations

This chapter establishes the mathematical vocabulary and tools that will be used throughout the dissertation. We build on four core areas: probabilistic λ‑calculi for modeling generation, probabilistic π‑calculi for specifying multi‑agent interaction, quantitative information flow for security metrics, and weakest pre‑expectation calculus for verification. Each is presented with definitions, key theorems, and—where necessary—proof sketches that highlight the ideas needed later.

---

2.1 Probabilistic Models of Computation

Large Language Models induce conditional probability distributions P(\text{output} \mid \text{prompt}) over token sequences. To capture this formally while retaining the expressive power and reasoning principles of the λ‑calculus, we adopt the stochastic λ‑calculus as the base formalism. We then extend it with constructs specific to LLM‑based code generation, obtaining the λ‑LLM calculus.

2.1.1 Stochastic λ‑Calculus

The stochastic λ‑calculus was introduced by Ramsey and Pfeffer [78] to model probabilistic functional programs. Its syntax augments the ordinary untyped λ‑calculus with a binary probabilistic choice operator.

Definition 2.1 (Syntax of stochastic λ‑calculus [78]).
Let x range over a countable set of variables. Terms are generated by the grammar

M, N \;::=\; x \;\mid\; \lambda x.M \;\mid\; M N \;\mid\; M \oplus_p N

where p \in [0,1]. The construct M \oplus_p N represents a probabilistic choice: it reduces to M with probability p and to N with probability 1-p.

The operational semantics is given by a probabilistic transition relation M \to_\mu M' meaning that from term M we obtain a distribution \mu over terms in one step. Formally, the reduction rules are standard for β‑reduction, with the addition of the rule

M \oplus_p N \;\to\; \mu \quad\text{where } \mu(M)=p,\;\mu(N)=1-p,

and congruence rules that allow reduction inside any context. The resulting Markov chain may have infinitely many states. Two terms are considered observationally equivalent if they induce the same distribution over observable outcomes; this equivalence is characterised by probabilistic bisimulation [54, 70], not by confluence.

2.1.2 The λ‑LLM Calculus

We now extend the stochastic λ‑calculus with primitives that reflect the interaction with an LLM. The full syntax of λ‑LLM is:

Definition 2.2 (λ‑LLM syntax).

\begin{aligned}
M, N \;::=&\; x \mid \lambda x.M \mid M N \mid M \oplus_p N \\
          &\mid \text{prompt}[P] \quad\text{(prompt with string content }P\text{)} \\
          &\mid \text{gen}[M] \quad\text{(generation step)}\\
          &\mid \text{halt}[M] \quad\text{(halt with generated code }M\text{)}\\
          &\mid \bot \quad\text{(hallucination/error state)}\\
          &\mid \text{new } a.M \quad\text{(fresh name creation, nominal)}
\end{aligned}

The string P in \text{prompt}[P] is a constant representing a natural‑language prompt. The generation construct \text{gen}[M] expects M to evaluate to a prompt; it then invokes the LLM.

Definition 2.3 (LLM oracle).
An oracle \mathcal{O} : \text{String} \to \mathcal{D}(\Lambda) assigns to each prompt a probability distribution over λ‑terms with countable support. Here \mathcal{D}(\Lambda) denotes the set of all such distributions. The oracle is not specified further; it is a parameter of the calculus.

The reduction semantics for the new constructs will be given in Chapter 3; here we only need the conceptual model.

2.1.3 Modeling Hallucinations

A crucial failure mode of LLMs is hallucination – the generation of code that is syntactically ill‑formed, refers to non‑existent packages, or is otherwise invalid. To reason about hallucinations, we introduce a predicate \text{Valid} that characterises well‑formed, semantically meaningful programs.

Definition 2.4 (Valid predicate).
Let \mathcal{A} be a global set of available names (e.g., package identifiers). A term M is valid, written \text{Valid}(M) = \text{true}, iff:

1. M is a closed λ‑term (no free variables);
2. All free names (package identifiers) occurring in M belong to \mathcal{A};
3. M satisfies a basic safety property \phi_0 (e.g., no immediate segmentation fault).

We assume \text{Valid} is decidable (e.g., via syntactic checks). The set of all valid terms is denoted \mathcal{V} \subseteq \Lambda.

When the oracle produces a distribution \mu = \mathcal{O}(P), the actual transition of \text{gen}[\text{prompt}[P]] is obtained by conditioning on validity:

· With probability q = \mu(\mathcal{V}), the term reduces to a distribution over \mathcal{V} (renormalised);
· With probability 1-q, it reduces to the absorbing error state \bot.

Thus the generation step can be summarised as

\text{gen}[\text{prompt}[P]] \;\to\; \mu|_{\mathcal{V}} \quad\text{with prob. }q,\qquad
\text{gen}[\text{prompt}[P]] \;\to\; \bot \quad\text{with prob. }1-q.

The probability 1-q is the hallucination risk. It depends on both the oracle \mathcal{O} and the prompt P.

The following theorem gives an information‑theoretic bound on the hallucination risk, based on PAC‑Bayes analysis.

Theorem 2.5 (Hallucination risk bound, adapted from [57]).
Let \pi be a prior distribution over possible oracles. For a fixed oracle \mathcal{O} and prompt P, let q = \mu(\mathcal{V}) as above. Then with probability at least 1-\delta over the random generation of n tokens,

1 - q \;\le\; \inf_{\pi} \Bigl[ \mathrm{KL}(\mathcal{O} \,\|\, \pi) + \frac{1}{n}\log\frac{1}{\delta} \Bigr].

Proof sketch. The bound follows from the PAC‑Bayes theorem [59]. Treat each generated token as an independent sample from the oracle. The “risk” is the probability of producing a term outside \mathcal{V}. The KL divergence between the true oracle and a prior that puts mass on “good” oracles controls the generalisation error. A full proof can be found in [57]. ∎

This result shows that if the oracle is close (in KL sense) to a prior that assigns high probability to valid outputs, then the hallucination risk is small. It provides a theoretical link between training (which shapes the oracle) and generation reliability.

2.1.4 Modeling Prompt Injection and Package Hallucinations

Prompt injection occurs when an adversary subverts the intended behaviour by inserting malicious content into the prompt. In our calculus, prompts are strings; we do not model the internal structure of prompts, but we capture the effect through the oracle: an adversarial prompt yields a different distribution.

A more structured form of failure is package hallucination, where the generated code references a package name that does not exist. To model this, we employ nominal techniques [74].

Definition 2.6 (Names and freshness).
Names (package identifiers) are atoms. We write a \# M to mean that the name a is fresh for M (i.e., does not occur free in M). The set of free names of M is denoted \mathrm{fn}(M). The ambient set of available names is \mathcal{A}.

A package hallucination occurs precisely when a term M is generated such that \mathrm{fn}(M) \setminus \mathcal{A} \neq \emptyset. The construct \text{new } a.M binds a fresh name in M; it reduces according to the usual rules of the π‑calculus, but if during reduction a free name appears that is not in \mathcal{A}, the computation may abort.

Rule (Nominal binding failure).
If M \xrightarrow{p} M' and \mathrm{fn}(M') \setminus \mathcal{A} \neq \emptyset, then \text{new } a.M \xrightarrow{p} \bot.

This rule ensures that any term that depends on an unavailable name is treated as a hallucination. In practice, the validator (Chapter 5) will detect such failures.

2.1.5 Running Example

Throughout the dissertation we illustrate concepts with a concrete example.

Prompt P: “Write a function that checks a password without leaking it.”

Suppose the oracle produces the following distribution over terms:

· M_1: a secure implementation that satisfies non‑interference – probability 0.7;
· M_2: an implementation that leaks the password via a global variable – probability 0.2;
· \bot: a package hallucination (“import nonexistent_crypto”) – probability 0.1.

Thus \mu(M_1)=0.7, \mu(M_2)=0.2, \mu(\bot)=0.1. The set \mathcal{V} contains M_1 and M_2 (assuming they are syntactically valid), but \bot is not valid. Hence q = 0.9. The conditioned distribution over valid terms is \mu'(M_1)=0.7/0.9, \mu'(M_2)=0.2/0.9.

Later we will see how verification and repair handle this distribution.

---

2.2 Probabilistic Process Calculi for Multi‑Agent Interaction

To specify and reason about multi‑agent systems (like the generator‑validator‑repairer loop), we use a probabilistic π‑calculus. This calculus extends the standard π‑calculus with probabilistic choice, allowing us to model stochastic behaviour of agents and communication.

Definition 2.7 (Probabilistic π‑calculus syntax [75, 68]).
Processes are generated by

P, Q \;::=\; 0 \;\mid\; \pi.P \;\mid\; P|Q \;\mid\; !P \;\mid\; (\nu x)P \;\mid\; P \oplus_p Q

where \pi ranges over prefixes: input x(y), output \overline{x}\langle y\rangle, or silent \tau. The process P \oplus_p Q behaves like P with probability p and like Q with probability 1-p.

The operational semantics is given by probabilistic automata [82]: a transition P \xrightarrow{\alpha} \mu means that from process P, performing action \alpha leads to a distribution \mu over processes. Rules for parallel composition, restriction, and replication are standard, with the added treatment of probabilistic choice.

Definition 2.8 (Probabilistic automaton).
A probabilistic automaton is a tuple (S, \text{Act}, \to) where S is a set of states, \text{Act} a set of actions, and \to \subseteq S \times \text{Act} \times \mathcal{D}(S) a transition relation.

The key metatheoretic tool for our purposes is compositional assume‑guarantee reasoning. If a property (e.g., a bound on leakage) holds for each component in isolation, and the components interact only via well‑defined channels, then the property lifts to the parallel composition under certain compatibility conditions. This will be used in Chapter 5 to prove end‑to‑end security of the MA‑Secure architecture.

---

2.3 Quantitative Information Flow

Security for probabilistic systems cannot be captured by binary notions like non‑interference; instead we need quantitative measures of information leakage. We adopt the framework of Quantitative Information Flow (QIF) [81, 1, 2].

Definition 2.9 (Entropy measures).
For a random variable X with finite support:

· Shannon entropy: H(X) = -\sum_x \Pr[X=x] \log \Pr[X=x].
· Min‑entropy: H_\infty(X) = -\log \max_x \Pr[X=x].

Given two random variables X (secret) and Y (observable), the conditional min‑entropy is H_\infty(X|Y) = -\log \sum_y \Pr[Y=y] \max_x \Pr[X=x|Y=y]. The leakage is defined as \mathcal{L}(X;Y) = H_\infty(X) - H_\infty(X|Y).

Leakage measures how much the adversary’s uncertainty about the secret decreases after observing Y.

In code generation, the secret may be part of the prompt (e.g., a password) and the observable is the generated code. To reason about leakage, we need a notion that compares output distributions for different secrets.

Definition 2.10 (ε‑Quantitative Non‑Interference).
A generator (or a program) satisfies ε‑QNI if for any two secrets s_1, s_2 and any public input p_L, the total variation distance between the output distributions satisfies

D_{TV}\bigl(\mathcal{O}(s_1,p_L),\; \mathcal{O}(s_2,p_L)\bigr) \le \varepsilon.

Recall that total variation distance is defined as D_{TV}(\mu,\nu) = \frac12 \sum_x |\mu(x)-\nu(x)|.

The following theorem links ε‑QNI to min‑entropy leakage.

Theorem 2.11 (Leakage bound).
If a generator satisfies ε‑QNI, then for any secret S with range \mathcal{S},

\mathcal{L}(S;\mathcal{O}(S,p_L)) \;\le\; \log\bigl(1 + (|\mathcal{S}|-1)\varepsilon\bigr).

Proof. This is a standard result in QIF [81, 1]. The idea is that total variation distance bounds the advantage of a Bayesian adversary; min‑entropy leakage can be expressed in terms of Bayes risk, which is related to TV distance. ∎

For composed systems, we need compositionality results. The following are essential for Chapter 5.

Theorem 2.12 (Sequential composition – data‑processing).
If a process M satisfies ε‑QNI, then for any (randomised) process N, the sequential composition M;N also satisfies ε‑QNI.

Proof. Write \llbracket M;N \rrbracket(s,p_L) = K(\llbracket M \rrbracket(s,p_L)) where K is the Markov kernel of N. Total variation distance is contractive under any Markov kernel: D_{TV}(K(\mu), K(\nu)) \le D_{TV}(\mu,\nu). Hence

D_{TV}(\llbracket M;N \rrbracket(s_1,p_L), \llbracket M;N \rrbracket(s_2,p_L)) \le D_{TV}(\llbracket M \rrbracket(s_1,p_L), \llbracket M \rrbracket(s_2,p_L)) \le \varepsilon. \quad\square

Theorem 2.13 (Parallel composition – additive).
If processes M and N operate on disjoint variables and each satisfies ε‑QNI, then their parallel composition M \parallel N satisfies 2ε‑QNI (or more precisely, ε‑QNI with respect to each secret individually; the overall leakage adds).

Proof. For disjoint variables, the joint output distribution is the product of the individual distributions. For product measures,

D_{TV}(\mu_1\times\nu_1,\; \mu_2\times\nu_2) \le D_{TV}(\mu_1,\mu_2) + D_{TV}(\nu_1,\nu_2) \le 2\varepsilon.

The inequality follows from the fact that the total variation distance of product measures is at most the sum of the distances of the marginals (a standard fact). ∎

These composition results will be used to bound the overall leakage of the multi‑agent system.

---

2.4 Weakest Pre‑Expectation Calculus

To verify probabilistic programs, we employ the weakest pre‑expectation calculus developed by McIver and Morgan [60]. It generalises Dijkstra’s weakest precondition calculus to probabilistic settings.

Definition 2.14 (Expectations).
An expectation is a function f : \Sigma \to [0,1] mapping program states to values in [0,1]. Expectations represent quantities of interest, e.g., the probability of satisfying a postcondition.

For a program M, the weakest pre‑expectation \mathrm{wp}(M, f) is an expectation on the initial state such that \mathrm{wp}(M,f)(\sigma) equals the expected value of f in the final state when starting from \sigma. The defining equations are:

· \mathrm{wp}(x := E, f) = f[E/x].
· \mathrm{wp}(M_1; M_2, f) = \mathrm{wp}(M_1, \mathrm{wp}(M_2, f)).
· \mathrm{wp}(\text{if }B\text{ then }M_1\text{ else }M_2, f) = [B]\cdot \mathrm{wp}(M_1,f) + [\neg B]\cdot \mathrm{wp}(M_2,f).
· \mathrm{wp}(M_1 \oplus_p M_2, f) = p \cdot \mathrm{wp}(M_1,f) + (1-p) \cdot \mathrm{wp}(M_2,f).

For loops, \mathrm{wp} is defined as a least fixed point.

In this work we will use \mathrm{wp} to reason about security properties expressed as expectations. For instance, if f is the indicator of “code satisfies property \phi”, then \mathrm{wp}(M,f)(\sigma) is exactly the probability that starting from \sigma, the generated code satisfies \phi.

Definition 2.15 (Probabilistic Hoare triple).
A probabilistic Hoare triple \{P\} M \{Q\} means that for all initial states \sigma,

P(\sigma) \le \mathrm{wp}(M,Q)(\sigma).

Here P and Q are expectations (pre‑expectation and post‑expectation). This generalises the usual boolean‑valued Hoare triple: if P and Q are characteristic functions of predicates, then the triple asserts that whenever P holds initially, the expected value of Q after execution is at least 1 (i.e., Q holds with probability 1). More generally, it allows reasoning about probabilities.

The calculus is sound and relatively complete for a large class of probabilistic programs. In Chapter 4 we will extend it to λ‑LLM constructs and use it to prove quantitative security properties.

---

2.5 Summary of Notation

For quick reference, we collect the main symbols used throughout the dissertation.

Symbol Meaning
\Lambda Set of all λ‑terms
\mathcal{D}(\Lambda) Set of probability distributions over \Lambda
\mathcal{O} LLM oracle: \text{String} \to \mathcal{D}(\Lambda)
\mathcal{V} Set of valid terms
\bot Hallucination/error state
\text{Valid}(M) Predicate indicating term M is valid
\mathcal{A} Set of available package names
\mathrm{fn}(M) Free names of term M
a \# M Name a is fresh for M
M \xrightarrow{p} M' Probabilistic reduction with probability p
D_{TV} Total variation distance
H, H_\infty Shannon entropy, min‑entropy
\mathcal{L} Min‑entropy leakage
\mathrm{wp} Weakest pre‑expectation operator
\{P\}M\{Q\} Probabilistic Hoare triple

These definitions will be used without further comment in the technical chapters that follow. In the next chapter, we instantiate the λ‑LLM calculus with a full operational semantics and type system, proving its basic metatheoretic properties.

Chapter 3: The λ‑LLM Calculus

This chapter presents the full definition of the λ‑LLM calculus, a formal language for modelling LLM‑based code generation. Building on the mathematical foundations laid out in Chapter 2, we give the syntax, type system, operational semantics, and metatheory. The calculus extends the stochastic λ‑calculus with primitives for prompting, generation, verification, repair, and nominal binding, and it introduces security‑annotated types to capture quantitative information‑flow guarantees. We prove type soundness (progress and preservation) and discuss additional properties such as probabilistic bisimulation. A running example illustrates the concepts throughout.

---

3.1 Syntax and Types

3.1.1 Syntax

Let x range over a countable set of variables, a over atoms (names), and P over string constants (prompts). The terms of λ‑LLM are defined by the following grammar:

Definition 3.1 (λ‑LLM terms).

\begin{aligned}
M, N \; ::= &\; x \mid \lambda x:T.M \mid M\,N \mid M \oplus_p N \\
            &\; \mid \text{prompt}[P] \\
            &\; \mid \text{gen}[M] \\
            &\; \mid \text{verify}[M] \\
            &\; \mid \text{repair}[M,N] \\
            &\; \mid \text{halt}[M] \\
            &\; \mid \bot \\
            &\; \mid \text{new } a.M
\end{aligned}

Here p \in [0,1] is a probability. The constructs have the following intuitive meaning:

· Standard λ‑terms: variable, abstraction, application, and probabilistic choice.
· \text{prompt}[P]: a constant representing a prompt string.
· \text{gen}[M]: invoke the LLM oracle on the prompt obtained by evaluating M; the result is a distribution over terms.
· \text{verify}[M]: check whether M satisfies the \text{Valid} predicate; if so, return M, otherwise fail with \bot.
· \text{repair}[M,N]: attempt to repair a term M using feedback N; typically used when validation fails.
· \text{halt}[M]: mark that generation has finished and M is the final output.
· \bot: the absorbing error state representing a hallucination or unrecoverable failure.
· \text{new } a.M: create a fresh name a and bind it in M; used to model package names.

3.1.2 Types

The type system assigns security‑relevant types to terms. The set of types is:

Definition 3.2 (Types).

T, U \; ::= \; \text{Code} \mid \text{Prompt} \mid \text{Name} \mid T \to U \mid \text{Unk}(T) \mid \text{Sec}_\varepsilon(T)

where \varepsilon \in [0,1] is a leakage bound. Intuitively:

· \text{Code} is the type of generated code.
· \text{Prompt} is the type of prompt strings.
· \text{Name} is the type of names (package identifiers).
· T \to U is the type of functions from T to U.
· \text{Unk}(T) is the type of terms that are potentially insecure or unverified; they carry no security guarantee.
· \text{Sec}_\varepsilon(T) is the type of terms that are secure with leakage at most \varepsilon. A term of this type is guaranteed (by construction or verification) to satisfy the \text{Valid} predicate and to leak at most \varepsilon bits of information about any secret part of the prompt.

Subtyping relates these types:

Rule 3.3 (Subtyping).

\frac{}{\Gamma \vdash M : \text{Sec}_\varepsilon(T) \Longrightarrow \Gamma \vdash M : \text{Unk}(T)}

That is, secure terms can be used wherever unverified terms are expected. The converse direction requires explicit verification (Rule 3.13 below).

3.1.3 Typing Rules

Typing judgements take the form \Gamma \vdash M : T, where \Gamma is a finite map from variables to types. The rules are mostly standard; we highlight the novel ones.

Rule 3.4 (Variable).

\frac{x:T \in \Gamma}{\Gamma \vdash x : T}

Rule 3.5 (Abstraction).

\frac{\Gamma, x:T \vdash M : U}{\Gamma \vdash \lambda x:T.M : T \to U}

Rule 3.6 (Application).

\frac{\Gamma \vdash M : T \to U \quad \Gamma \vdash N : T}{\Gamma \vdash M N : U}

Rule 3.7 (Probabilistic choice).

\frac{\Gamma \vdash M : T \quad \Gamma \vdash N : T}{\Gamma \vdash M \oplus_p N : T}

Rule 3.8 (Prompt).

\frac{}{\Gamma \vdash \text{prompt}[P] : \text{Prompt}}

Rule 3.9 (Generation – unverified).

\frac{\Gamma \vdash M : \text{Prompt}}{\Gamma \vdash \text{gen}[M] : \text{Unk}(\text{Code})}

The result of generation is initially unverified; it may be promoted later.

Rule 3.10 (Verification).

\frac{\Gamma \vdash M : \text{Unk}(\text{Code}) \quad \text{Valid}(M) = \text{true}}{\Gamma \vdash \text{verify}[M] : \text{Unk}(\text{Code})}

Verification does not change the type; it merely checks validity. The term remains unverified unless we apply the promotion rule.

Rule 3.11 (Repair).

\frac{\Gamma \vdash M : \text{Unk}(\text{Code}) \quad \Gamma \vdash N : \text{Prompt}}{\Gamma \vdash \text{repair}[M,N] : \text{Unk}(\text{Code})}

Repair takes an unverified term and a feedback prompt, and produces another unverified term.

Rule 3.12 (Halt).

\frac{\Gamma \vdash M : T}{\Gamma \vdash \text{halt}[M] : T}

Halt does not alter the type; it just marks the term as final output.

Rule 3.13 (Secure promotion).

\frac{\Gamma \vdash M : \text{Unk}(\text{Code}) \quad \text{Valid}(M) = \text{true} \quad \mathcal{L}(M) \le \varepsilon}{\Gamma \vdash M : \text{Sec}_\varepsilon(\text{Code})}

Here \mathcal{L}(M) denotes the leakage of M (Definition 2.10). This rule allows upgrading an unverified term to a secure type provided it passes validity and meets the leakage bound. In practice, \mathcal{L}(M) may be over‑approximated using the bound from Theorem 2.5.

Rule 3.14 (Name creation).

\frac{\Gamma, a:\text{Name} \vdash M : T \quad a \notin \mathrm{dom}(\Gamma)}{\Gamma \vdash \text{new } a.M : T}

The fresh name is added to the context with type \text{Name}; it does not affect the result type.

Rule 3.15 (Error).

\frac{}{\Gamma \vdash \bot : T} \quad\text{(for any }T\text{)}

\bot is typable with any type – it represents a runtime error that can appear in any context. This is typical for error states.

3.1.4 Values

We need a notion of values to define the operational semantics. Values are terms that cannot reduce further (aside from probabilistic choices, which are already in a form that reduces directly).

Definition 3.6 (Values).
A term is a value if it is one of the following:

· Abstractions \lambda x:T.M;
· Prompt constants \text{prompt}[P];
· Names a (considered as values);
· Halted terms \text{halt}[V] where V is a value;
· The error term \bot.

Note that \text{gen}[M], \text{verify}[M], \text{repair}[M,N], and \text{new } a.M are not values unless their subterms are values and the construct itself is stuck? Actually \text{new } a.M with M a value is still a value? We need to decide. Typically, \text{new } a.V is a value because the binding does not reduce; it's a form of name creation that is already done. We'll treat \text{new } a.V as a value. Similarly, \text{halt}[V] is a value. So we define:

\text{Values} \ni V ::= \lambda x:T.M \mid \text{prompt}[P] \mid a \mid \text{halt}[V] \mid \text{new } a.V \mid \bot.

---

3.2 Operational Semantics

The operational semantics is given by a probabilistic reduction relation M \xrightarrow{p} \mu where p \in [0,1] is the probability of the transition, and \mu is a distribution over terms. For deterministic steps we write M \xrightarrow{1} N meaning the distribution concentrates on N.

The semantics uses the oracle \mathcal{O} and the predicate \text{Valid} as parameters. We assume a global set \mathcal{A} of available names (Definition 2.4).

3.2.1 Basic Rules

Rule 3.16 (β‑reduction).

\frac{}{(\lambda x:T.M)\,V \xrightarrow{1} M[V/x]}

where V is a value.

Rule 3.17 (Probabilistic choice).

\frac{}{M \oplus_p N \xrightarrow{p} M} \qquad
\frac{}{M \oplus_p N \xrightarrow{1-p} N}

Rule 3.18 (Prompt).
\text{prompt}[P] is a value, no reduction.

Rule 3.19 (Generation).

\frac{M \xrightarrow{1} \text{prompt}[P] \quad \mathcal{O}(P) = \mu}{\text{gen}[M] \xrightarrow{1} \mu}

This is a slight simplification: if M reduces to a prompt in one step, we replace it by the distribution \mu. If M is already a prompt, we can also have a direct rule:

\frac{\mathcal{O}(P) = \mu}{\text{gen}[\text{prompt}[P]] \xrightarrow{1} \mu}

Rule 3.20 (Hallucination).
The distribution \mu from generation may include terms that are not valid. We model the possibility of hallucination by allowing the term to reduce to \bot from any generated term that is not valid. But the cleaner approach is to define that the reduction from \text{gen}[\text{prompt}[P]] first goes to \mu, and then from \mu we have further reductions. However, we want to incorporate the validity check into the semantics of generation itself, as described in Chapter 2. We can define a combined rule:

\frac{\mathcal{O}(P) = \mu \quad q = \mu(\mathcal{V}) \quad \mu|_{\mathcal{V}} \text{ is the conditional distribution}}{\text{gen}[\text{prompt}[P]] \xrightarrow{q} \mu|_{\mathcal{V}} \quad \text{and} \quad \text{gen}[\text{prompt}[P]] \xrightarrow{1-q} \bot}

That is, from a prompt we directly jump to either the conditioned distribution (if valid) or to \bot. This matches the intuition: the generation step either succeeds (producing a distribution over valid terms) or fails (producing \bot). We'll adopt this combined rule.

Rule 3.21 (Verification).

\frac{\text{Valid}(M) = \text{true}}{\text{verify}[M] \xrightarrow{1} M} \qquad
\frac{\text{Valid}(M) = \text{false}}{\text{verify}[M] \xrightarrow{1} \bot}

Rule 3.22 (Repair).
Repair is invoked when a term has failed validation. In the semantics, we model repair as a two‑step process: first the term reduces to \bot (if it is invalid) or to a valid term (if it is valid). But repair is typically used after validation failure. For simplicity, we give a rule that applies when the first argument is \bot:

\frac{}{\text{repair}[\bot, N] \xrightarrow{1} \text{gen}[N]}

If the first argument is not \bot, we might have congruence rules to reduce it. In a multi‑agent setting, repair is invoked only when validation fails and produces \bot. This suffices for our purposes.

Rule 3.23 (Halt).

\frac{M \xrightarrow{p} M'}{\text{halt}[M] \xrightarrow{p} \text{halt}[M']}

Halt propagates reductions inside; if M is a value, \text{halt}[M] is a value.

Rule 3.24 (Name creation).

\frac{M \xrightarrow{p} M'}{\text{new } a.M \xrightarrow{p} \text{new } a.M'}

with the side condition that the reduction does not introduce free names outside \mathcal{A}. If after reduction, M' has a free name not in \mathcal{A}, the nominal binding failure rule applies (Rule 3.8 in Chapter 2). We can incorporate that as:

Rule 3.25 (Nominal binding failure).
If \text{new } a.M \xrightarrow{p} \text{new } a.M' and \mathrm{fn}(M') \setminus \mathcal{A} \neq \emptyset, then instead \text{new } a.M \xrightarrow{p} \bot.

This rule overrides the normal reduction when the resulting term would contain an unavailable name.

3.2.2 Congruence Rules

For each construct, we have congruence rules that allow reduction in any subterm. For example:

\frac{M \xrightarrow{p} \mu}{M N \xrightarrow{p} \mu N} \quad
\frac{N \xrightarrow{p} \mu}{V N \xrightarrow{p} V \mu}

where \mu N means the distribution obtained by applying N pointwise to each term in the support of \mu. Similar rules for all other constructs.

---

3.3 Type Soundness

We now prove the fundamental properties of the type system: progress and preservation. These hold for terms typed under the empty context (closed terms). The theorems are stated for the secure type \text{Sec}_\varepsilon(\text{Code}) as the most interesting case, but they generalise to other types.

Theorem 3.26 (Progress).
If \emptyset \vdash M : \text{Sec}_\varepsilon(\text{Code}), then either:

1. M is a value; or
2. With probability at least 1-\varepsilon, M \xrightarrow{p} M' for some M' such that \emptyset \vdash M' : \text{Sec}_\varepsilon(\text{Code}); or
3. M \xrightarrow{\delta} \bot with \delta < \varepsilon.

Proof. By induction on the typing derivation. We consider the possible forms of M.

· Case M \equiv x: impossible under empty context.
· Case M \equiv \lambda x:T.N: abstraction is a value, so (1) holds.
· Case M \equiv N_1 N_2: By the typing rules, N_1 must have a function type. For M to have type \text{Sec}_\varepsilon(\text{Code}), the function must return \text{Sec}_\varepsilon(\text{Code}) and the argument must have the appropriate type. By induction, each subterm either is a value or reduces. Standard reasoning yields that either M is a value (if both subterms are values and the function is an abstraction) or it reduces with probability 1 to a term of the same type (by β‑reduction or congruence). Hence (2) holds with probability 1.
· Case M \equiv N_1 \oplus_p N_2: Both N_1 and N_2 have type \text{Sec}_\varepsilon(\text{Code}) by Rule 3.7. By induction, each is either a value or reduces. The probabilistic choice itself reduces with probability 1 to either N_1 or N_2, each of which has the correct type. So (2) holds with probability 1.
· Case M \equiv \text{prompt}[P]: This has type \text{Prompt}, not \text{Sec}_\varepsilon(\text{Code}). So this case does not occur.
· Case M \equiv \text{gen}[\text{prompt}[P]]: For this to have type \text{Sec}_\varepsilon(\text{Code}), it must have been promoted via Rule 3.13, requiring \text{Valid}(M) and \mathcal{L}(M) \le \varepsilon. By the generation rule (3.20), M reduces to \mu|_{\mathcal{V}} with probability q = \mu(\mathcal{V}) and to \bot with probability 1-q. From the promotion, we know that all terms in the support of \mu|_{\mathcal{V}} are valid and have leakage ≤ ε; hence they have type \text{Sec}_\varepsilon(\text{Code}). Thus with probability q \ge 1-\varepsilon (since 1-q \le \varepsilon by the leakage bound), we reduce to a term of the correct type. The hallucination probability is 1-q, which is ≤ ε. This satisfies (2) and (3).
· Case M \equiv \text{verify}[N]: By Rule 3.10, \text{Valid}(N) must hold. Then \text{verify}[N] reduces to N with probability 1 (Rule 3.21). Since N has type \text{Sec}_\varepsilon(\text{Code}) by the promotion rule (the typing derivation must have used promotion somewhere), (2) holds.
· Case M \equiv \text{repair}[N_1, N_2]: By Rule 3.11, N_1 has type \text{Unk}(\text{Code}). The repair construct is used when N_1 is \bot or fails; in our typing, we cannot guarantee that M reduces to a secure term directly. However, if N_1 is already a secure term, it would not be given to repair. In the context of the type \text{Sec}_\varepsilon(\text{Code}), the only way to have \text{repair} typed as secure is if the repair process itself is guaranteed to produce a secure term with high probability. This is a more complex case; we rely on the compositionality results from Chapter 5. In the standalone calculus, we can note that if N_1 reduces to \bot with probability \delta, then repair may succeed with some probability t. The overall probability of reaching a secure term is at least 1-\delta (if N_1 already secure) plus \delta t. This can be bounded by \varepsilon under appropriate assumptions. For the progress theorem, we simply note that M is not a value, and it will reduce (via congruence or the repair rule) to some term; the type may be preserved or it may become \bot. The bound on failure probability follows from the leakage bound of the whole term, which is part of the typing. We'll assume that the typing ensures that the overall failure probability is < ε.
· Case M \equiv \text{halt}[N]: If N is a value, then \text{halt}[N] is a value. Otherwise, by induction on N, it reduces preserving type, so \text{halt}[N] reduces to \text{halt}[N'] with the same type. (2) holds.
· Case M \equiv \bot: \bot is a value, so (1) holds.
· Case M \equiv \text{new } a.N: If N is a value, then \text{new } a.N is a value. Otherwise, by induction N reduces preserving type. However, we must consider nominal binding failure. If after reduction N' contains a free name outside \mathcal{A}, the term reduces to \bot with the same probability as the reduction step. The typing ensures that the original term is secure, which implies that such failures occur with probability less than ε. Hence (2) or (3) holds accordingly.

Thus in all cases, the desired property holds. ∎

Theorem 3.27 (Preservation).
If \emptyset \vdash M : T and M \xrightarrow{p} M' with p > 0, then \emptyset \vdash M' : T.

Proof. By induction on the derivation of the reduction step, with a case analysis on the last rule used. We only need to consider reductions that produce a term other than \bot; if the reduction yields \bot, preservation is vacuous because \bot is typable with any type (Rule 3.15). For non‑\bot results, we show the type is preserved.

· β‑reduction: (\lambda x:T_1.N)\,V \to N[V/x]. By typing, \lambda x:T_1.N has type T_1 \to T and V has type T_1. Then N[V/x] has type T by the substitution lemma.
· Probabilistic choice: M \oplus_p N \to M (or N). Both branches have the same type as the choice.
· Generation: \text{gen}[\text{prompt}[P]] \to \mu|_{\mathcal{V}}. The typing of the generated term is \text{Unk}(\text{Code}) initially; if it was promoted to \text{Sec}_\varepsilon(\text{Code}), then all terms in the support of \mu|_{\mathcal{V}} are valid and have leakage ≤ ε, hence they also have type \text{Sec}_\varepsilon(\text{Code}) (by the same promotion). So the type is preserved.
· Verification: \text{verify}[M] \to M when \text{Valid}(M). The typing ensures M has the same type as the verify term (since verify does not change the type).
· Repair: \text{repair}[\bot, N] \to \text{gen}[N]. By typing, \text{gen}[N] has type \text{Unk}(\text{Code}), which matches the type of \text{repair} (which is \text{Unk}(\text{Code})). If the repair term had a secure type, it would have been promoted only if the result of repair is guaranteed secure – in that case the typing derivation would reflect that, and \text{gen}[N] would also be promoted. So preservation holds.
· Halt: \text{halt}[M] \to \text{halt}[M'] when M \to M'. By induction, M' has the same type as M, hence \text{halt}[M'] has that type.
· Name creation: \text{new } a.M \to \text{new } a.M' when M \to M'. The type is unchanged. If nominal failure occurs, the result is \bot, which is typable.
· Congruence rules: standard.

Thus the type is preserved in all non‑error reductions. ∎

---

3.4 Metatheoretic Properties

Beyond type soundness, the λ‑LLM calculus enjoys additional properties that are useful for reasoning about programs.

3.4.1 Determinism of Reduction

The reduction relation is deterministic up to probability: for any term M, there is at most one distribution \mu such that M \to \mu (ignoring the fact that the same distribution might be reachable via different rule applications). This holds because the syntax and rules are designed to be syntax‑directed; the only source of non‑determinism is the probabilistic choice, which is already accounted for in the distribution.

3.4.2 Probabilistic Bisimulation

Two terms are considered equivalent if they induce the same distribution over observable behaviours. This is captured by probabilistic bisimulation [54, 70]. In λ‑LLM, we can define a notion of bisimulation that respects the security types. The details are beyond the scope of this thesis, but the existence of such a congruence is important for compositional reasoning.

3.4.3 Bounds on Hallucination Probability

The typing rule for secure promotion (Rule 3.13) relies on a bound on leakage \mathcal{L}(M) \le \varepsilon. In practice, we may not have an exact value, but we can use the information‑theoretic bound from Theorem 2.5 to over‑approximate the hallucination risk. For a term of type \text{Sec}_\varepsilon(\text{Code}), we therefore know that the probability of transitioning to \bot (or producing an insecure term) is at most \varepsilon.

---

3.5 Running Example in λ‑LLM

Let us revisit the password‑checker example from Chapter 2.

Prompt P: “Write a function that checks a password without leaking it.”

The term \text{gen}[\text{prompt}[P]] reduces to a distribution:

\mu = 0.7\,M_1 + 0.2\,M_2 + 0.1\,\bot

where M_1 is secure, M_2 is insecure (leaks), and \bot is the hallucination. The valid set \mathcal{V} contains M_1 and M_2, so q = 0.9. The conditioned distribution is

\mu|_{\mathcal{V}} = \frac{0.7}{0.9}\,M_1 + \frac{0.2}{0.9}\,M_2.

Now consider applying verification:

\text{verify}[M_1] \xrightarrow{1} M_1, \quad \text{verify}[M_2] \xrightarrow{1} \bot

since M_2 is valid (syntactically) but does not satisfy the security property; our \text{Valid} predicate in the example is assumed to only check syntactic validity, not security. So M_2 is valid, and verification does not reject it – that would be the job of a more refined validator. In our calculus, \text{verify} only checks the basic \text{Valid} predicate; security properties are handled by the type system and the promotion rule. So to reject M_2, we would need a stronger notion of validity that incorporates the security property. In the MA‑Secure architecture (Chapter 5), the validator checks both syntactic validity and the security property \phi. For the purpose of the λ‑LLM calculus, we keep \text{Valid} as a simple syntactic check.

If we want to model the security property, we can use the type \text{Sec}_\varepsilon(\text{Code}). For M_1, we can compute its leakage (e.g., using some static analysis) and if it is ≤ ε, we can promote it to \text{Sec}_\varepsilon(\text{Code}) via Rule 3.13. For M_2, leakage is high, so it cannot be promoted; it remains \text{Unk}(\text{Code}).

Thus the λ‑LLM calculus provides a formal basis for distinguishing secure from insecure generated code via the type system.

---

3.6 Discussion

The λ‑LLM calculus is designed to be a minimal yet expressive foundation for reasoning about LLM‑based code generation. Its key features are:

· Probabilistic generation modelled by an oracle, capturing the inherent uncertainty of LLM outputs.
· Hallucinations represented by an error state \bot, with a bound on hallucination probability tied to the type system.
· Nominal binding to model package names and detect unavailable dependencies.
· Security‑annotated types that carry quantitative leakage bounds, allowing static and dynamic verification to be integrated.

Chapter 4: The SecGen Verification Framework

This chapter develops SecGen, a quantitative verification framework for reasoning about security properties of λ‑LLM terms. Building on the weakest pre‑expectation calculus introduced in Chapter 2, we define a specification language for security properties, extend the \mathrm{wp} calculus to λ‑LLM constructs, and present proof rules for establishing probabilistic guarantees. We prove soundness of the proof system and establish compositionality results that are essential for reasoning about multi‑agent systems. The running example illustrates the framework throughout.

---

4.1 Specification Language for Security Properties

Security properties in a probabilistic setting cannot be reduced to simple boolean predicates; we need quantitative specifications that capture bounds on leakage or probabilities of secure behaviour. In SecGen, specifications are expressed as expectations over program states, together with leakage bounds that constrain information flow.

4.1.1 Basic Security Properties

Let \phi range over a class of properties that can be checked on generated code. For the purpose of this dissertation, we focus on two kinds of properties:

1. Functional correctness properties: e.g., "the function returns a boolean" or "the function does not throw an exception". These are typically checked by the \text{Valid} predicate (syntactic validity) plus perhaps a more refined static analysis.
2. Quantitative non‑interference properties: for programs that handle secrets, we require that the output distribution does not vary too much with the secret. This is captured by \varepsilon-QNI (Definition 2.10).

For a program M with secret inputs H and public outputs L, we define its leakage as the maximum total variation distance between output distributions for any two secrets:

Definition 4.1 (Leakage of a program).

\text{Leakage}(M) = \max_{h_1,h_2 \in \mathcal{H}} D_{TV}\bigl(\llbracket M \rrbracket(h_1),\; \llbracket M \rrbracket(h_2)\bigr)

where \llbracket M \rrbracket(h) denotes the distribution over outputs when the secret is h. A program is \varepsilon-secure if \text{Leakage}(M) \le \varepsilon.

In the context of LLM‑generated code, the "program" M itself is the generated code; the secrets are parts of the prompt that should not be leaked (e.g., passwords, API keys). We therefore need to reason about the leakage of M as a function of the prompt.

4.1.2 Specification Language

Specifications are expressed as judgements of the form

\Gamma \vdash \{P\} M \{Q\}

where P and Q are expectations – functions from states to [0,1]. Intuitively, this means that if the initial state satisfies P (in an expected‑value sense), then after executing M, the expected value of Q is at least P.

For security properties, we typically take Q to be an indicator of a property \phi (i.e., Q(\sigma) = 1 if \sigma satisfies \phi, else 0), and P to be a lower bound on the probability of satisfying \phi. For leakage properties, we use a different form of judgement that directly constrains the leakage of M.

Definition 4.2 (Leakage specification).
We write \Gamma \vdash M : \text{Leakage} \le \varepsilon to mean that for any two secrets h_1,h_2 consistent with \Gamma, the output distributions satisfy D_{TV}(\llbracket M \rrbracket(h_1), \llbracket M \rrbracket(h_2)) \le \varepsilon.

This is not a Hoare triple but a separate judgement that can be proved using the quantitative information flow results from Chapter 2.

4.1.3 Relating Leakage and Probabilistic Hoare Logic

For a given security property \phi, we can encode the requirement that M satisfies \phi with high probability using a probabilistic Hoare triple:

\{ \mathbf{1} \}\; M \; \{ [\phi] \}

where \mathbf{1} is the constant‑1 expectation, and [\phi] is the indicator of \phi. This triple asserts that the probability of satisfying \phi is at least 1 – i.e., \phi holds almost surely. More generally, we can specify a lower bound p:

\{ p \}\; M \; \{ [\phi] \}

means that the probability of satisfying \phi is at least p.

Leakage properties, however, are not directly expressible as post‑expectations because they involve a comparison across different initial secrets. They will be handled by separate proof rules (Section 4.3).

---

4.2 Weakest Pre‑Expectation for λ‑LLM

We now extend the weakest pre‑expectation calculus (Section 2.4) to the λ‑LLM constructs. Recall that \mathrm{wp}(M, f) gives the expected value of f after executing M, as a function of the initial state.

Definition 4.3 (wp for λ‑LLM).
For each term M and post‑expectation f, \mathrm{wp}(M, f) is defined by induction on M:

· Standard constructs (variables, abstraction, application, probabilistic choice) follow the usual rules [60]. For probabilistic choice:
  \mathrm{wp}(M \oplus_p N, f) = p \cdot \mathrm{wp}(M,f) + (1-p) \cdot \mathrm{wp}(N,f).
· Prompt: \mathrm{prompt}[P] is a value; its wp is simply f applied to the current state (which includes the prompt). Formally, we treat \mathrm{prompt}[P] as a constant that does not change the state, so
  \mathrm{wp}(\mathrm{prompt}[P], f) = f.
· Generation: \mathrm{gen}[M] first evaluates M to a prompt, then invokes the oracle. The wp is:
  \mathrm{wp}(\mathrm{gen}[M], f) = \mathrm{wp}(M, \lambda x. \sum_{N} \mu_x(N) \cdot \mathrm{wp}(N, f))
  where \mu_x is the distribution produced by the oracle on prompt x. This captures the expected value after generation: we take the expectation over the distribution of prompts (if M is probabilistic), then over the generated terms, then over their further evaluation.
  In the common case where M directly reduces to a prompt \mathrm{prompt}[P] with probability 1, this simplifies to
  \mathrm{wp}(\mathrm{gen}[\mathrm{prompt}[P]], f) = \sum_{N} \mu_P(N) \cdot \mathrm{wp}(N, f)
  where \mu_P = \mathcal{O}(P).
· Verification: \mathrm{verify}[M] checks validity. If M is valid, it returns M; otherwise it goes to \bot. The wp is:
  \mathrm{wp}(\mathrm{verify}[M], f) = \mathrm{wp}(M, \lambda x. \text{if } \mathrm{Valid}(x) \text{ then } f(x) \text{ else } f(\bot)).
  Since \bot is an absorbing state, we need to define f(\bot). Typically, f(\bot) is 0 for properties that require successful generation (i.e., \bot does not satisfy any useful property). We'll adopt that convention.
· Repair: \mathrm{repair}[M,N] first evaluates M; if it becomes \bot, it then generates from N. The wp is:
  \mathrm{wp}(\mathrm{repair}[M,N], f) = \mathrm{wp}(M, \lambda x. \text{if } x = \bot \text{ then } \mathrm{wp}(\mathrm{gen}[N], f) \text{ else } f(x)).
  This captures the two‑phase behaviour: if M succeeds, we take its result; if it fails (becomes \bot), we try generation from N.
· Halt: \mathrm{halt}[M] does not change the behaviour; it just marks termination. So
  \mathrm{wp}(\mathrm{halt}[M], f) = \mathrm{wp}(M, f).
· Name creation: \mathrm{new}\,a.M introduces a fresh name. The wp is:
  \mathrm{wp}(\mathrm{new}\,a.M, f) = \mathrm{wp}(M, f)
  assuming that the fresh name does not affect the evaluation of f (which it doesn't, as f is a function of the program state, not of names directly). A more refined treatment would account for the possibility of nominal binding failure, but we can incorporate that into the semantics as a transition to \bot and handle it via the wp for \bot.
· Error: \bot is an absorbing state. We define \mathrm{wp}(\bot, f) = f(\bot). Typically, f(\bot) is 0 for success properties.

These definitions are sound with respect to the operational semantics in the sense that they compute the correct expected value of f.

Lemma 4.4 (Soundness of wp).
For any term M and expectation f, if \emptyset \vdash M : T then

\mathrm{wp}(M, f)(\sigma) = \mathbb{E}_{\sigma' \sim \llbracket M \rrbracket(\sigma)}[f(\sigma')]

where \llbracket M \rrbracket(\sigma) is the distribution over final states obtained by executing M from initial state \sigma.

Proof sketch. By induction on the structure of M, using the definitions above and the fact that the operational semantics is deterministic up to probability. The generation case relies on the oracle definition; the verification and repair cases follow from the semantics of those constructs. ∎

---

4.3 Proof Rules for Security Properties

We now present a collection of proof rules for establishing security properties of λ‑LLM terms. The rules are divided into two categories: rules for probabilistic Hoare triples (establishing lower bounds on the probability of satisfying a property) and rules for leakage bounds.

4.3.1 Probabilistic Hoare Logic Rules

The following rules are standard for probabilistic programs [60], adapted to λ‑LLM.

Rule 4.5 (Probabilistic choice).

\frac{\{P\} M \{Q\} \quad \{P\} N \{Q\}}{\{P\} M \oplus_p N \{Q\}}

Rule 4.6 (Sequential composition).

\frac{\{P\} M \{R\} \quad \{R\} N \{Q\}}{\{P\} M;N \{Q\}}

Rule 4.7 (Consequence).

\frac{P \le P' \quad \{P'\} M \{Q'\} \quad Q' \le Q}{\{P\} M \{Q\}}

For λ‑LLM‑specific constructs, we have the following rules.

Rule 4.8 (Generation).
Let \mu = \mathcal{O}(P). Then

\frac{\forall N \in \mathrm{supp}(\mu).\; \{P_N\} N \{Q\}}{\{ \sum_N \mu(N) \cdot P_N \} \;\mathrm{gen}[\mathrm{prompt}[P]]\; \{Q\}}

where P_N is the pre‑expectation for branch N. In the simplest case where all P_N are equal to some P, this becomes

\{P\} \;\mathrm{gen}[\mathrm{prompt}[P]]\; \{Q\} \quad\text{if}\quad \forall N \in \mathrm{supp}(\mu).\; \{P\} N \{Q\}.

Rule 4.9 (Verification).

\frac{\{P\} M \{Q\} \quad \mathrm{Valid}(M) \text{ holds with probability } 1}{\{P\} \mathrm{verify}[M] \{Q\}}

If M may be invalid, we need to account for the possibility of \bot. A more precise rule is:

\frac{\{P\} M \{R\} \quad R \le [\mathrm{Valid}] \cdot Q + (1-[\mathrm{Valid}]) \cdot Q_\bot}{\{P\} \mathrm{verify}[M] \{Q\}}

where [\mathrm{Valid}] is the indicator of validity, and Q_\bot is the post‑expectation for \bot (typically 0).

Rule 4.10 (Repair).

\frac{\{P\} M \{R\} \quad R \le [\mathrm{Fail}] \cdot \mathrm{wp}(\mathrm{gen}[N],Q) + (1-[\mathrm{Fail}]) \cdot Q}{\{P\} \mathrm{repair}[M,N] \{Q\}}

where [\mathrm{Fail}] indicates that M resulted in \bot.

Rule 4.11 (Halt).

\frac{\{P\} M \{Q\}}{\{P\} \mathrm{halt}[M] \{Q\}}

4.3.2 Rules for Leakage Bounds

To reason about \varepsilon-QNI, we need rules that directly bound the total variation distance between output distributions.

Rule 4.12 (Sequential composition – data‑processing).
If M satisfies \varepsilon-QNI, then for any (randomised) N, the composition M;N also satisfies \varepsilon-QNI.

Justification. This follows from the data‑processing inequality for total variation distance (Theorem 2.12). No additional rule is needed; it's a property of the semantics.

Rule 4.13 (Parallel composition – additive).
If M satisfies \varepsilon_1-QNI and N satisfies \varepsilon_2-QNI, and they operate on disjoint variables, then M \parallel N satisfies (\varepsilon_1 + \varepsilon_2)-QNI.

Justification. This follows from the product bound (Theorem 2.13).

Rule 4.14 (Generation leakage).
For \mathrm{gen}[\mathrm{prompt}[P]], the leakage is at most the leakage of the oracle \mathcal{O} on prompt P. If the oracle itself satisfies \varepsilon-QNI (as a function of secrets embedded in the prompt), then the generated term satisfies \varepsilon-QNI.

This rule is not directly provable without assumptions on the oracle; we take it as an axiom for the purpose of reasoning about the system.

Rule 4.15 (Verification leakage).
Verification does not increase leakage: if M satisfies \varepsilon-QNI, then \mathrm{verify}[M] also satisfies \varepsilon-QNI. This holds because verification is a deterministic function of M (it either returns M or \bot), and deterministic functions do not increase total variation distance.

Rule 4.16 (Repair leakage).
Repair involves a second generation step, so its leakage is bounded by the leakage of the first component plus the leakage of the repair generation, appropriately combined. A precise bound requires careful analysis; in the MA‑Secure architecture (Chapter 5), we will use the composition theorems to bound the overall leakage.

---

4.4 Soundness of the Proof System

We must ensure that the proof rules are sound with respect to the semantics. For probabilistic Hoare triples, soundness means that if \{P\} M \{Q\} is derivable, then for any initial state \sigma, \mathbb{E}_{\sigma' \sim \llbracket M \rrbracket(\sigma)}[Q(\sigma')] \ge P(\sigma).

Theorem 4.17 (Soundness of probabilistic Hoare logic).
All rules in Section 4.3.1 are sound.

Proof. We prove soundness of each rule using the definition of \mathrm{wp}.

· Probabilistic choice: From the premise, \mathrm{wp}(M,Q) \succeq P and \mathrm{wp}(N,Q) \succeq P. Then
  \mathrm{wp}(M \oplus_p N, Q) = p\,\mathrm{wp}(M,Q) + (1-p)\,\mathrm{wp}(N,Q) \succeq pP + (1-p)P = P.
· Sequential composition: \mathrm{wp}(M;N, Q) = \mathrm{wp}(M, \mathrm{wp}(N,Q)). By premise, \mathrm{wp}(N,Q) \succeq R, and \mathrm{wp}(M,R) \succeq P. Monotonicity of \mathrm{wp} gives \mathrm{wp}(M, \mathrm{wp}(N,Q)) \succeq \mathrm{wp}(M,R) \succeq P.
· Consequence: Immediate from monotonicity.
· Generation: By definition, \mathrm{wp}(\mathrm{gen}[\mathrm{prompt}[P]], Q) = \sum_N \mu(N) \,\mathrm{wp}(N,Q). From the premise, each \mathrm{wp}(N,Q) \succeq P_N. Hence the sum is \succeq \sum_N \mu(N) P_N, which is the pre‑expectation in the conclusion.
· Verification: \mathrm{wp}(\mathrm{verify}[M], Q) = \mathrm{wp}(M, \lambda x. \text{if } \mathrm{Valid}(x) \text{ then } Q(x) \text{ else } Q(\bot)). If M satisfies its specification, the result follows.
· Repair: Similar, using the definition of \mathrm{wp} for repair.
· Halt: \mathrm{wp}(\mathrm{halt}[M], Q) = \mathrm{wp}(M, Q), so the rule is sound.

Thus all rules preserve the semantic interpretation. ∎

For leakage rules, soundness means that if the premises hold (semantically), then the conclusion holds semantically.

Theorem 4.18 (Soundness of leakage rules).
Rules 4.12–4.16 are sound.

Proof.

· Rule 4.12 follows directly from Theorem 2.12.
· Rule 4.13 follows from Theorem 2.13.
· Rule 4.14 is an axiom; its soundness depends on the oracle satisfying \varepsilon-QNI, which we assume.
· Rule 4.15: For any two secrets h_1,h_2, \llbracket \mathrm{verify}[M] \rrbracket(h_i) is either \llbracket M \rrbracket(h_i) or a point mass on \bot. The total variation distance between these outputs is at most the distance between the distributions of M, because the mapping to \bot is deterministic and can only decrease distance.
· Rule 4.16: This is a more complex composition; its soundness will be established in the context of the MA‑Secure architecture (Chapter 5) using the composition theorems. ∎

---

4.5 Compositionality Results

Compositionality is essential for reasoning about large systems built from smaller components. We have already seen two key composition theorems:

Theorem 4.19 (Sequential composition preserves leakage).
If M is \varepsilon-secure, then M;N is also \varepsilon-secure for any N.

Proof. This is Rule 4.12, already justified. ∎

Theorem 4.20 (Parallel composition adds leakage).
If M is \varepsilon_1-secure and N is \varepsilon_2-secure and they operate on disjoint variables, then M \parallel N is (\varepsilon_1 + \varepsilon_2)-secure.

Proof. This is Rule 4.13, already justified. ∎

These theorems allow us to build complex systems from simpler parts while keeping track of overall leakage. In the next chapter, we will use them to prove end‑to‑end security of the MA‑Secure multi‑agent architecture.

---

4.6 Running Example in SecGen

Returning to the password‑checker example, let M_1 be the secure implementation and M_2 the insecure one. Suppose we have analysed M_1 and determined that its leakage is at most 0.1. Then we can apply Rule 4.14 (generation leakage) to the generation step, obtaining that \mathrm{gen}[\mathrm{prompt}[P]] has leakage at most the oracle's leakage on P. If the oracle itself satisfies a leakage bound (say, 0.2), then the generated term (before verification) has leakage 0.2.

Now we apply verification. By Rule 4.15, verification does not increase leakage, so after verifying M_1 (which is valid), the term still has leakage 0.2. But we can also promote it to a secure type using Rule 3.13, which requires \mathcal{L}(M_1) \le \varepsilon. If we take \varepsilon = 0.1, then M_1 can be promoted to \text{Sec}_{0.1}(\text{Code}), meaning it satisfies a stronger leakage bound. This shows that verification alone does not improve leakage; it only checks validity. The promotion rule, however, allows us to certify that a term meets a certain leakage bound based on analysis.

For the insecure M_2, its leakage is high (say, 0.8). It cannot be promoted. In the multi‑agent system, the validator will reject it (if it checks the security property), and repair will be invoked.

We can also express the overall goal as a probabilistic Hoare triple:

\{ \mathbf{1} \}\; \text{Sys}(P) \; \{ [\text{Secure}] \}

where \text{Secure} is the property that the final code is both valid and has leakage ≤ ε. This triple asserts that the system produces a secure term with probability 1. In practice, we may only achieve probability 1-\delta; then we would write \{1-\delta\} \text{Sys}(P) \{[\text{Secure}]\}.

The next chapter will show how to derive such guarantees for the MA‑Secure architecture using the rules developed here.

---

4.7 Conclusion

This chapter has presented the SecGen verification framework, which provides a rigorous basis for reasoning about security properties of λ‑LLM terms. The key contributions are:

· A specification language for quantitative security properties, including leakage bounds and probabilistic Hoare triples.
· An extension of the weakest pre‑expectation calculus to λ‑LLM constructs, with soundness proved against the operational semantics.
· A set of proof rules for establishing both probabilistic correctness and leakage bounds, with soundness theorems.
· Compositionality results that enable modular reasoning.

The framework will be used in Chapter 5 to verify the MA‑Secure multi‑agent architecture, and in Chapter 6 to derive fundamental limits on what can be achieved.

In the next chapter, we move from verifying individual terms to verifying an entire system of interacting agents.

Chapter 5: The MA‑Secure Architecture

This chapter presents the MA‑Secure architecture, a formally specified multi‑agent system for secure code generation using LLMs. The architecture comprises three agents – a generator, a validator, and a repairer – that interact via typed communication channels. We model the system in the probabilistic π‑calculus (Section 2.2), which allows us to reason compositionally about its behaviour. We prove two main theorems: probabilistic convergence (the system eventually outputs a secure term with probability 1) and end‑to‑end security (a lower bound on the probability that the output satisfies a given security property). We also analyse robustness under adversarial attacks, bounding the leakage of the overall system.

---

5.1 Agent Specifications

We define three processes communicating over a set of channels. Let \mathit{ch}_{\mathrm{gen}}, \mathit{ch}_{\mathrm{val}}, \mathit{ch}_{\mathrm{rep}}, \mathit{success} be distinct channel names, each with an appropriate type. For simplicity, we assume that all communications are synchronous and that messages carry terms of type \mathsf{Code} or \mathsf{Prompt}.

5.1.1 Generator

The generator repeatedly produces a term from a current prompt. It receives feedback (a new prompt) on channel \mathit{ch}_{\mathrm{rep}} and sends generated terms on \mathit{ch}_{\mathrm{gen}}.

Definition 5.1 (Generator process).

\mathsf{Gen}(p) \;:=\; \overline{\mathit{ch}_{\mathrm{gen}}}\langle \mathsf{gen}[p] \rangle .\; \mathsf{Gen}(p) \;\oplus_{1}\; \mathit{ch}_{\mathrm{rep}}(x).\; \mathsf{Gen}(x)

The process first outputs the result of \mathsf{gen}[p] on \mathit{ch}_{\mathrm{gen}}. After that, with probability 1 (i.e., always) it continues as \mathsf{Gen}(p); the alternative branch is included to model the possibility of receiving a new prompt from the repairer. However, in a deterministic loop we would need a way to interleave input and output. A more accurate specification uses parallel composition and replication, but for the purpose of proving convergence we can abstract away the exact scheduling. We adopt a simpler model where the generator, after sending a term, waits for either a repair message or proceeds to generate again. The probabilistic choice \oplus_{1} is a trick to make the point; actually we need a non‑deterministic choice between output and input. In π‑calculus, we can write:

\mathsf{Gen}(p) \;:=\; \overline{\mathit{ch}_{\mathrm{gen}}}\langle \mathsf{gen}[p] \rangle .\mathsf{Gen}(p) \;+\; \mathit{ch}_{\mathrm{rep}}(x).\mathsf{Gen}(x)

where + denotes external choice (not probabilistic). Since our focus is on probabilistic behaviour, we can treat the choice as being resolved by the environment; the convergence proof only requires that whenever a repair message is available, it will eventually be taken. We'll keep the notation simple and assume a fair scheduler.

For the purpose of probabilistic analysis, we will work with a discrete‑time Markov chain abstraction that captures the essential loop: generate, validate, possibly repair.

5.1.2 Validator

The validator receives a term from the generator, checks whether it satisfies both the basic validity predicate \mathsf{Valid} and a given security property \phi. It then either accepts the term (sending it to the \mathit{success} channel) or rejects it (sending it to the repairer). The validator may be imperfect: it can mistakenly reject a secure term (false reject) or mistakenly accept an insecure term (false accept).

Definition 5.2 (Validator process).

\mathsf{Val} \;:=\; \mathit{ch}_{\mathrm{gen}}(x).\; \bigl( [\mathsf{Valid}(x) \land x \models \phi]_{s} \; \overline{\mathit{success}}\langle x \rangle \;\oplus_{1-s}\; \overline{\mathit{ch}_{\mathrm{rep}}}\langle x, \mathsf{feedback}(x) \rangle \bigr) .\; \mathsf{Val}

Here [\,\cdot\,]_{s} denotes a probabilistic test: if the condition holds, with probability s the validator accepts, and with probability 1-s it rejects (false reject). If the condition does not hold, the validator always rejects (i.e., sends to repair). We also allow the possibility of false accept: the validator might incorrectly accept an insecure term with some probability \delta. To model this, we can replace the condition with a more complex probabilistic function. For simplicity, we assume that the validator is sound for secure terms (never falsely rejects) and has a false accept probability \delta for insecure terms. Formally:

· If x \models \phi (and \mathsf{Valid}(x)), then \Pr[\mathsf{Val}\ \text{accepts}] = 1.
· If x \not\models \phi (or invalid), then \Pr[\mathsf{Val}\ \text{accepts}] \le \delta.

This is the standard assumption in verification: the validator may be incomplete (reject some secure terms) but must be sound (never accept insecure ones). However, in practice, false accepts are dangerous; we bound them by \delta. The process definition above can be adjusted to incorporate a probability of false accept; we omit the details for brevity.

The feedback message sent to the repairer includes the failed term x and possibly diagnostic information \mathsf{feedback}(x). In the simplest case, \mathsf{feedback}(x) could be a prompt that instructs the generator to avoid the mistake seen in x.

5.1.3 Repairer

The repairer listens on \mathit{ch}_{\mathrm{rep}} for a failed term and feedback. It then restarts the generator with an updated prompt, obtained by incorporating the feedback.

Definition 5.3 (Repairer process).

\mathsf{Rep} \;:=\; \mathit{ch}_{\mathrm{rep}}(x, f).\; \overline{\mathit{ch}_{\mathrm{gen}}}\langle \mathsf{gen}[\,\mathsf{update}(p_0, f)\,] \rangle .\; \mathsf{Rep}

Here \mathsf{update}(p_0, f) is a function that combines the original prompt p_0 (or the current prompt) with the feedback f to produce a new prompt. For analysis, we assume that the repair process may succeed with some probability t (i.e., the new generated term is secure with probability at least t), and that the probability does not decrease over successive repairs.

5.1.4 System Composition

The complete system is the parallel composition of the three agents, with all channels restricted so that they are internal.

Definition 5.4 (System).

\mathsf{Sys}(p_0) \;:=\; (\nu \mathit{ch}_{\mathrm{gen}}, \mathit{ch}_{\mathrm{val}}, \mathit{ch}_{\mathrm{rep}}, \mathit{success})\, \bigl( \mathsf{Gen}(p_0) \;\mid\; \mathsf{Val} \;\mid\; \mathsf{Rep} \bigr)

The output of the system is the term sent on \mathit{success}; we consider the system to have succeeded when such an output occurs.

---

5.2 Probabilistic Convergence

We first show that under reasonable assumptions, the system will eventually output a secure term with probability 1. This is essentially a liveness property.

Assumptions:

1. Generator reliability: For any prompt p, let p_s(p) = \Pr[\mathsf{gen}[p] \models \phi] be the probability that a single generated term satisfies the security property. We assume there exists a constant r_G > 0 such that for all prompts reachable in the system, p_s(p) \ge r_G. (This is a strong assumption; in practice, the generator may need training or fine‑tuning to ensure this. Feedback from the repairer should increase p_s.)
2. Validator soundness: If a term satisfies \phi, the validator accepts it with probability 1. (No false rejects for secure terms.)
3. Validator completeness: If a term does not satisfy \phi, the validator rejects it with probability at least 1-\delta (i.e., false accept probability ≤ \delta).
4. Repair effectiveness: When repair is invoked (i.e., a term is rejected), the new generated term is secure with probability at least t > 0. Moreover, this probability does not decrease over successive repairs.

Theorem 5.5 (Probabilistic convergence).
Under the above assumptions, the probability that \mathsf{Sys}(p_0) never outputs a secure term after k iterations of the generate‑validate‑repair loop is at most (1 - r_G)^k (if we ignore false accepts) or more precisely (1 - r_G(1-\delta))^k if we account for false accepts. In any case, as k \to \infty, this probability tends to 0. Hence the system eventually outputs a secure term with probability 1.

Proof sketch. Model the system as a discrete‑time Markov chain with states:

· S_0: generation phase,
· S_1: validation phase,
· S_2: repair phase,
· S_3: success (absorbing).

From generation, the system moves to validation with probability 1 (the generated term is sent). In validation:

· If the term is secure (prob. p_s), validator accepts with probability 1 (by soundness) → success.
· If the term is insecure (prob. 1-p_s), validator rejects with probability at least 1-\delta (by completeness) → repair; with probability at most \delta it falsely accepts → success with an insecure output (failure case, but for convergence we care about secure output). To avoid counting false accepts as success, we define success only when a secure term is output. Then the probability of moving to success from validation is exactly p_s (since secure terms are always accepted). The probability of moving to repair is 1-p_s (insecure terms are rejected with high probability; false accepts lead to insecure output, which we treat as a failure loop that may eventually lead to repair? Actually if an insecure term is falsely accepted, the system outputs it and stops, but that output is not secure, so the system has failed. For the purpose of convergence to a secure output, we must consider that a false accept terminates the system with a failure, so we cannot recover. Therefore the probability of ever reaching a secure output is at most p_s per attempt, and if a false accept occurs, the system stops with an insecure output. Thus the probability of never seeing a secure output after k attempts is at most (1-p_s)^k, because each attempt either succeeds (prob p_s) or fails (prob 1-p_s) – false accepts are included in the failure case because they don't lead to a secure output. So the bound (1-p_s)^k holds. With p_s \ge r_G > 0, this goes to 0. ∎

A more refined analysis would incorporate the possibility of repair improving p_s; the bound then becomes even smaller. The theorem shows that the system is probabilistically terminating with a secure output.

---

5.3 End‑to‑End Security Guarantee

We now derive a quantitative bound on the probability that the final output satisfies the security property \phi. This bound accounts for the generator's reliability, validator's false accept probability, and repair effectiveness.

Theorem 5.6 (End‑to‑end security).
Assume the generator satisfies p_s \ge r_G (probability of generating a secure term in any attempt). Assume the validator is sound (accepts all secure terms) and has false accept probability at most \delta for insecure terms. Assume that when repair is invoked, the new generated term is secure with probability at least t (and that successive repairs are independent in the sense that the probability does not decrease). Then the probability that the system outputs a secure term is at least

r_G + (1 - r_G)(1 - \delta) t.

Proof. The system can succeed in two ways:

1. Direct success: The first generated term is secure. Probability = r_G.
2. Success after repair: The first generated term is insecure (probability 1 - r_G). In that case:
   · With probability \delta, the validator falsely accepts it, and the system outputs an insecure term – this is a failure.
   · With probability 1 - \delta, the validator correctly rejects it, and repair is invoked. Given repair, the new generated term is secure with probability t. So the contribution from this branch is (1 - r_G)(1 - \delta) t.

Adding these gives the lower bound. ∎

If the system allows multiple repairs, the bound improves. For k repair attempts, the success probability is at least

1 - (1 - r_G)\bigl(\delta + (1-\delta)(1-t)\bigr)^k

assuming each repair attempt has the same success probability t. In the limit k \to \infty, if t > 0, the failure probability tends to (1 - r_G)\delta (the chance that an insecure term is falsely accepted and never repaired). This residual failure is due to validator imperfection; a perfect validator (\delta = 0) would guarantee eventual secure output with probability 1 (as in Theorem 5.5).

Corollary 5.7. If \delta = 0 (perfect validator) and t > 0, then the system outputs a secure term with probability 1 in the limit.

---

5.4 Compositional Leakage Bounds

Beyond the probability of producing a secure term, we also care about information leakage. The generator may leak information about secrets in the prompt through the generated code. The validator and repairer may also leak information via their outputs (e.g., whether a term was accepted or rejected). We need to bound the overall leakage of the system.

Recall from Chapter 2 that a process satisfies \varepsilon-QNI if for any two secrets s_1,s_2, the total variation distance between the output distributions is at most \varepsilon. We have composition rules:

· Sequential composition (data‑processing) does not increase leakage.
· Parallel composition of independent components adds leakage (if they operate on disjoint variables).

In our system, the generator, validator, and repairer are not independent; they share the secret through the generated term and the validation outcome. However, we can still bound the leakage by considering the information flow.

Theorem 5.8 (System leakage bound).
Let the generator satisfy \varepsilon_G-QNI (i.e., for any two secrets, the distributions of generated terms differ by at most \varepsilon_G in TV distance). Let the validator be such that its acceptance decision leaks at most \varepsilon_V bits (i.e., the distributions of acceptance given the secret differ by at most \varepsilon_V). Then the overall system, when it outputs a term on \mathit{success}, satisfies (\varepsilon_G + \varepsilon_V)-QNI.

Proof sketch. The output of the system is either a generated term (if accepted) or nothing (if it keeps repairing). The final output distribution is a mixture of the generator's output conditioned on acceptance and possibly on repair attempts. Using the data‑processing inequality and the fact that the validator's decision is a function of the generated term, we can bound the TV distance by \varepsilon_G + \varepsilon_V. The details are technical; a rigorous proof would use the composition results for probabilistic automata [68]. ∎

In practice, \varepsilon_V is small because the validator's decision is based on a deterministic check of the term; if the term itself leaks little, the decision leaks little. A more precise analysis would use the concept of conditional leakage and the fact that the validator's output is a function of the term, so its leakage is bounded by the leakage of the term.

---

5.5 Robustness Under Attack

We consider an adversary who can:

· Inject malicious prompts at generation time (e.g., through the initial prompt or via feedback channels),
· Observe the validator's outputs (success/failure),
· Tamper with communication channels (bounded capacity \eta).

We model the adversary as a process that interacts with the system. The goal is to bound how much the adversary can increase the leakage or degrade the security guarantee.

Theorem 5.9 (Robustness).
Under the attack model, the overall leakage of the system is at most

\varepsilon_{\text{raw}} + \delta \log |\mathcal{S}| + \eta

where \varepsilon_{\text{raw}} is the generator's leakage on the original prompt, \delta is the validator's false accept probability, and \eta is the channel tampering capacity.

Proof sketch. The adversary can influence the prompt, but the generator's leakage bound \varepsilon_G may increase. However, using the PAC‑Bayes bound (Theorem 2.5), the increase is bounded by the KL divergence between the original oracle and the adversarial one, which is limited by the adversary's capacity. The validator's false accepts give the adversary at most \delta advantage in guessing the secret (via the success/failure signal). Channel tampering adds at most \eta bits of leakage. Combining these yields the bound. ∎

This theorem shows that the system degrades gracefully: even under attack, the leakage remains bounded by a sum of manageable terms.

---

5.6 Running Example in MA‑Secure

Recall the password‑checker example. Let the generator have r_G = 0.7 (probability of producing a secure term). Let the validator be sound with false accept probability \delta = 0.05. Let the repair success probability be t = 0.8. Then by Theorem 5.6, the overall success probability is at least

0.7 + 0.3 \times 0.95 \times 0.8 = 0.7 + 0.228 = 0.928.

If we allow multiple repairs, the probability approaches 1 - 0.3 \times 0.05 = 0.985 (since the only failure mode is a false accept on the first attempt). This is a strong guarantee.

For leakage, suppose the generator leaks at most \varepsilon_G = 0.2 (TV distance). The validator's decision leaks at most \varepsilon_V = 0.01 (because it is nearly deterministic). Then the overall leakage is at most 0.21. Under attack, with channel tampering capacity \eta = 0.1, the leakage becomes at most 0.31. Still acceptable for many applications.

---

5.7 Conclusion

This chapter has presented the MA‑Secure architecture, a formally specified multi‑agent system for secure LLM code generation. We proved that under reasonable assumptions, the system converges to a secure output with probability 1, and we gave a quantitative lower bound on the probability of success. We also bounded the information leakage and showed robustness under attack. The architecture demonstrates that formal methods can provide provable guarantees for LLM‑based systems, complementing the foundational calculus and verification framework developed in earlier chapters.

In the next chapter, we step back and ask: what are the fundamental limits of such systems? We derive impossibility results that show that perfect security is unattainable, and we characterise the trade‑offs between complexity, reliability, and leakage.

---

Chapter 6: Fundamental Limits

This chapter explores the theoretical boundaries of what can be achieved in secure LLM‑based code generation. Drawing on information theory, computability theory, computational complexity, and learning theory, we establish upper bounds on achievable guarantees and prove impossibility results. These limits are not flaws of any particular architecture but inherent to the problem; they delineate the space of feasible solutions and guide future research.

---

6.1 Information‑Theoretic Bounds

The first limit comes from source coding: the probability that a randomly generated program satisfies a given property \phi cannot exceed the relative size of the set of \phi-satisfying programs in the generator's support.

Theorem 6.1 (Source‑coding bound).
Let \mathcal{O} be an LLM oracle that, for a given prompt P, produces a distribution \mu over programs. Let \mathcal{S}_\phi = \{M \mid M \models \phi\} be the set of programs satisfying \phi. Then

\Pr_{M \sim \mu}[M \models \phi] \le \frac{|\mathcal{S}_\phi \cap \mathrm{supp}(\mu)|}{|\mathrm{supp}(\mu)|} \le 2^{-(H_\infty(\mu) - \log |\mathcal{S}_\phi|)}

where H_\infty(\mu) is the min‑entropy of \mu. In particular, if \mu is uniform over its support, the probability is exactly |\mathcal{S}_\phi \cap \mathrm{supp}(\mu)| / |\mathrm{supp}(\mu)|.

Proof. The first inequality is trivial. The second follows from the fact that \max_x \mu(x) \le 2^{-H_\infty(\mu)}, so for any set A, \mu(A) \le |A| \cdot 2^{-H_\infty(\mu)}. Taking A = \mathcal{S}_\phi gives the bound. ∎

This bound shows that if the property \phi is very specific (small |\mathcal{S}_\phi|), the probability of generating a satisfying program is exponentially small in the min‑entropy gap. In practice, \mathcal{S}_\phi may be huge, but the bound still applies.

For non‑uniform generators, we can use a PAC‑Bayesian bound similar to Theorem 2.5 to relate the probability to the KL divergence between \mu and a prior that concentrates on \mathcal{S}_\phi.

Corollary 6.2. If the generator's output has entropy H, then the probability of generating a program that satisfies \phi is at most 2^{-(H - \log |\mathcal{S}_\phi|)}. Hence to achieve a success probability of at least 1-\delta, we need H \ge \log |\mathcal{S}_\phi| - \log(1-\delta). This is a form of entropy requirement: the generator must have enough randomness to cover the space of good programs.

---

6.2 Computability Limits

The next set of limits arise from undecidability. Since the λ‑calculus is Turing‑complete, any non‑trivial semantic property of programs is undecidable (Rice's theorem). This applies directly to the verification problem for generated code.

Theorem 6.3 (Rice‑LLM).
There is no computable validator that, given a term M and a non‑trivial security property \phi (i.e., a property that is not always true or always false), decides whether M \models \phi.

Proof. Immediate from Rice's theorem: the set \{M \mid M \models \phi\} is undecidable for any non‑trivial \phi. ∎

This means that any practical validator must be either incomplete (reject some secure programs) or unsound (accept some insecure ones). The MA‑Secure architecture embraces this by allowing a bounded false accept probability \delta.

A more specific impossibility result concerns exact non‑interference. Even if we restrict to a class of programs that is decidable, ensuring that a generated program leaks no information at all is impossible in general.

Theorem 6.4 (Impossibility of perfect non‑interference).
No computable generator can guarantee that for every prompt P and every pair of secrets s_1,s_2, the output distributions are identical (i.e., exact non‑interference). In fact, any generator that is expressive enough to simulate Turing machines will have prompts for which the leakage is positive.

Proof sketch. Suppose such a generator exists. Construct a family of prompts that encode the halting problem: for a Turing machine T, the prompt asks for a program that outputs 1 if T halts and 0 otherwise, but with the secret being the halt status. Exact non‑interference would require that the output distribution is the same regardless of whether T halts, which contradicts the fact that the program must distinguish the two cases (otherwise it wouldn't satisfy the specification). A more rigorous argument uses a reduction from the halting problem. ∎

Thus we must accept that some leakage is inevitable; the goal is to bound it, not eliminate it entirely.

---

6.3 Complexity‑Theoretic Bounds

Even if a property is decidable, it may be computationally intractable to verify. Many security properties (e.g., information flow, timing leaks) are NP‑hard or even undecidable in the worst case.

Theorem 6.5 (Verification hardness).
For a fixed security property \phi that is NP‑hard (e.g., “the program does not contain a buffer overflow” for certain models), any validator that decides \phi for all programs must run in exponential time in the worst case, assuming \mathrm{P} \neq \mathrm{NP}.

Proof. Immediate from the definition of NP‑hardness. ∎

This implies that validators must either be incomplete (skip some checks) or use heuristics that may miss vulnerabilities. In MA‑Secure, the validator can be designed to check only a tractable approximation of \phi, accepting a small false accept probability.

---

6.4 PAC Learning Bounds

The generator itself is learned from data. The theory of Probably Approximately Correct (PAC) learning gives bounds on the sample complexity needed to ensure that the generator produces secure code with high probability.

Definition 6.6 (PAC security).
A generation framework F is (\varepsilon,\delta)-PAC secure if for any distribution \mathcal{D} over prompts, with probability at least 1-\delta over the training set (of size m), we have

\Pr_{P \sim \mathcal{D}}\bigl[ \Pr[F(P) \models \phi] \ge 1 - \varepsilon \bigr] \ge 1 - \delta.

Theorem 6.7 (Sample complexity).
To achieve (\varepsilon,\delta)-PAC security for a property class of VC‑dimension d, the training set size must satisfy

m = \Omega\left( \frac{d + \log(1/\delta)}{\varepsilon} \right).

Proof. This is a standard PAC‑bound for learning a concept class; see [59, 87]. ∎

Thus if the property \phi is complex (high VC‑dimension), a huge amount of verified training data is needed to guarantee that the generator will produce secure code with high probability. This explains why off‑the‑shelf LLMs, trained on unfiltered Internet data, often generate insecure code: they have not seen enough secure examples.

---

6.5 The Softmax Bottleneck

A final limit arises from the architecture of Transformers themselves. Recent work [58] shows that the softmax attention mechanism has a combinatorial noise floor that limits the ability of two independent agents to agree on a subtle property.

Lemma 6.8 (Noise floor).
In a multi‑agent system where agents are implemented by Transformers with softmax attention, the probability that two agents independently agree on a security property \phi (i.e., both correctly classify a given term) is at most 1 - \Omega(1/\sqrt{d}), where d is the context window size.

Proof sketch. The softmax attention cannot focus on a single relevant token without exponentially large logits. When two agents attend to the context independently, the probability that they both attend to exactly the same set of tokens (necessary for perfect agreement on subtle properties) is bounded away from 1 by combinatorial noise. The bound \Omega(1/\sqrt{d}) comes from concentration inequalities for the maximum of d random variables. See [58] for details. ∎

This implies that even if we had perfect oracles, the inherent noise in attention limits the reliability of multi‑agent consensus. In MA‑Secure, this is mitigated by having a single validator; but if we had multiple validators voting, the agreement probability would be bounded.

---

6.6 Summary of Fundamental Limits

Constraint Class Formal Origin Result
Information Source coding Secure generation prob ≤ \(2^{-(H - \log 
Computability Halting problem, Rice's theorem No perfect security for all prompts; verification undecidable
Complexity NP‑hardness Exponential time needed in worst case
Learning PAC‑Bayes Sample complexity grows with property complexity
Architectural Softmax attention Agreement probability bounded away from 1

These limits are not discouraging; they delineate the space of what is possible. The MA‑Secure architecture respects these limits by aiming for probabilistic, bounded‑leakage guarantees rather than perfection, and by using a validator that is sound but possibly incomplete, and by relying on repair to boost probability. The information‑theoretic bound tells us that we cannot expect high probability for extremely specific properties unless the generator has high entropy. The computability and complexity bounds justify the use of heuristics and approximate verification. The PAC bound reminds us that training data matters. The softmax bottleneck is a caution for multi‑agent designs.

---

6.7 Conclusion

This chapter has established fundamental limits on secure LLM‑based code generation. No system can achieve perfect, universal guarantees; but we can achieve practical, probabilistic bounds with well‑designed architectures like MA‑Secure. The results here provide a theoretical foundation for understanding what is achievable and guide the design of future systems. In the final chapter, we conclude and discuss directions for future work.

