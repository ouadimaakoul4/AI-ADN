Meta-Cognitive Agent Graphs: Self-Reflective Agents in Heterogeneous Collaborative Networks

Abstract

We present Meta-Cognitive Agent Graphs (MCAG), a formal framework for multi-agent systems where each agent possesses learnable self-reflection capabilities and the system dynamically constructs collaboration graphs based on agents' meta-cognitive states. We introduce reflection-aware edges that propagate confidence scores and uncertainty metrics between agents, enabling adaptive collaboration based on collective self-awareness. This work establishes mathematical foundations for distributed computational metacognition, providing rigorous formulations for agent introspection, confidence calibration, dynamic graph construction, and meta-cognitive information flow. Through comprehensive theoretical analysis, we demonstrate how MCAG creates systems that are not only more accurate and efficient but also more interpretable and robust than existing approaches.

1. Introduction

1.1 The Formal Problem

Let $\mathcal{P}$ be a class of problems where each problem instance $q \in \mathcal{Q}$ requires producing an answer $y \in \mathcal{Y}$. Current approaches follow two paradigms:

1. Multi-Agent Systems (MAS): A set of agents $\mathcal{A} = \{A_1, \dots, A_n\}$ collaborate via a graph $G = (V, E)$, where $V \subseteq \mathcal{A}$ and $E \subseteq V \times V$ defines collaboration edges. The system optimizes a collaboration policy $\pi^{\text{collab}} : \mathcal{Q} \to \mathcal{G}$ mapping problems to graph structures.
2. Self-Reflective Agents: Individual agents $A_i$ employ meta-cognitive policies $\pi_i^{\text{ref}} : \mathcal{S}_i \to \mathcal{A}^{\text{ref}}$ mapping internal states to reflection actions.

The fundamental limitation is that MAS lacks agents with introspective capabilities ($\pi_i^{\text{ref}} = \emptyset$ for all $i$), while self-reflective agents lack collaborative meta-cognition (no mechanism for sharing $\pi_i^{\text{ref}}$ outputs).

1.2 Core Hypothesis

We hypothesize that embedding self-reflection capabilities into individual agents, and designing collaboration mechanisms that leverage these reflective states, creates multi-agent systems with superior formal properties. Specifically:

1. Reflection-aware collaboration enables more intelligent information routing through confidence-weighted fusion.
2. Dynamic graph reconstruction based on collective reflective states allows systems to adapt structure to problem difficulty.
3. Emergent meta-cognitive patterns create specialization in both domains and reflection strategies.

1.3 Formal Contributions

1. Mathematical Framework: Formal definition of MCAG as a tuple $(\mathcal{A}, \mathcal{G}, \Pi)$ with precise specifications for agent introspection, confidence calibration, and meta-cognitive edges.
2. Optimization Theory: Joint optimization of agent reflection policies and graph construction policies with theoretical guarantees.
3. Information-Theoretic Analysis: Formal connections between meta-cognitive communication and information bottlenecks in collaborative networks.
4. Robustness Formulations: Mathematical definitions of robustness metrics and adversarial defense mechanisms.

2. Formal Foundations

2.1 Preliminary Definitions

Definition 2.1 (Base Agent). A base agent $A_i^0$ is defined as:
A_i^0 = (M_i, \mathcal{X}_i, \mathcal{Y}_i)


where:

· $M_i : \mathcal{X}_i \to \mathcal{Y}_i$ is a deterministic or stochastic mapping
· $\mathcal{X}_i \subseteq \mathcal{Q}$ is the input space
· $\mathcal{Y}_i$ is the output space

Definition 2.2 (Multi-Agent System). A MAS is a tuple:
\text{MAS} = (\mathcal{A}, \mathcal{G}, \Phi, \Psi)


where:

· $\mathcal{A} = \{A_1^0, \dots, A_n^0\}$ is a set of base agents
· $\mathcal{G} \subseteq 2^{\mathcal{A}} \times 2^{\mathcal{A} \times \mathcal{A}}$ is the space of possible collaboration graphs
· $\Phi : \mathcal{Q} \to \mathcal{G}$ is the graph construction policy
· $\Psi : \mathcal{G} \times \mathcal{Q} \to \mathcal{Y}$ is the aggregation function

Definition 2.3 (Self-Reflective Agent). A self-reflective agent extends a base agent:
A_i^{\text{ref}} = (A_i^0, \mathcal{S}_i, \mathcal{A}_i^{\text{ref}}, \pi_i^{\text{ref}}, C_i)


where:

· $\mathcal{S}_i$ is the state space (internal representations)
· $\mathcal{A}_i^{\text{ref}} = \{\text{internal}, \text{external}, \text{rewrite}, \text{tool}, \text{delegate}\}$
· $\pi_i^{\text{ref}} : \mathcal{S}_i \to \Delta(\mathcal{A}_i^{\text{ref}})$ is the reflection policy
· $C_i : \mathcal{S}_i \to [0, 1]$ is the confidence calibrator

2.2 Information-Theoretic Preliminaries

Let $X$ be the input, $Y$ the target output, and $H_i$ the hidden state of agent $A_i$. We consider:

· Mutual Information: $I(X;Y) = \mathbb{E}_{p(x,y)}[\log\frac{p(x,y)}{p(x)p(y)}]$
· Conditional Mutual Information: $I(X;Y|Z) = \mathbb{E}_{p(x,y,z)}[\log\frac{p(x,y|z)}{p(x|z)p(y|z)}]$
· Information Bottleneck: For agent $A_i$, find representation $T_i$ that minimizes:
  \mathcal{L}_{\text{IB}} = I(X;T_i) - \beta I(T_i;Y)

2.3 Graph Theory Foundations

A collaboration graph $G = (V, E)$ where:

· $V = \{v_1, \dots, v_k\} \subseteq \mathcal{A}$ are vertices (agents)
· $E \subseteq V \times V$ are directed edges

We extend this to weighted multigraphs for MCAG:
G^{\text{MCAG}} = (V, E^{\text{std}}, E^{\text{meta}}, w^{\text{std}}, w^{\text{meta}})


where:

· $E^{\text{std}} \subseteq V \times V$ are standard collaboration edges
· $E^{\text{meta}} \subseteq V \times V \times \mathcal{T}$ are typed meta-cognitive edges with $\mathcal{T} = \{\text{conf}, \text{strat}, \text{unc}\}$
· $w^{\text{std}} : E^{\text{std}} \to \mathbb{R}^+$ are edge weights
· $w^{\text{meta}} : E^{\text{meta}} \to \mathbb{R}^+$ are meta-edge weights

3. Meta-Cognitive Agent Graphs: Formal Framework

3.1 Agent Model with Self-Reflection

Definition 3.1 (MCAG Agent). A meta-cognitive agent is defined as:
A_i = (M_i, R_i, C_i, \Theta_i, \Gamma_i)


where:

· $M_i : \mathcal{X}_i \to \mathcal{Y}_i$ is the base model
· $R_i = (\mathcal{S}_i, \mathcal{A}_i^{\text{ref}}, \pi_i^{\text{ref}})$ is the reflection module
· $C_i = (f_i^{\text{cal}}, \mathcal{H}_i)$ is the confidence calibrator with calibration function $f_i^{\text{cal}} : \mathcal{S}_i \times \mathcal{H}_i \to [0,1]$ and history $\mathcal{H}_i$
· $\Theta_i = \{\theta_i^a \in \mathbb{R}^d : a \in \mathcal{A}_i^{\text{ref}}\}$ are strategy embeddings
· $\Gamma_i = \{\gamma_i^j \in \mathbb{R}^+ : j \in \mathcal{A}\}$ are trust parameters for other agents

Definition 3.2 (Reflection State). The reflection state of agent $A_i$ is:
s_i^{\text{ref}} = (y_i, \hat{c}_i, a_i^{\text{ref}}, \mathcal{U}_i, t_i)


where:

· $y_i \in \mathcal{Y}_i$ is the current output
· $\hat{c}_i = f_i^{\text{cal}}(s_i, \mathcal{H}_i)$ is calibrated confidence
· $a_i^{\text{ref}} \sim \pi_i^{\text{ref}}(s_i)$ is the chosen reflection action
· $\mathcal{U}_i \in \mathcal{P}(\mathcal{Y}_i)$ is an uncertainty distribution over outputs
· $t_i \in \mathbb{N}$ is the reflection depth (number of reflection cycles)

Definition 3.3 (Reflection Policy). The reflection policy is parameterized as:
\pi_i^{\text{ref}}(a|s_i) = \frac{\exp(\phi(s_i)^\top \theta_i^a)}{\sum_{a' \in \mathcal{A}_i^{\text{ref}}} \exp(\phi(s_i)^\top \theta_i^{a'})}


where $\phi : \mathcal{S}_i \to \mathbb{R}^m$ is a feature extractor.

3.2 Confidence Calibration Formalism

Definition 3.4 (Perfect Calibration). Agent $A_i$ is perfectly calibrated if:
\mathbb{P}(Y = y_i | \hat{c}_i = p) = p \quad \forall p \in [0,1]

Definition 3.5 (Calibration Error). The expected calibration error (ECE) is:
\text{ECE}_i = \mathbb{E}_{p \sim \hat{c}_i}[|\mathbb{P}(Y = y_i | \hat{c}_i = p) - p|]

We model confidence calibration using temperature scaling:
\hat{c}_i = \sigma\left(\frac{z_i}{T_i}\right)


where $z_i$ is the logit vector, $\sigma$ is softmax, and $T_i > 0$ is the temperature parameter optimized to minimize ECE on validation data.

Definition 3.6 (Global Confidence Normalizer). To address calibration drift across agents, we define:
\hat{c}_i^{\text{global}} = g(\hat{c}_i, \Theta_i, \mathcal{H}_i)


where $g : [0,1] \times \mathbb{R}^{d \times |\mathcal{A}^{\text{ref}}|} \times \mathcal{H} \to [0,1]$ is a learned normalization function satisfying:

1. Monotonicity: $g$ is increasing in $\hat{c}_i$
2. Consistency: If $\text{ECE}_i = \text{ECE}_j$, then $g(\hat{c}_i) \approx g(\hat{c}_j)$ for $\hat{c}_i = \hat{c}_j$
3. Calibration: $\mathbb{P}(Y = y_i | \hat{c}_i^{\text{global}} = p) = p$

3.3 Graph Structure with Reflection-Aware Edges

Definition 3.7 (Meta-Cognitive Agent Graph). An MCAG is defined as:
G = (V, E, E^{\text{meta}}, \Pi^{\text{edge}})


where:

· $V \subseteq \mathcal{A}$ is a set of meta-cognitive agents
· $E \subseteq V \times V$ are standard collaboration edges
· $E^{\text{meta}} \subseteq V \times V \times \mathcal{T}$ are meta-cognitive edges with types $\mathcal{T} = \{\text{conf}, \text{strat}, \text{unc}\}$
· $\Pi^{\text{edge}} = \{\pi_{ij}^t : t \in \mathcal{T}\}$ are edge propagation functions

Definition 3.8 (Confidence Flow Edge). For edge $e_{ij}^{\text{conf}} \in E^{\text{meta}}$:
\mathbf{h}_j' = \mathbf{h}_j + \alpha_{ij}(\hat{c}_i) \cdot \phi_{\text{conf}}(\mathbf{h}_i, \hat{c}_i)


where:

· $\mathbf{h}_i, \mathbf{h}_j \in \mathbb{R}^d$ are hidden states
· $\phi_{\text{conf}} : \mathbb{R}^d \times [0,1] \to \mathbb{R}^d$ is a transformation network
· $\alpha_{ij} : [0,1] \to [0,1]$ is a confidence gate:

\alpha_{ij}(\hat{c}_i) = \sigma\left(\beta_{ij} \cdot (\hat{c}_i - \tau_{ij})\right)

with $\sigma$ sigmoid, $\beta_{ij} > 0$ sensitivity, $\tau_{ij} \in [0,1]$ threshold.

Definition 3.9 (Strategy Suggestion Edge). For edge $e_{ij}^{\text{strat}} \in E^{\text{meta}}$:
\Delta\pi_j^{\text{ref}} = f_{\text{strat}}(a_i^{\text{ref}}, \Theta_i, \Theta_j, \mathbf{h}_i, \mathbf{h}_j)


The receiving agent's policy becomes:
\pi_j^{\text{ref}'}(a|s) = \frac{\exp(\phi(s)^\top (\theta_j^a + \Delta\theta_j^a))}{\sum_{a'}\exp(\phi(s)^\top (\theta_j^{a'} + \Delta\theta_j^{a'}))}

Definition 3.10 (Uncertainty Propagation Edge). For edge $e_{ij}^{\text{unc}} \in E^{\text{meta}}$:
\mathcal{U}_j' = \mathcal{U}_j \oplus f_{\text{unc}}(\mathcal{U}_i, \mathcal{U}_j)


where $\oplus$ is a fusion operator (e.g., mixture, product of experts).

3.4 Dynamic Graph Construction with Reflective States

Definition 3.11 (Graph Construction Policy). The policy $\pi^{\text{graph}}$ maps a query and collective reflective states to a graph:
\pi^{\text{graph}}(G|q, \mathbf{s}^{\text{ref}}) = \pi^{\text{node}}(\mathcal{V}|q, \mathbf{s}^{\text{ref}}) \cdot \pi^{\text{edge}}(\mathcal{E}, \mathcal{E}^{\text{meta}}|q, \mathcal{V}, \mathbf{s}^{\text{ref}})


where $\mathbf{s}^{\text{ref}} = \{s_i^{\text{ref}}\}_{i \in \text{candidates}}$.

We decompose $\pi^{\text{node}}$ as:
\pi^{\text{node}}(\mathcal{V}|q, \mathbf{s}^{\text{ref}}) = \prod_{i=1}^n \text{Bernoulli}(p_i(q, \mathbf{s}^{\text{ref}}))


with inclusion probability:
p_i(q, \mathbf{s}^{\text{ref}}) = \sigma\left(w^\top \psi(q, s_i^{\text{ref}}) + b\right)


where $\psi$ extracts features from query and reflection state.

Definition 3.12 (Reflection-Aware Edge Formation). For potential edge $(i,j)$:
\pi^{\text{edge}}((i,j) \in \mathcal{E}|q, \mathcal{V}, \mathbf{s}^{\text{ref}}) = \sigma\left(\eta_{\text{std}}^\top [\psi(q, s_i^{\text{ref}}); \psi(q, s_j^{\text{ref}})]\right)


\pi^{\text{edge}}((i,j,t) \in \mathcal{E}^{\text{meta}}|q, \mathcal{V}, \mathbf{s}^{\text{ref}}) = \sigma\left(\eta_t^\top [\psi(q, s_i^{\text{ref}}); \psi(q, s_j^{\text{ref}})]\right)

3.5 Execution Semantics

Definition 3.13 (MCAG Execution). Given query $q$ and constructed graph $G$:

1. Initial Reflection: $\forall v_i \in V$, compute:
   s_i^{\text{ref}}(0) = (y_i(0), \hat{c}_i(0), a_i^{\text{ref}}(0), \mathcal{U}_i(0), 0)
2. Meta-Cognitive Propagation: For $t = 1$ to $T_{\text{max}}$:
   · Confidence flow: $\mathbf{h}_j(t) = \mathbf{h}_j(t-1) + \sum_{i: e_{ij}^{\text{conf}} \in E^{\text{meta}}} \alpha_{ij}(\hat{c}_i(t-1)) \cdot \phi_{\text{conf}}(\mathbf{h}_i(t-1), \hat{c}_i(t-1))$
   · Strategy updates: $\pi_j^{\text{ref}}(t) = \pi_j^{\text{ref}}(t-1) + \sum_{i: e_{ij}^{\text{strat}} \in E^{\text{meta}}} \Delta\pi_{ij}^{\text{ref}}(t-1)$
   · Uncertainty fusion: $\mathcal{U}_j(t) = \mathcal{U}_j(t-1) \oplus \bigoplus_{i: e_{ij}^{\text{unc}} \in E^{\text{meta}}} f_{\text{unc}}(\mathcal{U}_i(t-1), \mathcal{U}_j(t-1))$
3. Adaptive Execution: If $\exists i$ with $\hat{c}_i(t) < \tau_{\text{low}}$, trigger additional reflection:
   a_i^{\text{ref}}(t+1) \sim \pi_i^{\text{ref}}(s_i^{\text{ref}}(t))
4. Consensus Formation: When $\max_i \hat{c}_i(t) > \tau_{\text{high}}$ and $\text{Var}(\{\hat{c}_i(t) : y_i = y_{\text{mode}}\}) < \epsilon$, output:
   y_{\text{final}} = \arg\max_{y \in \mathcal{Y}} \sum_{i: y_i = y} \hat{c}_i(t)

Theorem 3.1 (Convergence). Under assumptions:

1. Confidence gates satisfy $\alpha_{ij}(c) \geq \alpha_{\min} > 0$ for $c > \tau_{\text{high}}$
2. Reflection policies are ergodic
3. Graph is strongly connected

Then MCAG execution converges almost surely to a consensus state.

Proof sketch: Model as Markov chain over joint reflection states. Strong connectivity ensures communication between states. Ergodicity ensures eventual transition to high-confidence states. Confidence gating ensures stability once consensus reached.

4. Learning Framework

4.1 Joint Optimization Objective

We jointly optimize agent reflection policies $\{\pi_i^{\text{ref}}\}$ and graph construction policy $\pi^{\text{graph}}$:

Definition 4.1 (Joint Objective). Maximize:
J(\pi^{\text{graph}}, \{\pi_i^{\text{ref}}\}) = \mathbb{E}_{q \sim \mathcal{D}, G \sim \pi^{\text{graph}}(\cdot|q, \mathbf{s}^{\text{ref}})}\left[U(y_G, y^*) - \lambda_1 C_{\text{comp}}(G) - \lambda_2 C_{\text{meta}}(G) + \lambda_3 R_{\text{cal}}(G)\right]

where:

· $U : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ is task utility
· $C_{\text{comp}}(G) = \sum_{v \in V} \text{FLOPs}(M_v) + \sum_{e \in E \cup E^{\text{meta}}} \text{CommCost}(e)$
· $C_{\text{meta}}(G) = \text{Depth}(G^{\text{meta}})$ counts meta-cognitive hops
· $R_{\text{cal}}(G) = -\mathbb{E}_{i \in V}[D_{\text{KL}}(p_i^{\text{ref}} \| p_{\text{calibrated}})]$

Definition 4.2 (Calibration KL Divergence). For agent $A_i$:
D_{\text{KL}}(p_i^{\text{ref}} \| p_{\text{calibrated}}) = \mathbb{E}_{p \sim \hat{c}_i}\left[\mathbb{P}(Y = y_i | \hat{c}_i = p) \log\frac{\mathbb{P}(Y = y_i | \hat{c}_i = p)}{p}\right]

4.2 Two-Stage Training Procedure

Stage 1: Agent Reflection Pre-training

For each agent $A_i$ independently:

1. Reflection Policy Learning: Maximize:
   J_i^{\text{ref}} = \mathbb{E}_{q \sim \mathcal{D}_i}\left[U(y_i, y^*) - \mu \cdot \text{Length}(a_i^{\text{ref}})\right]
   
   using REINFORCE with baseline.
2. Confidence Calibration: Minimize:
   \mathcal{L}_{\text{cal}}^i = \text{ECE}_i + \text{Brier}(p_i, y^*)
   
   where Brier score $\text{Brier}(p, y) = (p - \mathbb{1}_{y=y^*})^2$.

Stage 2: Joint Graph-Agent Learning

Freeze base models $M_i$, optimize:

1. Graph Policy: Using REINFORCE with critic baseline $V(q)$:
   \nabla_{\theta_{\text{graph}}} J \approx \mathbb{E}_{q \sim \mathcal{D}, G \sim \pi^{\text{graph}}}\left[(R(G) - V(q)) \nabla_{\theta_{\text{graph}}} \log \pi^{\text{graph}}(G|q, \mathbf{s}^{\text{ref}})\right]
   
   where $R(G) = U(y_G, y^*) - \lambda_1 C_{\text{comp}}(G)$.
2. Refinement of Reflection Policies: Supervised learning with graph-conditioned targets:
   \mathcal{L}_{\text{refine}}^i = \mathbb{E}_{q,G}\left[\text{CrossEntropy}(\pi_i^{\text{ref}}(s_i), a_i^{\text{ref}*})\right]
   
   where $a_i^{\text{ref}*}$ is the optimal action given graph context.

4.3 Novel Loss Components

Definition 4.3 (Reflection Consistency Loss). For agents $i,j$ in same graph:
\mathcal{L}_{\text{consist}}^{ij} = \mathbb{I}(y_i = y_j) \cdot \|\hat{c}_i - \hat{c}_j\|^2 + \mathbb{I}(y_i \neq y_j) \cdot \beta \cdot \min(\hat{c}_i, \hat{c}_j)

The global consistency loss:
\mathcal{L}_{\text{consist}} = \mathbb{E}_{G \sim \pi^{\text{graph}}, i,j \in V}\left[\mathcal{L}_{\text{consist}}^{ij}\right]

Theorem 4.1 (Consistency Properties). Under $\mathcal{L}_{\text{consist}}$:

1. If $y_i = y_j$, then $\hat{c}_i \to \hat{c}_j$ (agreement convergence)
2. If $y_i \neq y_j$, then $\min(\hat{c}_i, \hat{c}_j) \to 0$ (disagreement resolution)

Proof: Gradient descent on $\mathcal{L}_{\text{consist}}$ gives:
\frac{d\hat{c}_i}{dt} \propto -\frac{\partial \mathcal{L}_{\text{consist}}^{ij}}{\partial \hat{c}_i}


For agreement case: $\frac{d}{dt}(\hat{c}_i - \hat{c}_j) \propto -2(\hat{c}_i - \hat{c}_j)$, exponential convergence.
For disagreement case: $\frac{d\hat{c}_i}{dt} \propto -\beta\mathbb{I}(\hat{c}_i = \min(\hat{c}_i, \hat{c}_j))$, drives lower confidence to zero.

Definition 4.4 (Meta-Edge Effectiveness). For meta-edge type $t \in \mathcal{T}$:
\mathcal{L}_{\text{meta}}^t = -\mathbb{E}_{G,e \in E^{\text{meta}}_t}\left[\Delta U(e)\right]


where $\Delta U(e)$ is utility change when edge $e$ is active vs inactive.

Definition 4.5 (Collaborative Calibration Loss). For the ensemble:
\mathcal{L}_{\text{colcal}} = D_{\text{KL}}\left(p_{\text{ensemble}} \| p_{\text{calibrated}}\right)


where $p_{\text{ensemble}} = \text{softmax}\left(\sum_{i \in V} w_i \cdot \text{logit}(\hat{c}_i)\right)$ with $w_i \propto \hat{c}_i$.

Definition 4.6 (Loop Prevention Penalty). For delegation chains:
\mathcal{L}_{\text{loop}} = \mathbb{E}_{q}\left[\sum_{k=1}^K \gamma^k \cdot \mathbb{I}(\text{cycle}_k(q) \text{ detected})\right]


where $\gamma \in (0,1)$ discounts longer cycles.

4.4 Optimization Theory

Theorem 4.2 (Policy Gradient Variance Reduction). For graph policy $\pi^{\text{graph}}$ with $N$ agents, using per-agent baselines $b_i(q)$ reduces variance from $O(N^2)$ to $O(N)$.

Proof: Standard REINFORCE variance scales with squared number of binary decisions. Decomposing reward as sum of agent contributions $R(G) = \sum_i r_i(G)$ and using baseline $b_i(q) = \mathbb{E}[r_i|q]$, we get:
\text{Var}\left(\sum_i (r_i - b_i) \nabla \log \pi_i\right) = \sum_i \text{Var}((r_i - b_i) \nabla \log \pi_i) = O(N)

Theorem 4.3 (Calibration-Training Compatibility). Under mild conditions, minimizing $\mathcal{L}_{\text{task}} + \lambda \mathcal{L}_{\text{cal}}$ preserves task performance while improving calibration.

Proof: Let $\theta^*_{\text{task}} = \arg\min\mathcal{L}_{\text{task}}$, $\theta^*_{\text{cal}} = \arg\min\mathcal{L}_{\text{cal}}$. Using Taylor expansion around $\theta^*_{\text{task}}$:
\mathcal{L}_{\text{cal}}(\theta) \approx \mathcal{L}_{\text{cal}}(\theta^*_{\text{task}}) + \nabla\mathcal{L}_{\text{cal}}(\theta^*_{\text{task}})^\top (\theta - \theta^*_{\text{task}}) + \frac{1}{2}(\theta - \theta^*_{\text{task}})^\top H_{\text{cal}}(\theta - \theta^*_{\text{task}})


The combined objective has minimum at:
\theta^* = \theta^*_{\text{task}} - \lambda H_{\text{combined}}^{-1} \nabla\mathcal{L}_{\text{cal}}(\theta^*_{\text{task}})


where $H_{\text{combined}} = H_{\text{task}} + \lambda H_{\text{cal}}$. If $\nabla\mathcal{L}_{\text{cal}}(\theta^*_{\text{task}})$ is small (calibration doesn't strongly oppose task learning), then $\theta^* \approx \theta^*_{\text{task}}$.

5. Information-Theoretic Analysis

5.1 Meta-Cognitive Communication as Compression

Theorem 5.1 (Confidence as Sufficient Statistic). For agent $A_i$, confidence $\hat{c}_i$ is a sufficient statistic for the utility of hidden state $h_i$ in collaborative decision-making if:
I(Y; \hat{c}_i, h_i) = I(Y; \hat{c}_i)

Proof: By data processing inequality: $I(Y; \hat{c}_i) \leq I(Y; h_i)$. If equality holds, then $\hat{c}_i$ captures all information in $h_i$ relevant to $Y$.

Corollary 5.1.1 (Compression Ratio). The compression ratio achieved by sharing confidence vs hidden states is:
\rho_i = \frac{\text{Bits}(\hat{c}_i)}{\text{Bits}(h_i)} \approx \frac{\log_2(1/\epsilon)}{\dim(h_i) \cdot \log_2(|\mathcal{H}|)}


where $\epsilon$ is precision for confidence representation.

Definition 5.1 (Meta-Cognitive Bottleneck). For collaborative system with bandwidth constraint $B$, we optimize:
\max_{\{\hat{c}_i\}} I(Y; \{\hat{c}_i\}) \quad \text{s.t.} \quad \sum_i \text{Bits}(\hat{c}_i) \leq B

Theorem 5.2 (Optimal Confidence Allocation). Under bandwidth constraint $B$, optimal confidence bit allocation satisfies:
\frac{\partial I(Y; \hat{c}_i)}{\partial \text{Bits}(\hat{c}_i)} = \lambda \quad \forall i


where $\lambda$ is Lagrange multiplier.

Proof: Standard rate-distortion argument using Lagrangian:
\mathcal{L} = I(Y; \{\hat{c}_i\}) + \lambda(B - \sum_i \text{Bits}(\hat{c}_i))


Stationarity gives equal marginal information per bit.

5.2 Collective Information Dynamics

Definition 5.2 (Collective Information State). For MCAG $G$, the collective information state is:
I_G(t) = (I(Y; \{h_i(t)\}), I(Y; \{\hat{c}_i(t)\}), \text{Var}(\{\hat{c}_i(t)\}))

Theorem 5.3 (Information Growth). Under confidence-gated propagation, collective information evolves as:
\frac{d}{dt}I(Y; \{\hat{c}_i(t)\}) = \sum_{i,j} \alpha_{ij}(\hat{c}_i) \cdot \frac{\partial I(Y; \hat{c}_j)}{\partial \hat{c}_j} \cdot \frac{d\hat{c}_j}{d\mathbf{h}_i} \cdot \frac{d\mathbf{h}_i}{dt}

Proof: Chain rule applied to mutual information gradient.

Definition 5.3 (Meta-Cognitive Efficiency). The efficiency of meta-cognitive communication is:
\eta_{\text{meta}} = \frac{I(Y; \{\hat{c}_i^{\text{post}}\}) - I(Y; \{\hat{c}_i^{\text{pre}}\})}{\sum_{e \in E^{\text{meta}}} \text{Cost}(e)}

Theorem 5.4 (Efficiency Bound). For any meta-cognitive system:
\eta_{\text{meta}} \leq \frac{H(Y)}{\min_{e \in E^{\text{meta}}} \text{Cost}(e)}

Proof: Since $I(Y; \{\hat{c}_i\}) \leq H(Y)$ and costs are positive.

5.3 Uncertainty Propagation Theory

Definition 5.4 (Uncertainty Algebra). Define uncertainty fusion operator $\oplus$ satisfying:

1. Associativity: $( \mathcal{U}_1 \oplus \mathcal{U}_2 ) \oplus \mathcal{U}_3 = \mathcal{U}_1 \oplus ( \mathcal{U}_2 \oplus \mathcal{U}_3 )$
2. Commutativity: $\mathcal{U}_1 \oplus \mathcal{U}_2 = \mathcal{U}_2 \oplus \mathcal{U}_1$
3. Identity: $\exists \mathcal{U}_{\text{id}}$ such that $\mathcal{U} \oplus \mathcal{U}_{\text{id}} = \mathcal{U}$
4. Consistency: If $\mathcal{U}_1, \mathcal{U}_2$ represent same distribution, then $\mathcal{U}_1 \oplus \mathcal{U}_2 \approx \mathcal{U}_1$

Theorem 5.5 (Uncertainty Convergence). For strongly connected uncertainty propagation with associative, commutative $\oplus$, the system converges to:
\mathcal{U}_{\infty} = \bigoplus_{i=1}^n w_i \mathcal{U}_i(0)


where $w_i$ are determined by eigenvector of propagation matrix.

Proof: Model as linear system in appropriate uncertainty representation space. Strong connectivity implies primitive propagation matrix, which converges to stationary distribution by Perron-Frobenius.

6. Robustness Formulations

6.1 Adversarial Models

Definition 6.1 (Adversarial Perturbation). An adversary can apply transformations:

1. Perceptual: $\tilde{x}_i = x_i + \delta_i$, $\|\delta_i\| \leq \epsilon_{\text{perc}}$
2. Confidence: $\tilde{c}_i = c_i + \xi_i$, $\xi_i \sim \mathcal{N}(0, \sigma_{\text{conf}}^2)$
3. Strategy: With probability $p_{\text{sab}}$, force $a_i^{\text{ref}} = a_{\text{wrong}}$

Definition 6.2 (Robustness Decay Ratio). For perturbation level $\epsilon$:
\text{RDR}(\epsilon) = \frac{\text{Accuracy}_{\text{MCAG}}(\epsilon) / \text{Accuracy}_{\text{MCAG}}(0)}{\text{Accuracy}_{\text{Baseline}}(\epsilon) / \text{Accuracy}_{\text{Baseline}}(0)}

Theorem 6.1 (RDR Bound for MCAG). Under confidence-gated propagation:
\text{RDR}(\epsilon) \geq 1 - \frac{\epsilon^2}{\tau_{\text{low}}^2} \cdot \frac{\sum_i \|\nabla_x \hat{c}_i\|^2}{\sum_i \hat{c}_i^2}

Proof: Using Taylor expansion of accuracy as function of confidence, and confidence as function of input. Gating at $\tau_{\text{low}}$ prevents small confidence changes from affecting outputs.

6.2 Calibration under Distribution Shift

Definition 6.3 (Meta-Cognitive Calibration Error). For distribution shift $\mathcal{D}'$:
\text{MCE}(\mathcal{D}') = \sup_{p \in [0,1]} \left|\mathbb{P}_{\mathcal{D}'}(Y = y_{\text{ensemble}} | C_{\text{agg}} = p) - p\right|

where $C_{\text{agg}} = \frac{\sum_i \omega_i \hat{c}_i}{\sum_i \omega_i}$ with $\omega_i = \text{centrality}(i)$.

Theorem 6.2 (MCE Transfer Bound). For shift from $\mathcal{D}$ to $\mathcal{D}'$:
\text{MCE}(\mathcal{D}') \leq \text{MCE}(\mathcal{D}) + \text{TV}(\mathcal{D}, \mathcal{D}') + \sup_i |\text{ECE}_i(\mathcal{D}') - \text{ECE}_i(\mathcal{D})|

where TV is total variation distance.

Proof: Triangle inequality on calibration error difference plus distribution distance.

6.3 Stability Analysis

Definition 6.4 (Meta-Cognitive Stability). System is $(\epsilon,\delta)$-stable if for perturbations $\|\Delta x\| \leq \epsilon$:
\mathbb{P}(\| \Delta y_{\text{final}} \| > \delta) \leq \delta

Theorem 6.3 (Stability Condition). MCAG is $(\epsilon,\delta)$-stable if:

1. Confidence Lipschitz: $|\hat{c}_i(x) - \hat{c}_i(x')| \leq L_c \|x - x'\|\ \forall i$
2. Gate thresholds satisfy: $\tau_{\text{low}} > L_c \epsilon$
3. Consensus threshold: $\tau_{\text{high}} - \tau_{\text{low}} > 2L_c \epsilon$

Proof: Under conditions, confidence ordering preserved, same consensus reached.

7. Theoretical Implications

7.1 Computational Transactive Memory Systems

Definition 7.1 (Computational TMS). A system implements computational TMS if:

1. Specialization: $\exists$ mapping $f : \mathcal{Q} \to \Delta(\mathcal{A})$ assigning problems to agents
2. Credibility: $\exists$ mapping $g : \mathcal{A} \times \mathcal{Q} \to [0,1]$ estimating agent reliability
3. Coordination: $\exists$ mechanism for reallocation when $g(a,q) < \tau$

Theorem 7.1 (MCAG as TMS). MCAG implements computational TMS with:

· $f(q) = \pi^{\text{node}}(\cdot|q, \mathbf{s}^{\text{ref}})$
· $g(a,q) = \hat{c}_a(q)$
· Reallocation via $\langle \text{delegate} \rangle$ action

Corollary 7.1.1 (Optimal Specialization). Under information bottleneck constraints, MCAG's specialization approaches optimal TMS when:
I(Y; f(Q)) \to I(Y; Q)

7.2 Distributed Metacognition

Definition 7.2 (Distributed Metacognitive System). A system exhibits distributed metacognition if:

1. Individual Monitoring: $\forall a \in \mathcal{A}$, can estimate $P(\text{correct}|a)$
2. Collective Monitoring: System can estimate $P(\text{correct}|\mathcal{A})$
3. Control: Can adjust behavior based on monitoring

Theorem 7.2 (MCAG Metacognitive Capacity). MCAG's distributed metacognitive capacity is:
C_{\text{meta}} = I(\{\hat{c}_i\}; \{\mathbb{1}_{y_i=y^*}\})

Proof: Mutual information between confidence signals and correctness measures monitoring accuracy.

7.3 Emergent Organization Principles

Definition 7.3 (Meta-Cognitive Organization). The emergent graph structure follows principles:

1. Confidence Homophily: $P((i,j) \in E^{\text{meta}}) \propto \exp(-\lambda|\hat{c}_i - \hat{c}_j|)$
2. Competence Complementarity: $P((i,j) \in E) \propto \exp(\mu \cdot I(Y; y_i, y_j) - I(Y; y_i) - I(Y; y_j))$
3. Reflective Hierarchy: $\exists$ partial order $\prec$ where $i \prec j$ if $j$ delegates to $i$

Theorem 7.3 (Optimal Organization). Under cost constraints, the organization that maximizes:
U_{\text{org}} = I(Y; \text{Output}) - \lambda C_{\text{org}}


satisfies the three principles with specific $\lambda,\mu$.

Proof sketch: Derive from rate-distortion theory with two types of information: task-relevant and confidence-relevant.

8. Complexity Analysis

8.1 Computational Complexity

Theorem 8.1 (Time Complexity). For MCAG with $n$ agents, maximum reflection depth $d$, and $m$ meta-edges per agent:

· Inference: $O(n \cdot (T_{\text{base}} + d \cdot T_{\text{ref}} + m \cdot T_{\text{meta}}))$
· Graph construction: $O(n^2 \cdot T_{\text{feat}})$
· Training: $O(n \cdot (T_{\text{pretrain}} + T_{\text{joint}}))$

where $T_{\text{base}}, T_{\text{ref}}, T_{\text{meta}}, T_{\text{feat}}, T_{\text{pretrain}}, T_{\text{joint}}$ are operation counts.

Theorem 8.2 (Space Complexity). MCAG requires:

· Model parameters: $O(\sum_i |\theta_i| + |\theta_{\text{graph}}|)$
· Execution state: $O(n \cdot d \cdot s_{\text{state}})$
· Training: $O(n \cdot b \cdot s_{\text{grad}})$ for batch size $b$

8.2 Communication Complexity

Definition 8.1 (Meta-Cognitive Communication Cost). For graph $G$:
C_{\text{comm}}(G) = \sum_{e \in E} c_{\text{std}} + \sum_{e \in E^{\text{meta}}} c_{\text{meta}}(t_e)

Theorem 8.3 (Optimal Communication). Under accuracy constraint $A_{\min}$, minimum communication cost is achieved when:
\frac{\partial A}{\partial c_{\text{meta}}(t)} = \lambda \quad \forall t \in \mathcal{T}

Corollary 8.3.1 (Confidence-Weighted Routing). Optimal routing allocates bandwidth proportional to $\hat{c}_i^{1/\alpha}$ where $\alpha$ depends on channel.

8.3 Scalability Analysis

Definition 8.2 (Scale Efficiency). System scales efficiently if:
\lim_{n \to \infty} \frac{\text{Performance}(n)}{\text{Cost}(n)} > 0

Theorem 8.4 (MCAG Scalability). For $n$ agents with i.i.d. capabilities:

· If $\text{Cost}(n) = O(n \log n)$, then scale efficient
· If $\text{Cost}(n) = \omega(n \log n)$, not scale efficient

Proof: Using submodularity of information gain from additional agents.

9. Conclusion

We have presented a comprehensive mathematical framework for Meta-Cognitive Agent Graphs, establishing formal foundations for self-reflective agents in heterogeneous collaborative networks. Key contributions include:

1. Formal Agent Model: Precise definition of meta-cognitive agents with reflection policies, confidence calibration, and strategy embeddings.
2. Graph Theory Extensions: Formalization of reflection-aware edges and dynamic graph construction based on collective reflective states.
3. Optimization Theory: Joint learning framework with novel loss functions ensuring calibration consistency and preventing pathological behaviors.
4. Information-Theoretic Analysis: Formal characterization of meta-cognitive communication as efficient compression and analysis of collective information dynamics.
5. Robustness Formulations: Mathematical definitions of adversarial resilience and calibration under distribution shift.
6. Complexity Bounds: Computational and communication complexity analysis with scalability results.

The MCAG framework bridges AI systems engineering with cognitive science, implementing computational versions of transactive memory and distributed metacognition. Future work includes extending the theory to hierarchical reflection, cross-modal transfer, and autonomous discovery of reflection strategies.

This theoretical foundation enables rigorous analysis of collaborative AI systems and provides mathematical tools for designing more robust, efficient, and interpretable multi-agent architectures.

---

Key Theoretical Results Summary:

1. Convergence: MCAG execution converges to consensus under connectivity and ergodicity conditions.
2. Calibration Compatibility: Task learning and calibration can be jointly optimized without significant trade-off.
3. Information Compression: Confidence scores serve as sufficient statistics for collaborative decision-making.
4. Robustness: Confidence gating provides formal guarantees against adversarial perturbations.
5. Optimal Organization: Emergent graph structures optimize information flow under communication constraints.

The mathematical framework presented here provides a foundation for both theoretical analysis and practical implementation of socially intelligent AI systems capable of collaborative self-reflection.


Meta-Cognitive Agent Graphs: Technical Architecture & Implementation

Project Structure & Codebase Architecture

```
mcag-framework/
│
├── README.md
├── pyproject.toml
├── requirements.txt
├── setup.py
│
├── configs/
│   ├── agents/
│   │   ├── speech_agent.yaml
│   │   ├── vision_agent.yaml
│   │   └── reasoning_agent.yaml
│   ├── graphs/
│   │   ├── small_graph.yaml
│   │   ├── medium_graph.yaml
│   │   └── large_graph.yaml
│   └── training/
│       ├── pretrain.yaml
│       └── joint_train.yaml
│
├── docs/
│   ├── api/
│   ├── tutorials/
│   └── theory/
│
├── src/
│   └── mcag/
│       ├── __init__.py
│       ├── core/
│       │   ├── __init__.py
│       │   ├── definitions.py          # Formal definitions and types
│       │   ├── agent.py               # Agent base classes
│       │   ├── graph.py               # Graph structures and operations
│       │   └── execution.py           # Execution engine
│       │
│       ├── agents/
│       │   ├── __init__.py
│       │   ├── base_agent.py          # Abstract agent interface
│       │   ├── meta_agent.py          # Meta-cognitive agent implementation
│       │   ├── reflection_policy.py   # Reflection policy networks
│       │   └── calibrator.py          # Confidence calibration models
│       │
│       ├── graphs/
│       │   ├── __init__.py
│       │   ├── graph_builder.py       # Dynamic graph construction
│       │   ├── edge_types.py          # Standard and meta-cognitive edges
│       │   ├── propagation.py         # Information propagation rules
│       │   └── topology.py            # Graph topology utilities
│       │
│       ├── learning/
│       │   ├── __init__.py
│       │   ├── objectives.py          # Loss functions and objectives
│       │   ├── training.py            # Training procedures
│       │   ├── optimizers.py          # Custom optimizers
│       │   └── curriculum.py          # Training curriculum
│       │
│       ├── utils/
│       │   ├── __init__.py
│       │   ├── math_utils.py          # Mathematical utilities
│       │   ├── communication.py       # Communication cost tracking
│       │   ├── metrics.py             # Evaluation metrics
│       │   └── visualization.py       # Visualization tools
│       │
│       └── theory/
│           ├── __init__.py
│           ├── information.py         # Information-theoretic analysis
│           ├── convergence.py         # Convergence proofs
│           ├── robustness.py          # Robustness analysis
│           └── complexity.py          # Complexity analysis
│
├── tests/
│   ├── unit/
│   │   ├── test_agents.py
│   │   ├── test_graphs.py
│   │   ├── test_learning.py
│   │   └── test_math.py
│   ├── integration/
│   │   ├── test_execution.py
│   │   └── test_training.py
│   └── property/
│       └── test_properties.py         # Property-based tests
│
├── examples/
│   ├── basic_usage.py
│   ├── custom_agent.py
│   ├── graph_construction.py
│   └── adversarial_tests.py
│
├── notebooks/
│   ├── 01_introduction.ipynb
│   ├── 02_agent_reflection.ipynb
│   ├── 03_graph_construction.ipynb
│   ├── 04_training_demo.ipynb
│   └── 05_analysis.ipynb
│
└── scripts/
    ├── train_pretrain.py
    ├── train_joint.py
    ├── evaluate.py
    └── analyze_results.py
```

Core Technical Components

1. Base Agent Interface (src/mcag/agents/base_agent.py)

```python
"""
Base agent interface with formal type signatures.
"""

from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Optional, Tuple, Dict, Any
from dataclasses import dataclass
import torch
import torch.nn as nn
import torch.nn.functional as F

T = TypeVar('T')  # Input type
U = TypeVar('U')  # Output type
S = TypeVar('S')  # State type

@dataclass
class AgentState:
    """Formal state representation for agents."""
    hidden: torch.Tensor  # Hidden state h ∈ ℝ^d
    confidence: torch.Tensor  # Confidence c ∈ [0, 1]
    uncertainty: torch.Tensor  # Uncertainty distribution U ∈ Δ(Y)
    strategy: torch.Tensor  # Strategy embedding θ ∈ ℝ^m
    timestamp: int  # Reflection depth t ∈ ℕ

class BaseAgent(ABC, Generic[T, U, S]):
    """Abstract base agent following Definition 2.1."""
    
    def __init__(self, 
                 agent_id: str,
                 input_dim: int,
                 output_dim: int,
                 hidden_dim: int = 512):
        self.agent_id = agent_id
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        
    @abstractmethod
    def forward(self, x: T) -> Tuple[U, AgentState]:
        """Forward pass: x → (y, s)."""
        pass
    
    @abstractmethod
    def compute_loss(self, 
                    predictions: U,
                    targets: U,
                    state: AgentState) -> torch.Tensor:
        """Compute task-specific loss."""
        pass
    
    @abstractmethod
    def get_parameters(self) -> Dict[str, torch.Tensor]:
        """Get all trainable parameters."""
        pass
```

2. Meta-Cognitive Agent Implementation (src/mcag/agents/meta_agent.py)

```python
"""
Implementation of meta-cognitive agents following Definition 3.1.
"""

import torch
import torch.nn as nn
import torch.distributions as dist
from typing import Dict, List, Tuple, Optional
from enum import Enum

class ReflectionAction(Enum):
    """Reflection action space following Definition 3.1."""
    INTERNAL = "internal"
    EXTERNAL = "external"
    REWRITE = "rewrite"
    TOOL = "tool"
    DELEGATE = "delegate"

class MetaCognitiveAgent(BaseAgent):
    """
    Meta-cognitive agent implementation.
    
    Implements: A_i = (M_i, R_i, C_i, Θ_i, Γ_i)
    """
    
    def __init__(self,
                 agent_id: str,
                 base_model: nn.Module,
                 input_dim: int,
                 output_dim: int,
                 hidden_dim: int = 512,
                 num_strategies: int = 5,
                 calibration_history_size: int = 1000):
        super().__init__(agent_id, input_dim, output_dim, hidden_dim)
        
        # M_i: Base model
        self.base_model = base_model
        
        # R_i: Reflection policy network
        self.reflection_policy = ReflectionPolicy(
            state_dim=hidden_dim,
            action_dim=num_strategies
        )
        
        # C_i: Confidence calibrator with temperature scaling
        self.confidence_calibrator = ConfidenceCalibrator(
            input_dim=hidden_dim,
            history_size=calibration_history_size
        )
        
        # Θ_i: Strategy embeddings
        self.strategy_embeddings = nn.Embedding(
            num_embeddings=num_strategies,
            embedding_dim=hidden_dim
        )
        
        # Γ_i: Trust parameters for other agents
        self.trust_parameters = nn.ParameterDict()
        
        # State tracking
        self.history = []
        self.reflection_depth = 0
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, AgentState]:
        """
        Forward pass with reflection.
        
        Implements the reflection process:
        1. Base model inference
        2. Confidence calibration
        3. Reflection action selection
        4. State update
        """
        # Base model computation
        hidden = self.base_model.encode(x)  # h ∈ ℝ^d
        logits = self.base_model.decode(hidden)
        y_pred = F.softmax(logits, dim=-1)
        
        # Confidence calibration (temperature scaling)
        confidence = self.confidence_calibrator(hidden)  # ĉ ∈ [0, 1]
        
        # Uncertainty estimation (Dirichlet distribution)
        uncertainty = self._compute_uncertainty(logits)
        
        # Reflection action selection
        reflection_state = self._create_reflection_state(hidden, confidence, y_pred)
        action_probs = self.reflection_policy(reflection_state)
        action_dist = dist.Categorical(action_probs)
        action = action_dist.sample()
        
        # Update state
        state = AgentState(
            hidden=hidden,
            confidence=confidence,
            uncertainty=uncertainty,
            strategy=self.strategy_embeddings(action),
            timestamp=self.reflection_depth
        )
        
        # Store for training
        self.history.append({
            'state': reflection_state,
            'action': action,
            'confidence': confidence,
            'output': y_pred
        })
        
        self.reflection_depth += 1
        
        return y_pred, state
    
    def _create_reflection_state(self, 
                                 hidden: torch.Tensor,
                                 confidence: torch.Tensor,
                                 output: torch.Tensor) -> torch.Tensor:
        """Create reflection state vector."""
        # Concatenate features for reflection policy
        return torch.cat([
            hidden,
            confidence.unsqueeze(-1),
            output.mean(dim=-1, keepdim=True)  # Output entropy proxy
        ], dim=-1)
    
    def _compute_uncertainty(self, logits: torch.Tensor) -> torch.Tensor:
        """
        Compute uncertainty distribution U ∈ Δ(Y).
        
        Uses Dirichlet distribution for uncertainty estimation:
        α = exp(logits) + 1
        U = Dirichlet(α)
        """
        # Convert to concentration parameters
        concentration = torch.exp(logits) + 1.0
        dirichlet = dist.Dirichlet(concentration)
        
        # Sample from distribution for uncertainty representation
        return dirichlet.rsample()
    
    def apply_meta_edge_effect(self,
                              edge_type: str,
                              sender_state: AgentState,
                              receiver_state: AgentState) -> AgentState:
        """
        Apply meta-cognitive edge effect.
        
        Implements edge propagation rules from Definitions 3.8-3.10.
        """
        if edge_type == 'confidence':
            return self._apply_confidence_edge(sender_state, receiver_state)
        elif edge_type == 'strategy':
            return self._apply_strategy_edge(sender_state, receiver_state)
        elif edge_type == 'uncertainty':
            return self._apply_uncertainty_edge(sender_state, receiver_state)
        else:
            raise ValueError(f"Unknown edge type: {edge_type}")
    
    def _apply_confidence_edge(self,
                              sender: AgentState,
                              receiver: AgentState) -> AgentState:
        """
        Apply confidence flow edge.
        
        Implements: h_j' = h_j + α_{ij}(ĉ_i) · φ_conf(h_i, ĉ_i)
        """
        # Confidence gate α_{ij}(c)
        confidence_gate = self._confidence_gate(sender.confidence)
        
        # Transformation φ_conf
        transformed = self.confidence_transformer(
            torch.cat([sender.hidden, sender.confidence.unsqueeze(-1)], dim=-1)
        )
        
        # Update hidden state
        new_hidden = receiver.hidden + confidence_gate * transformed
        
        return AgentState(
            hidden=new_hidden,
            confidence=receiver.confidence,
            uncertainty=receiver.uncertainty,
            strategy=receiver.strategy,
            timestamp=receiver.timestamp
        )
    
    def _confidence_gate(self, confidence: torch.Tensor) -> torch.Tensor:
        """
        Confidence gate function α(c) = σ(β·(c - τ))
        
        Implements the gating mechanism from Section 3.5.
        """
        beta = nn.Parameter(torch.tensor(1.0))  # Sensitivity
        tau = nn.Parameter(torch.tensor(0.5))   # Threshold
        
        gate = torch.sigmoid(beta * (confidence - tau))
        return gate
```

3. Reflection Policy Network (src/mcag/agents/reflection_policy.py)

```python
"""
Reflection policy network implementing π_i^ref.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple

class ReflectionPolicy(nn.Module):
    """
    Reflection policy network.
    
    Implements: π_i^ref(a|s) = exp(φ(s)^⊤θ_i^a) / Σ_{a'} exp(φ(s)^⊤θ_i^{a'})
    """
    
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 hidden_dim: int = 256,
                 num_layers: int = 3):
        super().__init__()
        
        # Feature extractor φ: S → ℝ^m
        layers = []
        current_dim = state_dim
        for _ in range(num_layers - 1):
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            current_dim = hidden_dim
        
        self.feature_extractor = nn.Sequential(*layers)
        
        # Strategy embeddings Θ_i = {θ_i^a}
        self.strategy_embeddings = nn.Embedding(
            num_embeddings=action_dim,
            embedding_dim=current_dim
        )
        
        # Final projection
        self.projection = nn.Linear(current_dim, current_dim)
        
        # Initialize with small values
        nn.init.normal_(self.strategy_embeddings.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.projection.weight, mean=0.0, std=0.02)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        Compute reflection action probabilities.
        
        Args:
            state: Batch of states [batch_size, state_dim]
            
        Returns:
            Action probabilities [batch_size, action_dim]
        """
        # Extract features φ(s)
        features = self.feature_extractor(state)  # [batch_size, hidden_dim]
        projected = self.projection(features)     # [batch_size, hidden_dim]
        
        # Compute logits: φ(s)^⊤θ_i^a
        embeddings = self.strategy_embeddings.weight  # [action_dim, hidden_dim]
        logits = torch.matmul(projected, embeddings.T)  # [batch_size, action_dim]
        
        # Convert to probabilities
        action_probs = F.softmax(logits, dim=-1)
        
        return action_probs
    
    def update_with_suggestion(self,
                               suggestion: torch.Tensor,
                               strength: float = 0.1) -> None:
        """
        Update strategy embeddings with external suggestion.
        
        Implements: θ_j^a' = θ_j^a + Δθ_j^a
        """
        with torch.no_grad():
            self.strategy_embeddings.weight += strength * suggestion
```

4. Confidence Calibrator (src/mcag/agents/calibrator.py)

```python
"""
Confidence calibration module implementing Definition 3.4-3.6.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple
import numpy as np
from collections import deque

class ConfidenceCalibrator(nn.Module):
    """
    Confidence calibration module with temperature scaling.
    
    Implements global confidence normalization to address calibration drift.
    """
    
    def __init__(self,
                 input_dim: int,
                 history_size: int = 1000,
                 num_bins: int = 10):
        super().__init__()
        
        # Temperature parameter for scaling
        self.temperature = nn.Parameter(torch.ones(1))
        
        # Global normalizer network g
        self.global_normalizer = nn.Sequential(
            nn.Linear(input_dim + 1, 64),  # +1 for raw confidence
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        
        # Calibration history
        self.history = deque(maxlen=history_size)
        self.num_bins = num_bins
        
        # Calibration statistics
        self.register_buffer('confidence_sum', torch.zeros(num_bins))
        self.register_buffer('accuracy_sum', torch.zeros(num_bins))
        self.register_buffer('counts', torch.zeros(num_bins))
    
    def forward(self, hidden: torch.Tensor, raw_confidence: torch.Tensor = None) -> torch.Tensor:
        """
        Compute calibrated confidence.
        
        Args:
            hidden: Hidden state h ∈ ℝ^d
            raw_confidence: Raw confidence if available
            
        Returns:
            Calibrated confidence ĉ ∈ [0, 1]
        """
        if raw_confidence is None:
            # Compute raw confidence from hidden state
            raw_confidence = self._compute_raw_confidence(hidden)
        
        # Temperature scaling
        scaled_confidence = torch.sigmoid(
            torch.logit(raw_confidence) / self.temperature
        )
        
        # Global normalization
        features = torch.cat([hidden, scaled_confidence.unsqueeze(-1)], dim=-1)
        calibrated = self.global_normalizer(features).squeeze(-1)
        
        # Clamp to valid range
        calibrated = torch.clamp(calibrated, 0.0, 1.0)
        
        return calibrated
    
    def _compute_raw_confidence(self, hidden: torch.Tensor) -> torch.Tensor:
        """Compute raw confidence from hidden state."""
        # Simple MLP for confidence estimation
        confidence_logit = self.confidence_estimator(hidden)
        raw_confidence = torch.sigmoid(confidence_logit)
        return raw_confidence
    
    def update_calibration(self,
                          predictions: torch.Tensor,
                          targets: torch.Tensor,
                          confidences: torch.Tensor) -> None:
        """
        Update calibration statistics.
        
        Implements calibration tracking for ECE calculation.
        """
        # Bin confidences
        bin_indices = torch.floor(confidences * self.num_bins).long()
        bin_indices = torch.clamp(bin_indices, 0, self.num_bins - 1)
        
        # Compute accuracies
        correct = (predictions.argmax(dim=-1) == targets).float()
        
        # Update statistics
        for i in range(self.num_bins):
            mask = (bin_indices == i)
            if mask.any():
                self.confidence_sum[i] += confidences[mask].sum()
                self.accuracy_sum[i] += correct[mask].sum()
                self.counts[i] += mask.sum()
    
    def compute_ece(self) -> torch.Tensor:
        """Compute Expected Calibration Error."""
        # Avoid division by zero
        valid_bins = self.counts > 0
        if not valid_bins.any():
            return torch.tensor(0.0)
        
        conf_means = self.confidence_sum[valid_bins] / self.counts[valid_bins]
        acc_means = self.accuracy_sum[valid_bins] / self.counts[valid_bins]
        
        weights = self.counts[valid_bins] / self.counts.sum()
        ece = (weights * torch.abs(acc_means - conf_means)).sum()
        
        return ece
```

5. Graph Structure (src/mcag/graphs/graph_builder.py)

```python
"""
Dynamic graph construction implementing Definitions 3.11-3.12.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Set, Optional
import networkx as nx
from dataclasses import dataclass

@dataclass
class GraphConfig:
    """Configuration for graph construction."""
    max_nodes: int = 10
    max_meta_edges_per_node: int = 3
    confidence_threshold: float = 0.5
    uncertainty_threshold: float = 0.3
    use_confidence_gating: bool = True
    use_strategy_suggestions: bool = True
    use_uncertainty_propagation: bool = True

class GraphBuilder(nn.Module):
    """
    Dynamic graph construction policy.
    
    Implements: π^graph(G|q, s^ref) = π^node(V|q, s^ref) · π^edge(E, E^meta|q, V, s^ref)
    """
    
    def __init__(self,
                 agent_feature_dim: int,
                 query_feature_dim: int,
                 hidden_dim: int = 256,
                 config: GraphConfig = None):
        super().__init__()
        
        self.config = config or GraphConfig()
        
        # Node inclusion policy π^node
        self.node_policy = nn.Sequential(
            nn.Linear(agent_feature_dim + query_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # Edge formation policies
        self.edge_policies = nn.ModuleDict({
            'standard': self._create_edge_policy(agent_feature_dim, query_feature_dim),
            'confidence': self._create_edge_policy(agent_feature_dim, query_feature_dim),
            'strategy': self._create_edge_policy(agent_feature_dim, query_feature_dim),
            'uncertainty': self._create_edge_policy(agent_feature_dim, query_feature_dim)
        })
        
        # Feature extractor for queries and states
        self.query_encoder = nn.Sequential(
            nn.Linear(query_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        self.agent_encoder = nn.Sequential(
            nn.Linear(agent_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
    
    def _create_edge_policy(self, agent_dim: int, query_dim: int) -> nn.Module:
        """Create edge formation policy network."""
        return nn.Sequential(
            nn.Linear(2 * agent_dim + query_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self,
                query: torch.Tensor,
                agent_states: Dict[str, Dict]) -> Tuple[nx.DiGraph, Dict]:
        """
        Construct graph based on query and agent states.
        
        Args:
            query: Query features [query_dim]
            agent_states: Dict mapping agent_id → state dict
            
        Returns:
            graph: Constructed NetworkX graph
            metadata: Construction metadata
        """
        # Encode query
        query_features = self.query_encoder(query)
        
        # Node selection
        selected_nodes = self._select_nodes(query_features, agent_states)
        
        # Edge formation
        graph = self._form_edges(query_features, selected_nodes, agent_states)
        
        # Add meta-cognitive edges
        if self.config.use_confidence_gating:
            self._add_confidence_edges(graph, agent_states)
        
        if self.config.use_strategy_suggestions:
            self._add_strategy_edges(graph, agent_states)
        
        if self.config.use_uncertainty_propagation:
            self._add_uncertainty_edges(graph, agent_states)
        
        return graph
    
    def _select_nodes(self,
                      query_features: torch.Tensor,
                      agent_states: Dict[str, Dict]) -> List[str]:
        """
        Select nodes using π^node.
        
        Implements node inclusion probabilities.
        """
        node_probs = {}
        selected_nodes = []
        
        for agent_id, state in agent_states.items():
            # Extract agent features
            agent_feats = self._extract_agent_features(state)
            
            # Concatenate with query features
            combined = torch.cat([agent_feats, query_features])
            
            # Compute inclusion probability
            prob = self.node_policy(combined.unsqueeze(0)).squeeze()
            node_probs[agent_id] = prob.item()
            
            # Sample inclusion
            if prob > 0.5:  # Threshold for inclusion
                selected_nodes.append(agent_id)
        
        # Limit to max_nodes
        if len(selected_nodes) > self.config.max_nodes:
            # Select top-k by probability
            sorted_nodes = sorted(node_probs.items(), 
                                 key=lambda x: x[1], 
                                 reverse=True)
            selected_nodes = [node for node, _ in 
                             sorted_nodes[:self.config.max_nodes]]
        
        return selected_nodes
    
    def _form_edges(self,
                    query_features: torch.Tensor,
                    selected_nodes: List[str],
                    agent_states: Dict[str, Dict]) -> nx.DiGraph:
        """
        Form edges between selected nodes.
        """
        graph = nx.DiGraph()
        
        # Add nodes
        for node in selected_nodes:
            graph.add_node(node, **agent_states[node])
        
        # Form edges
        for i, node_i in enumerate(selected_nodes):
            for j, node_j in enumerate(selected_nodes):
                if i == j:
                    continue
                
                # Get agent features
                feat_i = self._extract_agent_features(agent_states[node_i])
                feat_j = self._extract_agent_features(agent_states[node_j])
                
                # Compute edge probability for each type
                for edge_type, policy in self.edge_policies.items():
                    # Concatenate features
                    combined = torch.cat([feat_i, feat_j, query_features])
                    
                    # Compute probability
                    prob = policy(combined.unsqueeze(0)).squeeze()
                    
                    # Add edge if probability exceeds threshold
                    if prob > 0.3:  # Edge formation threshold
                        graph.add_edge(node_i, node_j, 
                                      type=edge_type,
                                      weight=prob.item(),
                                      probability=prob.item())
        
        return graph
    
    def _add_confidence_edges(self,
                             graph: nx.DiGraph,
                             agent_states: Dict[str, Dict]) -> None:
        """Add confidence flow edges based on confidence levels."""
        for node_i in graph.nodes():
            for node_j in graph.nodes():
                if node_i == node_j:
                    continue
                
                conf_i = agent_states[node_i].get('confidence', 0.5)
                conf_j = agent_states[node_j].get('confidence', 0.5)
                
                # Add confidence edge if confidence difference is significant
                if abs(conf_i - conf_j) > 0.2:  # Threshold
                    weight = 1.0 - abs(conf_i - conf_j)  # Higher weight for similar confidence
                    graph.add_edge(node_i, node_j,
                                  type='confidence_flow',
                                  weight=weight,
                                  confidence_diff=conf_i - conf_j)
```

6. Loss Functions (src/mcag/learning/objectives.py)

```python
"""
Loss functions implementing Section 4.3.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
import numpy as np

class ReflectionConsistencyLoss(nn.Module):
    """
    Reflection consistency loss.
    
    Implements: L_consist = E[||ĉ_i - ĉ_j||^2 · I(y_i = y_j)] 
                + β · E[min(ĉ_i, ĉ_j) · I(y_i ≠ y_j)]
    """
    
    def __init__(self, beta: float = 1.0, reduction: str = 'mean'):
        super().__init__()
        self.beta = beta
        self.reduction = reduction
        
    def forward(self,
                agent_outputs: Dict[str, torch.Tensor],
                agent_confidences: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Compute consistency loss.
        
        Args:
            agent_outputs: Dict mapping agent_id → output predictions
            agent_confidences: Dict mapping agent_id → confidence scores
            
        Returns:
            Consistency loss
        """
        agent_ids = list(agent_outputs.keys())
        n_agents = len(agent_ids)
        
        if n_agents < 2:
            return torch.tensor(0.0, device=next(iter(agent_outputs.values())).device)
        
        total_loss = 0.0
        pair_count = 0
        
        for i in range(n_agents):
            for j in range(i + 1, n_agents):
                id_i, id_j = agent_ids[i], agent_ids[j]
                
                # Get outputs and confidences
                y_i = agent_outputs[id_i]
                y_j = agent_outputs[id_j]
                c_i = agent_confidences[id_i]
                c_j = agent_confidences[id_j]
                
                # Check if outputs agree
                if self._outputs_agree(y_i, y_j):
                    # Agreement term
                    loss = F.mse_loss(c_i, c_j, reduction='none')
                else:
                    # Disagreement term
                    min_confidence = torch.min(c_i, c_j)
                    loss = self.beta * min_confidence
                
                total_loss += loss.mean()
                pair_count += 1
        
        if pair_count == 0:
            return torch.tensor(0.0, device=total_loss.device)
        
        if self.reduction == 'mean':
            return total_loss / pair_count
        else:
            return total_loss
    
    def _outputs_agree(self, y1: torch.Tensor, y2: torch.Tensor) -> torch.Tensor:
        """Check if two outputs agree."""
        if y1.dim() > 1 and y1.size(-1) > 1:  # Classification
            pred1 = y1.argmax(dim=-1)
            pred2 = y2.argmax(dim=-1)
            return pred1 == pred2
        else:  # Regression or binary
            return torch.isclose(y1, y2, rtol=1e-3)

class CollaborativeCalibrationLoss(nn.Module):
    """
    Collaborative calibration loss.
    
    Implements: L_colcal = D_KL(p_ensemble || p_calibrated)
    """
    
    def __init__(self, num_bins: int = 10):
        super().__init__()
        self.num_bins = num_bins
        
    def forward(self,
                ensemble_confidence: torch.Tensor,
                accuracy: torch.Tensor) -> torch.Tensor:
        """
        Compute KL divergence between ensemble confidence and actual accuracy.
        
        Args:
            ensemble_confidence: Confidence scores [batch_size]
            accuracy: Actual accuracy (0 or 1) [batch_size]
            
        Returns:
            KL divergence loss
        """
        # Bin the confidences
        bins = torch.linspace(0, 1, self.num_bins + 1)
        
        # Compute empirical distribution in bins
        bin_indices = torch.bucketize(ensemble_confidence, bins) - 1
        bin_indices = torch.clamp(bin_indices, 0, self.num_bins - 1)
        
        # Count samples in each bin
        bin_counts = torch.zeros(self.num_bins, 
                                device=ensemble_confidence.device)
        for i in range(self.num_bins):
            mask = (bin_indices == i)
            if mask.any():
                bin_counts[i] = mask.float().sum()
        
        # Normalize to get probability distribution
        p_ensemble = bin_counts / bin_counts.sum()
        
        # Compute accuracy in each bin
        bin_accuracy = torch.zeros(self.num_bins,
                                  device=ensemble_confidence.device)
        for i in range(self.num_bins):
            mask = (bin_indices == i)
            if mask.any():
                bin_accuracy[i] = accuracy[mask].float().mean()
        
        # Avoid zeros for KL divergence
        p_ensemble = torch.clamp(p_ensemble, 1e-10, 1.0)
        bin_accuracy = torch.clamp(bin_accuracy, 1e-10, 1.0)
        
        # Compute KL divergence
        kl_div = F.kl_div(
            torch.log(p_ensemble),
            bin_accuracy,
            reduction='batchmean'
        )
        
        return kl_div

class MetaEdgeEffectivenessLoss(nn.Module):
    """
    Meta-edge effectiveness loss.
    
    Implements: L_meta = -E[ΔU | meta-edge active]
    """
    
    def __init__(self, discount_factor: float = 0.99):
        super().__init__()
        self.discount_factor = discount_factor
        
    def forward(self,
                utility_with_edge: torch.Tensor,
                utility_without_edge: torch.Tensor,
                edge_active: torch.Tensor) -> torch.Tensor:
        """
        Compute effectiveness loss.
        
        Args:
            utility_with_edge: Utility when meta-edge is active
            utility_without_edge: Utility when meta-edge is inactive
            edge_active: Binary indicator of edge activation
            
        Returns:
            Effectiveness loss
        """
        # Compute utility difference
        delta_utility = utility_with_edge - utility_without_edge
        
        # Mask by edge activation
        masked_delta = delta_utility * edge_active
        
        # Compute loss (negative because we want to maximize utility)
        loss = -masked_delta.mean()
        
        return loss
```

7. Training Procedure (src/mcag/learning/training.py)

```python
"""
Two-stage training procedure implementing Section 4.2.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, List, Tuple, Optional, Callable
import numpy as np
from tqdm import tqdm
import wandb

class MCTrainer:
    """
    Meta-Cognitive Agent Graph trainer.
    
    Implements the two-stage training procedure:
    1. Agent reflection pre-training
    2. Joint graph-agent learning
    """
    
    def __init__(self,
                 agents: Dict[str, nn.Module],
                 graph_builder: nn.Module,
                 config: Dict):
        """
        Initialize trainer.
        
        Args:
            agents: Dictionary of agents
            graph_builder: Graph construction policy
            config: Training configuration
        """
        self.agents = agents
        self.graph_builder = graph_builder
        self.config = config
        
        # Optimizers
        self.agent_optimizers = {
            agent_id: optim.AdamW(
                agent.parameters(),
                lr=config.get('agent_lr', 1e-4),
                weight_decay=config.get('weight_decay', 1e-5)
            )
            for agent_id, agent in agents.items()
        }
        
        self.graph_optimizer = optim.AdamW(
            graph_builder.parameters(),
            lr=config.get('graph_lr', 1e-3),
            weight_decay=config.get('weight_decay', 1e-5)
        )
        
        # Loss functions
        self.consistency_loss = ReflectionConsistencyLoss(
            beta=config.get('consistency_beta', 1.0)
        )
        self.calibration_loss = CollaborativeCalibrationLoss(
            num_bins=config.get('num_calibration_bins', 10)
        )
        self.meta_edge_loss = MetaEdgeEffectivenessLoss(
            discount_factor=config.get('discount_factor', 0.99)
        )
        
        # Tracking
        self.training_history = {
            'stage1_losses': [],
            'stage2_losses': [],
            'agent_metrics': {agent_id: [] for agent_id in agents},
            'graph_metrics': []
        }
    
    def pretrain_agents(self,
                       dataloader: DataLoader,
                       num_epochs: int) -> Dict[str, List[float]]:
        """
        Stage 1: Agent reflection pre-training.
        
        Trains each agent independently on single-agent tasks.
        """
        print("Starting Stage 1: Agent Reflection Pre-training")
        
        agent_losses = {agent_id: [] for agent_id in self.agents}
        
        for epoch in range(num_epochs):
            epoch_losses = {agent_id: 0.0 for agent_id in self.agents}
            
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")
            for batch_idx, batch in enumerate(pbar):
                # Train each agent independently
                for agent_id, agent in self.agents.items():
                    # Get agent-specific data
                    agent_data = self._prepare_agent_data(batch, agent_id)
                    
                    if agent_data is None:
                        continue
                    
                    # Forward pass
                    outputs, states = agent(agent_data['inputs'])
                    
                    # Compute losses
                    task_loss = agent.compute_loss(
                        outputs, agent_data['targets'], states
                    )
                    
                    # Confidence calibration loss
                    confidence = states.confidence
                    accuracy = self._compute_accuracy(outputs, agent_data['targets'])
                    cal_loss = self._compute_calibration_loss(confidence, accuracy)
                    
                    # Total loss
                    total_loss = task_loss + self.config.get('cal_lambda', 0.1) * cal_loss
                    
                    # Backward pass
                    self.agent_optimizers[agent_id].zero_grad()
                    total_loss.backward()
                    self.agent_optimizers[agent_id].step()
                    
                    # Track losses
                    epoch_losses[agent_id] += total_loss.item()
                    
                    # Update progress bar
                    pbar.set_postfix({
                        f'{agent_id}_loss': total_loss.item()
                    })
                
                # Log to wandb if enabled
                if self.config.get('use_wandb', False):
                    wandb.log({
                        f'pretrain/{agent_id}_loss': total_loss.item()
                        for agent_id in self.agents
                    })
            
            # Average losses for epoch
            for agent_id in self.agents:
                avg_loss = epoch_losses[agent_id] / len(dataloader)
                agent_losses[agent_id].append(avg_loss)
                self.training_history['agent_metrics'][agent_id].append(avg_loss)
            
            print(f"Epoch {epoch+1} completed. Average losses:")
            for agent_id, losses in agent_losses.items():
                print(f"  {agent_id}: {losses[-1]:.4f}")
        
        return agent_losses
    
    def train_jointly(self,
                     dataloader: DataLoader,
                     num_epochs: int) -> Dict[str, List[float]]:
        """
        Stage 2: Joint graph-agent learning.
        
        Jointly trains graph construction policy and refines agent reflection policies.
        """
        print("Starting Stage 2: Joint Graph-Agent Learning")
        
        # Freeze base models (M_i)
        for agent in self.agents.values():
            agent.freeze_base_model()
        
        joint_losses = []
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            epoch_consistency_loss = 0.0
            epoch_calibration_loss = 0.0
            epoch_meta_edge_loss = 0.0
            
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")
            for batch_idx, batch in enumerate(pbar):
                # Prepare batch
                queries = batch['query']
                targets = batch['target']
                
                # Collect initial reflection states from all agents
                agent_states = {}
                agent_outputs = {}
                agent_confidences = {}
                
                for agent_id, agent in self.agents.items():
                    outputs, states = agent(queries)
                    agent_states[agent_id] = {
                        'output': outputs,
                        'confidence': states.confidence,
                        'uncertainty': states.uncertainty,
                        'strategy': states.strategy,
                        'hidden': states.hidden
                    }
                    agent_outputs[agent_id] = outputs
                    agent_confidences[agent_id] = states.confidence
                
                # Construct graph
                graph = self.graph_builder(queries, agent_states)
                
                # Execute graph
                final_output, execution_metrics = self._execute_graph(
                    graph, queries, agent_states
                )
                
                # Compute utility (task performance)
                utility = self._compute_utility(final_output, targets)
                
                # Compute computational cost
                comp_cost = self._compute_computational_cost(graph, execution_metrics)
                
                # Compute losses
                # 1. Consistency loss
                consistency_loss = self.consistency_loss(
                    agent_outputs, agent_confidences
                )
                
                # 2. Calibration loss
                ensemble_confidence = self._compute_ensemble_confidence(
                    agent_confidences, graph
                )
                accuracy = self._compute_accuracy(final_output, targets)
                calibration_loss = self.calibration_loss(
                    ensemble_confidence, accuracy
                )
                
                # 3. Meta-edge effectiveness loss
                meta_edge_loss = self._compute_meta_edge_loss(
                    graph, execution_metrics, utility
                )
                
                # Total loss (negative utility maximization)
                total_loss = (
                    -self.config.get('utility_weight', 1.0) * utility
                    + self.config.get('cost_weight', 0.1) * comp_cost
                    + self.config.get('consistency_weight', 0.1) * consistency_loss
                    + self.config.get('calibration_weight', 0.1) * calibration_loss
                    + self.config.get('meta_edge_weight', 0.05) * meta_edge_loss
                )
                
                # Backward pass
                self.graph_optimizer.zero_grad()
                for optimizer in self.agent_optimizers.values():
                    optimizer.zero_grad()
                
                total_loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(
                    self.graph_builder.parameters(),
                    self.config.get('max_grad_norm', 1.0)
                )
                
                # Update parameters
                self.graph_optimizer.step()
                for optimizer in self.agent_optimizers.values():
                    optimizer.step()
                
                # Track losses
                epoch_loss += total_loss.item()
                epoch_consistency_loss += consistency_loss.item()
                epoch_calibration_loss += calibration_loss.item()
                epoch_meta_edge_loss += meta_edge_loss.item()
                
                # Update progress bar
                pbar.set_postfix({
                    'total_loss': total_loss.item(),
                    'utility': utility.item(),
                    'cost': comp_cost.item()
                })
                
                # Log to wandb
                if self.config.get('use_wandb', False):
                    wandb.log({
                        'joint/total_loss': total_loss.item(),
                        'joint/utility': utility.item(),
                        'joint/cost': comp_cost.item(),
                        'joint/consistency_loss': consistency_loss.item(),
                        'joint/calibration_loss': calibration_loss.item(),
                        'joint/meta_edge_loss': meta_edge_loss.item()
                    })
            
            # Average losses for epoch
            avg_loss = epoch_loss / len(dataloader)
            joint_losses.append(avg_loss)
            self.training_history['stage2_losses'].append(avg_loss)
            
            print(f"Epoch {epoch+1} completed:")
            print(f"  Total loss: {avg_loss:.4f}")
            print(f"  Consistency loss: {epoch_consistency_loss/len(dataloader):.4f}")
            print(f"  Calibration loss: {epoch_calibration_loss/len(dataloader):.4f}")
            print(f"  Meta-edge loss: {epoch_meta_edge_loss/len(dataloader):.4f}")
        
        return joint_losses
    
    def _execute_graph(self,
                      graph: nx.DiGraph,
                      query: torch.Tensor,
                      agent_states: Dict) -> Tuple[torch.Tensor, Dict]:
        """
        Execute query through the constructed graph.
        
        Implements the execution semantics from Definition 3.13.
        """
        # Initialize execution state
        execution_state = {
            node: {
                'hidden': agent_states[node]['hidden'],
                'confidence': agent_states[node]['confidence'],
                'output': agent_states[node]['output'],
                'visited': False
            }
            for node in graph.nodes()
        }
        
        # Find source nodes (nodes with no incoming edges)
        source_nodes = [node for node in graph.nodes() 
                       if graph.in_degree(node) == 0]
        
        if not source_nodes:
            # If no source nodes, use all nodes as starting points
            source_nodes = list(graph.nodes())
        
        # Initialize queue with source nodes
        queue = list(source_nodes)
        
        # Process nodes in topological order
        while queue:
            current_node = queue.pop(0)
            
            if execution_state[current_node]['visited']:
                continue
            
            # Get current state
            current_state = execution_state[current_node]
            
            # Process incoming edges
            predecessors = list(graph.predecessors(current_node))
            
            if predecessors:
                # Aggregate inputs from predecessors
                aggregated_hidden = torch.zeros_like(current_state['hidden'])
                aggregated_confidence = torch.tensor(0.0, device=current_state['confidence'].device)
                
                for pred in predecessors:
                    pred_state = execution_state[pred]
                    
                    # Apply edge transformations based on edge type
                    edge_data = graph.get_edge_data(pred, current_node)
                    
                    if edge_data['type'] == 'standard':
                        # Standard collaboration: pass outputs
                        aggregated_hidden += pred_state['hidden']
                        
                    elif edge_data['type'] == 'confidence_flow':
                        # Confidence flow: gate by confidence
                        confidence_gate = self._confidence_gate(pred_state['confidence'])
                        transformed = self._transform_hidden(
                            pred_state['hidden'],
                            pred_state['confidence']
                        )
                        aggregated_hidden += confidence_gate * transformed
                        aggregated_confidence += pred_state['confidence'] * edge_data['weight']
                    
                    elif edge_data['type'] == 'strategy_suggestion':
                        # Strategy suggestion: update reflection policy
                        self._apply_strategy_suggestion(
                            current_node, pred, edge_data
                        )
                
                # Update current node state
                if aggregated_hidden.norm() > 0:
                    execution_state[current_node]['hidden'] = (
                        current_state['hidden'] + aggregated_hidden
                    )
                
                if aggregated_confidence > 0:
                    execution_state[current_node]['confidence'] = (
                        current_state['confidence'] + aggregated_confidence
                    )
            
            # Mark as visited
            execution_state[current_node]['visited'] = True
            
            # Add successors to queue
            successors = list(graph.successors(current_node))
            queue.extend([s for s in successors 
                         if not execution_state[s]['visited']])
        
        # Aggregate outputs from terminal nodes
        terminal_nodes = [node for node in graph.nodes() 
                         if graph.out_degree(node) == 0]
        
        if not terminal_nodes:
            terminal_nodes = list(graph.nodes())
        
        # Weighted aggregation by confidence
        total_confidence = sum(execution_state[node]['confidence'] 
                              for node in terminal_nodes)
        
        if total_confidence > 0:
            weights = [execution_state[node]['confidence'] / total_confidence 
                      for node in terminal_nodes]
            final_output = sum(w * execution_state[node]['output'] 
                             for w, node in zip(weights, terminal_nodes))
        else:
            # Fallback: average
            final_output = sum(execution_state[node]['output'] 
                             for node in terminal_nodes) / len(terminal_nodes)
        
        # Collect execution metrics
        execution_metrics = {
            'num_nodes_visited': sum(1 for state in execution_state.values() 
                                    if state['visited']),
            'avg_confidence': torch.tensor([execution_state[node]['confidence'] 
                                           for node in terminal_nodes]).mean(),
            'confidence_variance': torch.tensor([execution_state[node]['confidence'] 
                                                for node in terminal_nodes]).var(),
            'execution_depth': max(execution_state[node].get('depth', 0) 
                                  for node in graph.nodes())
        }
        
        return final_output, execution_metrics
```

8. Information-Theoretic Utilities (src/mcag/theory/information.py)

```python
"""
Information-theoretic analysis utilities.
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Dict, List
from scipy.special import psi

def mutual_information(x: torch.Tensor, 
                      y: torch.Tensor,
                      bins: int = 20) -> float:
    """
    Estimate mutual information I(X;Y) using histogram method.
    
    Args:
        x: Samples from X [n_samples, x_dim]
        y: Samples from Y [n_samples, y_dim]
        bins: Number of bins for discretization
        
    Returns:
        Mutual information estimate
    """
    n_samples = x.shape[0]
    
    # Discretize continuous variables
    if x.dim() > 1:
        # For multivariate X, use product of marginal discretizations
        x_discrete = torch.zeros(n_samples, dtype=torch.long)
        for i in range(x.shape[1]):
            x_i = x[:, i]
            bins_i = torch.linspace(x_i.min(), x_i.max(), bins + 1)
            indices = torch.bucketize(x_i, bins_i) - 1
            x_discrete = x_discrete * bins + indices
    else:
        bins_x = torch.linspace(x.min(), x.max(), bins + 1)
        x_discrete = torch.bucketize(x, bins_x) - 1
    
    if y.dim() > 1:
        y_discrete = torch.zeros(n_samples, dtype=torch.long)
        for i in range(y.shape[1]):
            y_i = y[:, i]
            bins_i = torch.linspace(y_i.min(), y_i.max(), bins + 1)
            indices = torch.bucketize(y_i, bins_i) - 1
            y_discrete = y_discrete * bins + indices
    else:
        bins_y = torch.linspace(y.min(), y.max(), bins + 1)
        y_discrete = torch.bucketize(y, bins_y) - 1
    
    # Compute joint and marginal distributions
    joint_counts = torch.zeros(bins ** x.shape[1] if x.dim() > 1 else bins,
                              bins ** y.shape[1] if y.dim() > 1 else bins)
    
    for i in range(n_samples):
        joint_counts[x_discrete[i], y_discrete[i]] += 1
    
    joint_probs = joint_counts / n_samples
    
    # Marginal distributions
    p_x = joint_probs.sum(dim=1)
    p_y = joint_probs.sum(dim=0)
    
    # Compute mutual information
    mi = 0.0
    for i in range(joint_probs.shape[0]):
        for j in range(joint_probs.shape[1]):
            p_xy = joint_probs[i, j].item()
            p_x_i = p_x[i].item()
            p_y_j = p_y[j].item()
            
            if p_xy > 0 and p_x_i > 0 and p_y_j > 0:
                mi += p_xy * np.log(p_xy / (p_x_i * p_y_j))
    
    return mi

def information_bottleneck(x: torch.Tensor,
                          y: torch.Tensor,
                          beta: float = 0.1,
                          hidden_dim: int = 32) -> Tuple[torch.Tensor, float]:
    """
    Compute information bottleneck representation.
    
    Args:
        x: Input samples [n_samples, x_dim]
        y: Target samples [n_samples, y_dim]
        beta: Trade-off parameter
        hidden_dim: Dimension of bottleneck representation
        
    Returns:
        bottleneck: Bottleneck representation
        ib_loss: Information bottleneck loss
    """
    n_samples, x_dim = x.shape
    
    # Encoder: p(t|x)
    encoder = torch.nn.Sequential(
        torch.nn.Linear(x_dim, 128),
        torch.nn.ReLU(),
        torch.nn.Linear(128, hidden_dim * 2)  # Mean and log-variance
    )
    
    # Decoder: p(y|t)
    decoder = torch.nn.Sequential(
        torch.nn.Linear(hidden_dim, 128),
        torch.nn.ReLU(),
        torch.nn.Linear(128, y.shape[1] if y.dim() > 1 else 1)
    )
    
    # Encode
    encoder_output = encoder(x)
    mu, log_var = encoder_output.chunk(2, dim=-1)
    
    # Reparameterization trick
    std = torch.exp(0.5 * log_var)
    eps = torch.randn_like(std)
    bottleneck = mu + eps * std
    
    # Decode
    y_pred = decoder(bottleneck)
    
    # Compute losses
    # 1. Reconstruction loss (negative log likelihood)
    if y.dim() > 1:  # Multivariate
        recon_loss = F.mse_loss(y_pred, y)
    else:  # Univariate
        recon_loss = F.mse_loss(y_pred.squeeze(), y)
    
    # 2. KL divergence (information rate)
    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    kl_loss = kl_loss / n_samples
    
    # Total IB loss
    ib_loss = recon_loss + beta * kl_loss
    
    return bottleneck, ib_loss.item()

def compute_capacity(confidences: Dict[str, torch.Tensor],
                    correctness: Dict[str, torch.Tensor]) -> float:
    """
    Compute metacognitive capacity: I({ĉ_i}; {1_{y_i=y*}})
    
    Args:
        confidences: Dict mapping agent_id → confidence scores
        correctness: Dict mapping agent_id → binary correctness
        
    Returns:
        metacognitive_capacity: Mutual information between confidence and correctness
    """
    # Stack all confidences and correctness values
    all_confidences = torch.cat(list(confidences.values()))
    all_correctness = torch.cat(list(correctness.values()))
    
    # Discretize confidences
    bins = torch.linspace(0, 1, 11)  # 10 bins
    conf_discrete = torch.bucketize(all_confidences, bins) - 1
    
    # Compute mutual information
    mi = mutual_information(conf_discrete.unsqueeze(1), 
                           all_correctness.unsqueeze(1),
                           bins=10)
    
    return mi
```

9. Main Configuration (configs/training/joint_train.yaml)

```yaml
# Joint training configuration
training:
  num_epochs: 100
  batch_size: 32
  learning_rate: 1e-4
  weight_decay: 1e-5
  max_grad_norm: 1.0
  
  # Stage 1: Agent pre-training
  stage1:
    num_epochs: 50
    agent_lr: 1e-4
    cal_lambda: 0.1
    
  # Stage 2: Joint training
  stage2:
    num_epochs: 50
    graph_lr: 1e-3
    utility_weight: 1.0
    cost_weight: 0.1
    consistency_weight: 0.1
    calibration_weight: 0.1
    meta_edge_weight: 0.05
    
  # Loss parameters
  losses:
    consistency_beta: 1.0
    num_calibration_bins: 10
    discount_factor: 0.99
    
  # Graph construction
  graph:
    max_nodes: 10
    max_meta_edges_per_node: 3
    confidence_threshold: 0.5
    uncertainty_threshold: 0.3
    use_confidence_gating: true
    use_strategy_suggestions: true
    use_uncertainty_propagation: true
    
  # Monitoring
  logging:
    use_wandb: true
    log_interval: 10
    save_interval: 5
    eval_interval: 1
    
  # Checkpointing
  checkpoint:
    save_dir: ./checkpoints
    save_best: true
    early_stopping_patience: 10
```

10. Example Usage (examples/basic_usage.py)

```python
"""
Basic usage example of the MCAG framework.
"""

import torch
import torch.nn as nn
from mcag.agents import MetaCognitiveAgent
from mcag.graphs import GraphBuilder, GraphConfig
from mcag.learning import MCTrainer
from mcag.utils.visualization import visualize_graph, plot_confidence_flow

def main():
    # Configuration
    config = {
        'training': {
            'num_epochs': 100,
            'batch_size': 32,
            'use_wandb': False
        },
        'graph': {
            'max_nodes': 5,
            'confidence_threshold': 0.5
        }
    }
    
    # Create base models (simplified examples)
    class SimpleSpeechModel(nn.Module):
        def __init__(self, input_dim=128, hidden_dim=512, output_dim=100):
            super().__init__()
            self.encoder = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim)
            )
            self.decoder = nn.Linear(hidden_dim, output_dim)
        
        def forward(self, x):
            hidden = self.encoder(x)
            logits = self.decoder(hidden)
            return hidden, logits
    
    # Create agents
    agents = {}
    agent_types = ['speech', 'vision', 'reasoning', 'verification', 'tool']
    
    for agent_type in agent_types:
        base_model = SimpleSpeechModel()
        
        agent = MetaCognitiveAgent(
            agent_id=f"{agent_type}_agent",
            base_model=base_model,
            input_dim=128,
            output_dim=100,
            hidden_dim=512,
            num_strategies=5
        )
        
        agents[agent.agent_id] = agent
    
    # Create graph builder
    graph_config = GraphConfig(
        max_nodes=5,
        confidence_threshold=0.5
    )
    
    graph_builder = GraphBuilder(
        agent_feature_dim=512,
        query_feature_dim=128,
        hidden_dim=256,
        config=graph_config
    )
    
    # Create trainer
    trainer = MCTrainer(agents, graph_builder, config['training'])
    
    # Create dummy data
    n_samples = 1000
    dummy_queries = torch.randn(n_samples, 128)
    dummy_targets = torch.randint(0, 100, (n_samples,))
    
    # Create dataloader
    from torch.utils.data import TensorDataset, DataLoader
    dataset = TensorDataset(dummy_queries, dummy_targets)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    print(f"Training {len(agents)} agents with dynamic graph construction")
    print(f"Configuration: {config}")
    
    # Stage 1: Pre-train agents
    print("\n=== Stage 1: Agent Pre-training ===")
    agent_losses = trainer.pretrain_agents(dataloader, num_epochs=10)
    
    # Stage 2: Joint training
    print("\n=== Stage 2: Joint Training ===")
    joint_losses = trainer.train_jointly(dataloader, num_epochs=10)
    
    # Test execution
    print("\n=== Testing Graph Execution ===")
    test_query = torch.randn(1, 128)
    
    # Get initial agent states
    agent_states = {}
    for agent_id, agent in agents.items():
        output, state = agent(test_query)
        agent_states[agent_id] = {
            'output': output,
            'confidence': state.confidence,
            'hidden': state.hidden,
            'strategy': state.strategy
        }
    
    # Construct graph
    graph = graph_builder(test_query.squeeze(), agent_states)
    
    # Visualize graph
    visualize_graph(graph, 
                   title="Constructed MCAG",
                   save_path="./graph_visualization.png")
    
    print(f"Graph constructed with {graph.number_of_nodes()} nodes")
    print(f"Graph has {graph.number_of_edges()} edges")
    
    # Execute query
    final_output, metrics = trainer._execute_graph(graph, test_query, agent_states)
    
    print(f"\nExecution Metrics:")
    for key, value in metrics.items():
        print(f"  {key}: {value}")
    
    print(f"\nFinal output shape: {final_output.shape}")
    print("MCAG framework test completed successfully!")

if __name__ == "__main__":
    main()
```

Installation & Setup

Requirements File (requirements.txt)

```txt
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
scipy>=1.10.0
networkx>=3.0
matplotlib>=3.7.0
seaborn>=0.12.0
pandas>=2.0.0
tqdm>=4.65.0
wandb>=0.15.0
scikit-learn>=1.3.0
pyyaml>=6.0
hydra-core>=1.3.0
jupyter>=1.0.0
ipython>=8.12.0
black>=23.0.0
flake8>=6.0.0
mypy>=1.3.0
pytest>=7.3.0
hypothesis>=6.75.0
```

Setup Script (setup.py)

```python
from setuptools import setup, find_packages

setup(
    name="mcag-framework",
    version="0.1.0",
    description="Meta-Cognitive Agent Graphs Framework",
    author="Your Name",
    author_email="your.email@example.com",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "torch>=2.0.0",
        "networkx>=3.0",
        "numpy>=1.24.0",
        "scipy>=1.10.0",
        "matplotlib>=3.7.0",
        "tqdm>=4.65.0",
        "pyyaml>=6.0",
    ],
    python_requires=">=3.8",
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Science/Research",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
    ],
)
```

Testing Framework

Property-Based Tests (tests/property/test_properties.py)

```python
"""
Property-based tests for MCAG framework invariants.
"""

import torch
import hypothesis
from hypothesis import given, strategies as st
from hypothesis import settings, HealthCheck
import numpy as np

from mcag.core.definitions import AgentState
from mcag.agents.meta_agent import MetaCognitiveAgent
from mcag.graphs.graph_builder import GraphBuilder

@given(
    batch_size=st.integers(min_value=1, max_value=32),
    input_dim=st.integers(min_value=10, max_value=100),
    output_dim=st.integers(min_value=2, max_value=20),
)
@settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
def test_agent_output_range(batch_size, input_dim, output_dim):
    """Test that agent outputs are valid probability distributions."""
    # Create dummy agent
    base_model = torch.nn.Linear(input_dim, output_dim)
    agent = MetaCognitiveAgent(
        agent_id="test_agent",
        base_model=base_model,
        input_dim=input_dim,
        output_dim=output_dim,
        hidden_dim=64,
        num_strategies=5
    )
    
    # Generate random input
    x = torch.randn(batch_size, input_dim)
    
    # Forward pass
    output, state = agent(x)
    
    # Check output properties
    assert torch.all(output >= 0), "Output probabilities must be non-negative"
    assert torch.allclose(output.sum(dim=-1), torch.ones(batch_size)), \
        "Output must sum to 1 across classes"
    
    # Check confidence properties
    confidence = state.confidence
    assert torch.all(confidence >= 0) and torch.all(confidence <= 1), \
        "Confidence must be in [0, 1]"

@given(
    n_agents=st.integers(min_value=2, max_value=10),
    feature_dim=st.integers(min_value=16, max_value=128),
)
def test_graph_construction_properties(n_agents, feature_dim):
    """Test graph construction invariant properties."""
    # Create dummy agent states
    agent_states = {}
    for i in range(n_agents):
        agent_states[f"agent_{i}"] = {
            'hidden': torch.randn(feature_dim),
            'confidence': torch.tensor(np.random.random()),
            'output': torch.randn(10).softmax(dim=-1),
            'strategy': torch.randn(feature_dim)
        }
    
    # Create graph builder
    builder = GraphBuilder(
        agent_feature_dim=feature_dim,
        query_feature_dim=32,
        hidden_dim=64
    )
    
    # Create dummy query
    query = torch.randn(32)
    
    # Construct graph
    graph = builder(query, agent_states)
    
    # Test properties
    assert graph.number_of_nodes() <= n_agents, \
        "Graph cannot have more nodes than available agents"
    
    assert graph.number_of_nodes() >= 1, \
        "Graph must have at least one node"
    
    # Test that no agent appears multiple times
    nodes = list(graph.nodes())
    assert len(nodes) == len(set(nodes)), \
        "Duplicate nodes in graph"
    
    # Test edge weight ranges
    for _, _, data in graph.edges(data=True):
        if 'weight' in data:
            assert 0 <= data['weight'] <= 1, \
                f"Edge weight {data['weight']} not in [0, 1]"

def test_reflection_consistency_loss_properties():
    """Test properties of reflection consistency loss."""
    from mcag.learning.objectives import ReflectionConsistencyLoss
    
    loss_fn = ReflectionConsistencyLoss(beta=1.0)
    
    # Test 1: Agreement should minimize confidence difference
    agent_outputs = {
        'agent1': torch.tensor([[0.8, 0.2], [0.3, 0.7]]),
        'agent2': torch.tensor([[0.8, 0.2], [0.3, 0.7]])
    }
    agent_confidences = {
        'agent1': torch.tensor([0.9, 0.6]),
        'agent2': torch.tensor([0.5, 0.4])
    }
    
    loss = loss_fn(agent_outputs, agent_confidences)
    
    # Test 2: Disagreement should penalize high confidence
    agent_outputs2 = {
        'agent1': torch.tensor([[0.8, 0.2], [0.3, 0.7]]),
        'agent2': torch.tensor([[0.2, 0.8], [0.7, 0.3]])
    }
    agent_confidences2 = {
        'agent1': torch.tensor([0.9, 0.9]),
        'agent2': torch.tensor([0.8, 0.8])
    }
    
    loss2 = loss_fn(agent_outputs2, agent_confidences2)
    
    # Property: Loss should be non-negative
    assert loss >= 0, "Loss must be non-negative"
    assert loss2 >= 0, "Loss must be non-negative"
    
    # Property: Perfect agreement with same confidence should give zero loss
    agent_outputs3 = {
        'agent1': torch.tensor([[0.8, 0.2]]),
        'agent2': torch.tensor([[0.8, 0.2]])
    }
    agent_confidences3 = {
        'agent1': torch.tensor([0.7]),
        'agent2': torch.tensor([0.7])
    }
    
    loss3 = loss_fn(agent_outputs3, agent_confidences3)
    assert torch.allclose(loss3, torch.tensor(0.0), atol=1e-6), \
        "Perfect agreement with same confidence should give zero loss"
```

Documentation

API Documentation Template

```python
"""
MCAG Framework API Documentation.

This module implements the Meta-Cognitive Agent Graphs framework.
"""

class MetaCognitiveAgent:
    """
    A meta-cognitive agent with self-reflection capabilities.
    
    Implements the formal definition:
    A_i = (M_i, R_i, C_i, Θ_i, Γ_i)
    
    Parameters
    ----------
    agent_id : str
        Unique identifier for the agent
    base_model : torch.nn.Module
        Base processing model M_i
    input_dim : int
        Dimension of input space
    output_dim : int
        Dimension of output space
    hidden_dim : int, optional
        Dimension of hidden states (default: 512)
    num_strategies : int, optional
        Number of reflection strategies (default: 5)
    
    Attributes
    ----------
    reflection_policy : ReflectionPolicy
        Policy network π_i^ref for reflection action selection
    confidence_calibrator : ConfidenceCalibrator
        Module for confidence calibration with temperature scaling
    strategy_embeddings : torch.nn.Embedding
        Learnable embeddings for reflection strategies Θ_i
    trust_parameters : torch.nn.ParameterDict
        Trust parameters Γ_i for other agents
    
    Methods
    -------
    forward(x: torch.Tensor) -> Tuple[torch.Tensor, AgentState]
        Forward pass with reflection
    apply_meta_edge_effect(edge_type: str, sender_state: AgentState, 
                          receiver_state: AgentState) -> AgentState
        Apply meta-cognitive edge effects
    update_calibration(predictions: torch.Tensor, 
                      targets: torch.Tensor, 
                      confidences: torch.Tensor) -> None
        Update calibration statistics
    
    Examples
    --------
    >>> from mcag.agents import MetaCognitiveAgent
    >>> from mcag.graphs import GraphBuilder
    >>> 
    >>> # Create agent
    >>> agent = MetaCognitiveAgent(
    ...     agent_id="speech_agent",
    ...     base_model=speech_model,
    ...     input_dim=128,
    ...     output_dim=100
    ... )
    >>> 
    >>> # Forward pass
    >>> x = torch.randn(32, 128)
    >>> output, state = agent(x)
    >>> 
    >>> print(f"Output shape: {output.shape}")
    >>> print(f"Confidence: {state.confidence.mean():.3f}")
    
    Notes
    -----
    The agent implements the following mathematical operations:
    1. Base computation: h = M_i(x)
    2. Confidence calibration: ĉ = f_i^cal(h)
    3. Reflection: a ∼ π_i^ref(φ(h, ĉ, y))
    4. State update: s = (h, ĉ, U, θ^a, t)
    
    