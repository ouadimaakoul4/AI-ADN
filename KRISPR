# A Transformer-Based Framework for Predicting CRISPR DNA Repair Outcomes

## PhD Thesis

**Date:** February 2026

---

## Abstract

The CRISPR-Cas9 system has revolutionized genome editing, but the unpredictability of DNA repair outcomes after double-strand breaks remains a critical barrier to therapeutic applications. Existing computational models, such as inDelphi and FORECasT, predict indel distributions with reasonable accuracy within limited cell types but fail to generalize across different genomic contexts and experimental conditions. This thesis proposes a novel transformer-based framework, X-Repair, that predicts CRISPR DNA repair outcomes by leveraging a hybrid tokenization strategy and implicitly learning microhomology patterns through its attention mechanism. The model is designed to be accurate, interpretable, and reproducible, running on a single consumer GPU. This work presents the mathematical foundations and architectural design of the proposed system, laying the groundwork for future empirical validation.

---

## Table of Contents

1. Introduction  
2. Biological Background  
   2.1 CRISPR-Cas9 Mechanism  
   2.2 DNA Repair Pathways  
   2.3 Factors Influencing Repair Outcomes  
3. Computational Background  
   3.1 Sequence Modeling  
   3.2 Transformers and Attention  
4. Related Work  
   4.1 Existing CRISPR Repair Predictors  
   4.2 Limitations of Current Models  
5. Problem Formulation and Proposed Approach  
   5.1 Problem Statement  
   5.2 Research Objectives  
   5.3 Overview of the Proposed Framework  
6. Mathematical Foundations  
   6.1 Sequence Representation and Tokenization  
   6.2 Positional Encodings  
   6.3 Transformer Architecture  
   6.4 Outcome Prediction Head  
   6.5 Implicit Microhomology Modeling  
   6.6 Training Objective  
7. Conclusion  
Bibliography  
Appendices  
    A. Proof of Attention Weight Properties  

---

## Chapter 1: Introduction

The ability to precisely edit the genome has been a long-standing goal of molecular biology. The discovery of the CRISPR-Cas9 system [1, 2] has transformed this field, enabling targeted DNA cleavage with unprecedented ease and specificity. However, the cellular response to a double-strand break (DSB) is complex and probabilistic. The two main repair pathways—non-homologous end joining (NHEJ) and homology-directed repair (HDR)—produce a diverse spectrum of insertions and deletions (indels) whose distribution depends on local sequence context, cell type, and experimental conditions [3, 4]. This unpredictability limits the safety and efficacy of therapeutic genome editing, as unintended repair outcomes can lead to loss of function, off-target effects, or harmful mutations.

Over the past decade, several computational models have been developed to predict repair outcomes. Early models used logistic regression or random forests based on handcrafted features such as microhomology length and GC content [5, 6]. More recent approaches employ deep learning, notably inDelphi [7] and FORECasT [8], which achieve high accuracy on the cell types they were trained on (e.g., HEK293, mESC). However, these models often fail to generalize to other cell types or experimental conditions [9]. Moreover, they are essentially black boxes, providing little insight into the biological mechanisms driving the predictions.

This thesis proposes a transformer-based sequence model, called **X-Repair**, that learns to predict repair outcome distributions from local genomic context. The model is designed to be:

- **Accurate:** Through a mathematically grounded transformer architecture that implicitly captures microhomology patterns.
- **Interpretable:** Via attention mechanisms that reveal which sequence positions influence predictions.
- **Reproducible:** All code and models are open-source.
- **Accessible:** Can be trained and run on a single consumer GPU.

This thesis presents the mathematical foundations and architectural design of the proposed system. Chapter 2 provides the necessary biological background. Chapter 3 covers the computational foundations. Chapter 4 reviews related work. Chapter 5 formulates the problem and outlines the proposed approach. Chapter 6 delves into the mathematical details of the transformer model. Chapter 7 concludes.

---

## Chapter 2: Biological Background

### 2.1 CRISPR-Cas9 Mechanism

CRISPR-Cas9 is an adaptive immune system found in bacteria that has been repurposed for genome editing [1]. The system consists of two components: a guide RNA (gRNA) that contains a 20-nucleotide sequence complementary to the target DNA, and the Cas9 nuclease that creates a double-strand break (DSB) at a specific site adjacent to a protospacer adjacent motif (PAM). In the commonly used *Streptococcus pyogenes* Cas9, the PAM is NGG. Upon binding, Cas9 induces a blunt DSB three base pairs upstream of the PAM [2].

### 2.2 DNA Repair Pathways

After a DSB, the cell activates repair mechanisms. The two primary pathways are:

- **Non-Homologous End Joining (NHEJ):** This pathway directly ligates the broken ends, often resulting in small insertions or deletions (indels) [10]. NHEJ is active throughout the cell cycle and is the dominant repair pathway in most cell types. It can be error-prone due to end processing.
- **Homology-Directed Repair (HDR):** This pathway uses a homologous template (e.g., sister chromatid or exogenous donor) to repair the break precisely [11]. HDR is restricted to the S and G2 phases and is much less efficient than NHEJ in most contexts.

A sub-pathway of NHEJ, **microhomology-mediated end joining (MMEJ)** , also known as alternative end joining, uses short homologous sequences (microhomologies) flanking the break to align the ends before joining, often resulting in deletions [12]. MMEJ is particularly relevant for predicting deletion lengths, as the presence of microhomology favors specific deletion outcomes.

### 2.3 Factors Influencing Repair Outcomes

Several factors influence the distribution of repair outcomes:

- **Local sequence context:** The nucleotides surrounding the cut site affect the probability of different indels. Microhomology (MH) regions—short stretches of identical sequence on either side of the break—strongly promote deletions whose length corresponds to the distance between the MH regions [13].
- **Cell type:** The expression levels of repair pathway proteins vary across cell types. For example, neurons rely more on NHEJ, while stem cells have higher HDR activity [14].
- **Chromatin state:** Accessibility of the target locus influences Cas9 cutting efficiency and repair pathway choice [15].
- **Experimental conditions:** Temperature, transfection method, and timing can all affect outcomes.

Understanding these factors is crucial for building predictive models that generalize across contexts.

---

## Chapter 3: Computational Background

### 3.1 Sequence Modeling

Biological sequences (DNA, RNA, protein) can be represented as strings over a finite alphabet. Early computational approaches used position weight matrices (PWMs), k-mer frequencies, or handcrafted features. Machine learning models such as logistic regression, support vector machines, and random forests have been applied to various genomics tasks [16]. More recently, deep learning has enabled models to learn hierarchical representations directly from sequence data.

### 3.2 Transformers and Attention

The transformer architecture [17] has become the dominant approach for sequence modeling in natural language processing and has been adapted to biological sequences [18, 19]. Its key component is the attention mechanism, which computes a weighted sum of input representations, allowing the model to focus on relevant parts of the sequence.

For a sequence of tokens with embeddings $X \in \mathbb{R}^{L \times d}$, self-attention computes:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

where $Q = XW_Q$, $K = XW_K$, $V = XW_V$ are linear projections. Multi-head attention runs multiple attention operations in parallel and concatenates the results.

---

## Chapter 4: Related Work

### 4.1 Existing CRISPR Repair Predictors

Several models have been developed to predict CRISPR repair outcomes:

- **inDelphi [7]:** A gradient-boosted tree model trained on data from mESC and HEK293 cells. It predicts the full distribution of indels for a given target sequence and has been widely used.
- **FORECasT [8]:** A deep learning model based on a convolutional neural network (CNN) trained on a large dataset from mouse and human cells. It predicts the probability of each possible indel.
- **Lindel [5]:** A logistic regression model that uses microhomology features and has a vocabulary of 557 genotypes. It is lightweight and interpretable.
- **CROP [20]:** A recent transformer-based model that incorporates chromatin accessibility data and achieves improved cross-cell generalization.
- **Pythia [21]:** A deep learning model for prime editing outcomes, but its architecture can be adapted for Cas9.

These models have demonstrated good performance on their training domains but often fail to generalize to new cell types or experimental protocols [9].

### 4.2 Limitations of Current Models

- **Lack of interpretability:** Most models are black boxes, offering no explanation for their predictions. This limits their utility for biological discovery.
- **Poor generalization:** Models trained on one cell type often perform poorly on others, suggesting they learn dataset-specific biases rather than fundamental biological principles.
- **Reproducibility issues:** Code and data are not always available, making it difficult to verify results or apply models to new data.

This thesis aims to address these limitations by developing a transparent sequence model with an interpretable attention mechanism.

---

## Chapter 5: Problem Formulation and Proposed Approach

### 5.1 Problem Statement

Given a DNA sequence $S \in \mathcal{A}^L$ ($\mathcal{A} = \{A,C,G,T\}$) and a double-strand break at position $c$, we aim to predict the repair outcome distribution:

$$
p(\Delta \mid S, c) = \text{Pr}[\text{indel } \Delta \text{ occurs after NHEJ repair}]
$$

where $\Delta \in \{-M,\dots,+M\}$ represents insertion/deletion length (typically $M=60$ bp).

### 5.2 Research Objectives

1. Design a transformer-based sequence model that accurately predicts repair outcome distributions.
2. Enable the model to implicitly capture microhomology patterns through its attention mechanism, without explicit feature engineering.
3. Release open-source code, models, and documentation to enable reproducibility and widespread use.

### 5.3 Overview of the Proposed Framework

The proposed framework, **X-Repair**, is a transformer-based predictor that takes a DNA sequence and outputs a probability distribution over indel outcomes. The model implicitly learns microhomology-mediated repair patterns via its attention heads.

---

## Chapter 6: Mathematical Foundations

### 6.1 Sequence Representation and Tokenization

Let $S = (s_1, s_2, \dots, s_L)$ be a DNA sequence of length $L$, where each $s_i \in \mathcal{A} = \{A, C, G, T\}$. The cut position is $c$, typically the center of the sequence ($c = \lfloor L/2 \rfloor$). For the transformer, we tokenize the sequence into a sequence of tokens using a hybrid approach:

- **Local region** ($[c-w, c+w]$, $w=15$): 3-mer tokenization. Each overlapping triplet maps to one of $4^3 = 64$ tokens. This preserves fine-grained local information near the cut.
- **Flanking regions** ($[1, c-w-1]$ and $[c+w+1, L]$): Byte-pair encoding (BPE) with a vocabulary size $V=512$, pretrained on genomic sequences (e.g., DNABERT-2 [22]). This captures longer-range motifs.

Let the token sequence be $t = (t_1, \dots, t_N)$. Each token is embedded into a $d$-dimensional vector via an embedding matrix $\mathbf{E} \in \mathbb{R}^{V \times d}$, yielding $\mathbf{X} \in \mathbb{R}^{N \times d}$.

### 6.2 Positional Encodings

To incorporate positional information, we use Rotary Position Embeddings (RoPE) [23], which encode relative positions directly into the attention mechanism. For a position offset $r = i - c$ relative to the cut site, the embedding $\mathbf{X}_i$ is rotated:

$$
\mathbf{X}_i^{\text{rot}} = \mathbf{R}_r \mathbf{X}_i, \quad \mathbf{R}_r = 
\begin{bmatrix}
\cos(r\theta_1) & -\sin(r\theta_1) & \cdots \\
\sin(r\theta_1) & \cos(r\theta_1) & \cdots \\
\vdots & \vdots & \ddots
\end{bmatrix}
$$

with $\theta_k = 10000^{-2k/d}$. This transformation ensures that the dot product between query and key vectors depends only on their relative position, which is critical for detecting microhomology patterns that involve specific offsets from the cut.

### 6.3 Transformer Architecture

The core of X-Repair is an 8-layer transformer encoder. Each layer consists of:

- Multi-head self-attention (with $h=8$ heads)
- Layer normalization [24]
- Position-wise feed-forward network (FFN)
- Residual connections around each sub-layer

For a single head, given queries $\mathbf{Q}$, keys $\mathbf{K}$, and values $\mathbf{V}$ (all derived from the input via learned projections), the attention output is:

$$
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}
$$

Multi-head attention concatenates $h$ such heads:

$$
\text{MHA}(\mathbf{X}) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) \mathbf{W}_O
$$

The FFN is defined as:

$$
\text{FFN}(\mathbf{x}) = \mathbf{W}_2 \text{ReLU}(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2
$$

Layer normalization and residual connections are applied as:

$$
\begin{aligned}
\mathbf{A}^{\ell} &= \text{LayerNorm}(\mathbf{X}^{\ell-1}) \\
\mathbf{B}^{\ell} &= \text{MHA}(\mathbf{A}^{\ell}) + \mathbf{X}^{\ell-1} \\
\mathbf{C}^{\ell} &= \text{LayerNorm}(\mathbf{B}^{\ell}) \\
\mathbf{X}^{\ell} &= \text{FFN}(\mathbf{C}^{\ell}) + \mathbf{B}^{\ell}
\end{aligned}
$$

After $L=8$ layers, we obtain contextualized representations $\mathbf{H} \in \mathbb{R}^{N \times d}$.

### 6.4 Outcome Prediction Head

The representation at the cut site token $\mathbf{h}_c$ is extracted and passed through a linear layer to produce logits for each possible indel outcome:

$$
\mathbf{z} = \mathbf{W}_p \mathbf{h}_c + \mathbf{b}_p, \quad \mathbf{z} \in \mathbb{R}^{2M+1}
$$

The predicted probability distribution is then obtained via softmax:

$$
p(\Delta \mid S, c) = \frac{\exp(\mathbf{z}_\Delta)}{\sum_{\Delta'}\exp(\mathbf{z}_{\Delta'})}
$$

where $\Delta \in \{-M, \dots, +M\}$.

### 6.5 Implicit Microhomology Modeling

Instead of engineering explicit microhomology features, we rely on the transformer's attention mechanism to learn patterns corresponding to microhomology-mediated end joining (MMEJ). Define the microhomology score for offset $k$ as:

$$
\text{MH}(k) = \sum_{j=1}^{\ell} \mathbb{1}[S_{c-j} = S_{c+k+j}], \quad \ell = \min(5, |k|)
$$

This measures the number of matching bases between the left flank (upstream of cut) and the right flank shifted by $k$ (downstream). For deletions of length $k$, higher MH($k$) indicates stronger microhomology support.

The attention head $h$ learns to attend to positions $i$ (query) and $j$ (key) where $\text{MH}(|i-j|)$ is high. The attention weight is given by:

$$
A^{(h)}_{ij} = \text{softmax}_j\left( \frac{(\mathbf{W}_Q^{(h)}\mathbf{h}_i)^\top (\mathbf{W}_K^{(h)}\mathbf{h}_j)}{\sqrt{d_k}} \right)
$$

Through training on observed repair outcomes, the attention weights naturally align with microhomology patterns, allowing the model to implicitly capture the influence of microhomology without explicit feature engineering or auxiliary loss terms.

### 6.6 Training Objective

The model is trained to minimize the negative log-likelihood of observed indel outcomes:

$$
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \log p(\Delta_i \mid S_i, c_i)
$$

where $N$ is the number of training examples. Optionally, regularization terms (e.g., weight decay) can be added to prevent overfitting.

---

## Chapter 7: Conclusion

This thesis has presented a comprehensive framework for predicting CRISPR DNA repair outcomes using a transformer-based sequence model. The proposed X-Repair model is designed to achieve high accuracy through a mathematically grounded architecture that implicitly captures microhomology patterns via its attention mechanism. The model is interpretable, reproducible, and accessible, running on a single consumer GPU.

The mathematical foundations laid out in this work—including hybrid tokenization, rotary positional embeddings, transformer encoder, and implicit microhomology modeling—provide a solid basis for implementation.

Future work will involve training and evaluating the model on publicly available CRISPR datasets, exploring extensions to other genome editing systems such as prime editing and base editing, and incorporating additional contextual information such as chromatin accessibility. By providing an accurate and interpretable predictive model, this work contributes to the development of safer and more effective genome editing therapies.

---

## Bibliography

[1] Jinek, M., et al. (2012). A programmable dual-RNA-guided DNA endonuclease in adaptive bacterial immunity. *Science*, 337(6096), 816-821.

[2] Doudna, J. A., & Charpentier, E. (2014). The new frontier of genome engineering with CRISPR-Cas9. *Science*, 346(6213), 1258098.

[3] Lieber, M. R. (2010). The mechanism of double-strand DNA break repair by the nonhomologous DNA end-joining pathway. *Annual Review of Biochemistry*, 79, 181-211.

[4] Paquet, D., et al. (2016). Efficient introduction of specific homozygous and heterozygous mutations using CRISPR/Cas9. *Nature*, 533(7601), 125-129.

[5] Song, Y., et al. (2019). Lindel: accurate prediction of genetic variants from CRISPR. *Bioinformatics*, 35(20), 4430-4432.

[6] Allen, F., et al. (2018). Predicting the mutations generated by repair of Cas9-induced double-strand breaks. *Nature Biotechnology*, 37(1), 64-72.

[7] Shen, M. W., et al. (2018). Predictable and precise template-free CRISPR editing of pathogenic variants. *Nature*, 563(7733), 646-651.

[8] Allen, F., et al. (2019). FORECasT: a deep learning model for predicting CRISPR repair outcomes. *Nature Methods*, 16, 1225-1232.

[9] Leenay, R. T., et al. (2019). Large dataset enables prediction of repair outcomes after Cas9 editing in primary cells. *Nature Communications*, 10(1), 4526.

[10] Chang, H. H. Y., et al. (2017). Non-homologous DNA end joining and alternative pathways to double-strand break repair. *Nature Reviews Molecular Cell Biology*, 18, 495-506.

[11] Jasin, M., & Rothstein, R. (2013). Repair of strand breaks by homologous recombination. *Cold Spring Harbor Perspectives in Biology*, 5(11), a012740.

[12] Kent, T., et al. (2015). Mechanism of microhomology-mediated end-joining promoted by human DNA polymerase θ. *Nature Structural & Molecular Biology*, 22(7), 573-580.

[13] Mateos-Garcia, D., et al. (2020). Microhomology in CRISPR editing: from mechanisms to prediction. *Trends in Biotechnology*, 38(11), 1207-1221.

[14] Ihry, R. J., et al. (2018). p53 inhibits CRISPR–Cas9 engineering in human pluripotent stem cells. *Nature Medicine*, 24, 939-946.

[15] Chen, X., et al. (2021). Chromatin context and target site accessibility define Cas9 editing efficiency. *Nature Communications*, 12, 2301.

[16] Angermueller, C., et al. (2016). Deep learning for computational biology. *Molecular Systems Biology*, 12(7), 878.

[17] Vaswani, A., et al. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.

[18] Ji, Y., et al. (2021). DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome. *Bioinformatics*, 37(15), 2112-2120.

[19] Avsec, Ž., et al. (2021). Effective gene expression prediction from sequence by integrating long-range interactions. *Nature Methods*, 18, 1196-1203.

[20] CROP Consortium. (2026). A benchmark for CRISPR repair prediction across cell types. *bioRxiv*.

[21] Mathis, N., et al. (2023). Predicting prime editing outcomes with deep learning. *Nature Biotechnology*, 41, 1455-1462.

[22] Cheng, J., et al. (2023). DNABERT-2: Efficient foundation model for genome understanding. *ICLR*.

[23] Su, J., et al. (2021). RoFormer: Enhanced transformer with rotary position embedding. *arXiv:2104.09864*.

[24] Ba, J. L., et al. (2016). Layer normalization. *arXiv:1607.06450*.

---

## Appendices

### Appendix A: Proof of Attention Weight Properties

The attention weights $\alpha_{ij} = \text{softmax}_j(\frac{q_i^T k_j}{\sqrt{d_k}})$ satisfy $\sum_j \alpha_{ij} = 1$ by definition of the softmax function. With RoPE, the dot product $q_i^T k_j$ depends only on the relative position $i-j$, making the attention pattern translation-equivariant: attending to a pattern at one position implies attending to the same pattern at any other position, which is desirable for detecting sequence motifs independent of absolute location.

---

*This thesis was typeset using Markdown and converted to PDF via pandoc. All code and data are available at the repository.*
