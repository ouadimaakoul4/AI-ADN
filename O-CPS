Autonomous AI Agents for Distributed Coordination of Mega-Constellations: A Cyberâ€‘Physical Systems Approach to Scalable Orbital Infrastructure

1. Introduction: The Megaâ€‘Constellation Scaling Crisis

The orbital environment is undergoing a transformation from sparse, isolated assets to dense, interconnected infrastructure. Megaâ€‘constellationsâ€”dynamic meshes of thousands of satellitesâ€”are becoming the backbone of global connectivity, Earth observation, and scientific research. This shift introduces unprecedented coordination challenges that cannot be solved by scaling existing groundâ€‘based control paradigms.

1.1 The Tripartite Scaling Problem

1. Latencyâ€‘Complexity Wall: Centralized ground control cannot solve O(NÂ²) linkâ€‘scheduling and Nâ€‘body collisionâ€‘avoidance problems with subâ€‘second latency over slow, congested groundâ€‘space links.
2. Dynamic Topology Problem: LEO satellites move at ~7.5 km/s, creating network topologies that change on millisecond timescales.
3. Resourceâ€‘Awareness Gap: Current architectures treat communication and navigation as separate optimization problems, ignoring their coupling through shared, finite physical resources.

1.2 Thesis Statement

Formal Statement: Autonomous, distributed AI agents, operating within a formal cyberâ€‘physical framework that explicitly models the bidirectional coupling between orbital mechanics and network performance, provide the only viable path to scalable, resilient megaâ€‘constellation coordination.

1.3 Formal Research Questions

1. RQ1 (Consensus): How can agents reach global agreement using only local neighbor communication with provable convergence guarantees?
2. RQ2 (Optimization): What are the mathematical properties of the Pareto frontier between data throughput and fuel consumption?
3. RQ3 (Safety): How can safety constraints be formally guaranteed in a distributed coordination system?
4. RQ4 (Resilience): What formal methods ensure Byzantine fault tolerance in orbital networks?

2. Theoretical Foundations: Mathematical Modeling of Orbital Cyberâ€‘Physical Systems

2.1 Orbital CPS Formal Definition

Definition 2.1 (Orbital Cyberâ€‘Physical System): An orbital CPS is a tuple â„´ = (ğ’«, ğ’, Î¦_pc, Î¦_cp) where:

Â· ğ’« = {ğ’«_i}_{i=1}^N is the set of physical satellite subsystems
Â· ğ’ = {ğ’_i}_{i=1}^N is the set of cyber (computational) subsystems
Â· Î¦_pc: ğ’« â†’ ğ’ maps physical state to cyber constraints
Â· Î¦_cp: ğ’ â†’ ğ’« maps cyber decisions to physical effects

2.2 Physical Dynamics Model

For each satellite i, the equations of motion in Earthâ€‘Centered Inertial (ECI) frame:

\begin{aligned}
\dot{\mathbf{r}}_i &= \mathbf{v}_i \\
\dot{\mathbf{v}}_i &= -\frac{\mu}{r_i^3}\mathbf{r}_i + \mathbf{a}_{J_2,i} + \mathbf{a}_{drag,i} + \mathbf{a}_{SRP,i} + \frac{\mathbf{F}_i}{m_i}
\end{aligned}

where:

Â· Î¼ = 3.986 Ã— 10Â¹â´ mÂ³/sÂ² (Earth's gravitational parameter)
Â· a_Jâ‚‚,i = -3/2 Jâ‚‚(Î¼R_EÂ²/r_iâµ)[x_i(5z_iÂ²/r_iÂ²-1), y_i(5z_iÂ²/r_iÂ²-1), z_i(5z_iÂ²/r_iÂ²-3)]^T
Â· Jâ‚‚ = 1.08263 Ã— 10â»Â³ (Earth's oblateness coefficient)
Â· R_E = 6,378 km (Earth's equatorial radius)

Energy dynamics:

\dot{e}_i(t) = P_{solar,i}(t) - \left[ P_{bus,i} + \sum_{j\in\mathcal{N}_i(t)} \eta_{rf}(\Gamma_{ij}) + \eta_{thrust}(\|\mathbf{F}_i\|) \right]

where Î·_rf(Î“) = P_0 + Î±Î“ (RF power consumption) and Î·_thrust(ğ…) = â€–ğ…â€–/(I_sp g_0) (thruster power).

Mass dynamics:

\dot{m}_i(t) = -\frac{\|\mathbf{F}_i(t)\|}{I_{sp}g_0}

2.3 Communication Network Model

Definition 2.2 (Timeâ€‘Varying Graph): G(t) = (V, E(t)) where V = {1,â€¦,N} and E(t) = {(i,j) : L_ij(t) = 1}.

Link existence function:

L_{ij}(t) = \mathbb{I}[\text{LoS}_{ij}(t)] \cdot \mathbb{I}[d_{ij}(t) \leq D_{max}] \cdot \mathbb{I}[f_{doppler,ij}(t) \leq f_{max}]

where:

Â· \text{LoS}_{ij}(t) = 1 \iff \|\mathbf{r}_i \times \mathbf{r}_j\|/\|\mathbf{r}_i - \mathbf{r}_j\| \geq R_E + h_{atm}
Â· d_{ij}(t) = \|\mathbf{r}_i(t) - \mathbf{r}_j(t)\|
Â· f_{doppler,ij}(t) = f_c \cdot \|\mathbf{v}_i(t) - \mathbf{v}_j(t)\|/c

Theorem 2.1 (Maximum Transmission Rate):

\Gamma_{ij}^{max}(t) = B \log_2\left(1 + \frac{P_t G_t G_r \lambda^2}{(4\pi d_{ij}(t))^2 k T B} \cdot L_{atm} \cdot L_{pointing}\right)

Proof: Direct application of Shannonâ€‘Hartley theorem to timeâ€‘varying AWGN channel.

2.4 Composite State Space

Definition 2.3 (Global State Space):

\mathbf{S}(t) = [\mathbf{P}(t), \mathbf{C}(t)] \in \mathbb{R}^{11N} \times \mathcal{G}_N

where P(t) = [p_1(t),â€¦,p_N(t)] âˆˆ â„Â¹Â¹á´º (physical states) and C(t) = [Q(t), R(t)] (cyber states).

Theorem 2.2 (State Space Dimensionality): Minimal representation has dimension 11N + N(N-1)/2 but exhibits O(NÂ²) coupling terms.

Proof: 11 dimensions per satellite (position, velocity, mass, fuel, energy, temperature) plus N(N-1)/2 potential links. Coupling through link existence and resource consumption creates O(NÂ²) interaction terms.

2.5 Cyberâ€‘Physical Coupling Theorems

Theorem 2.3 (Bidirectional Coupling):

1. Physicsâ†’Cyber: âˆ‚E(t)/âˆ‚r_i â‰  0 (position affects link existence)
2. Cyberâ†’Physics: âˆ‚Ä—_i/âˆ‚Î“_ij = -Î± < 0 (data rate affects energy consumption)

Proof: From Definition 2.2, L_ij(t) depends on positions/velocities. From energy dynamics, âˆ‚Ä—_i/âˆ‚Î“_ij = -âˆ‚Î·_rf/âˆ‚Î“_ij = -Î±.

Theorem 2.4 (Conservation Constraints): For any policy Ï€:

\sum_{i=1}^N \int_0^T \dot{e}_i(t) dt \leq \sum_{i=1}^N \int_0^T P_{solar,i}(t) dt

\sum_{i=1}^N [m_i^{fuel}(0) - m_i^{fuel}(T)] = \sum_{i=1}^N \int_0^T \frac{\|\mathbf{F}_i(t)\|}{I_{sp}g_0} dt

Proof: Integration of energy and mass dynamics with boundary conditions.

3. Formal Architecture: Constrained Decâ€‘POMDP Framework

3.1 Decâ€‘POMDP Formulation

Definition 3.1 (Constrained Decâ€‘POMDP):

\mathcal{M} = \langle \mathcal{I}, \mathcal{S}, \{\mathcal{A}_i\}, \{\mathcal{O}_i\}, P, R, \{\mathcal{C}_k\}, \gamma \rangle

where:

Â· â„ = {1,â€¦,N} (agents)
Â· ğ’® = ğ’« Ã— ğ’ (composite state space)
Â· ğ’œáµ¢ = â„Â³ Ã— â„â‚Š^Máµ¢ Ã— â„›áµ¢ (thrust, data rates, routes)
Â· ğ’ªáµ¢ (observation space)
Â· P: ğ’® Ã— ğ’œ Ã— ğ’® â†’ [0,1] (transition probability)
Â· R: ğ’® Ã— ğ’œ â†’ â„ (reward function)
Â· ğ’â‚–: ğ’® Ã— ğ’œ â†’ â„ (constraint functions)
Â· Î³ âˆˆ [0,1) (discount factor)

3.2 Partial Observability Model

Theorem 3.1 (Observability Limitations):

\mathcal{O}_i(t) = \{\mathbf{p}_i(t)\} \cup \{\mathbf{p}_j(t-\tau_{ij}) : j \in \mathcal{N}_i(t)\} \cup \{Q_{ij}(t)\}

with â€–ğ’ªáµ¢(t)â€– = O(K) where K is maximum neighbors.

Proof: Limited by communication range (Definition 2.2) and speedâ€‘ofâ€‘light delays Ï„_ij â‰¥ â€–r_i - r_jâ€–/c.

Corollary 3.1.1 (Information Asymmetry): For d_ij > D_max, observations become statistically independent after Î”t > (d_ij - D_max)/v_max.

3.3 Constraint Formulation as Barrier Functions

Logarithmic barrier for resource constraints:

B_{energy}(e_i) = -\log\left(\frac{e_i - e_{min}}{e_{max} - e_{min}}\right)

Quadratic barrier for safety constraints:

B_{collision}(\mathbf{r}_{ij}) = \max\left(0, 1 - \frac{\|\mathbf{r}_{ij}\|}{d_{safe}}\right)^2

Theorem 3.2 (Barrier Function Properties): For B(x) = -log(x - x_min):

1. B(x) â†’ âˆ as x â†’ x_minâº
2. âˆ‡B(x) = -1/(x - x_min)
3. âˆ‡Â²B(x) = 1/(x - x_min)Â² > 0 (convex)

Proof: Direct differentiation.

3.4 Multiâ€‘Objective Reward Design

Definition 3.2 (Multiâ€‘Objective Reward):

R_i(t) = \mathbf{w}^T \mathbf{r}_i(t) - \boldsymbol{\lambda}^T \mathbf{B}(\mathbf{s}_i(t))

where r_i = [Throughput_i, -Latency_i, -Î”v_i] and B = [B_energy, B_fuel, B_collision].

Theorem 3.3 (Pareto Optimality): For given w, policy maximizing ğ”¼[âˆ‘ Î³áµ— R_i(t)] is Pareto optimal w.r.t. objectives in r_i.

Proof: Scalarization theorem for convex multiâ€‘objective optimization.

3.5 Hybrid Policy Architecture

Definition 3.3 (Hybrid Policy Class):

\Pi_i = \{\pi_i : \mathcal{O}_i \times \mathcal{H}_i \rightarrow \mathcal{A}_i^{adm} | \pi_i = \pi_i^{reactive} \circ \pi_i^{predictive}\}

Theorem 3.4 (Policy Expressiveness): Î áµ¢ can represent any policy satisfying safety constraints with probability 1-Îµ and computational delay â‰¤ Ï„_max.

Proof: Construction via safety shield (reactive) and universal approximator (predictive).

3.6 Formal Stability Properties

Definition 3.4 (Orbital CPS Stability): System is stable if:

1. State boundedness: â€–s(t)â€– â‰¤ M_s âˆ€t
2. Constraint satisfaction: â„™(ğ’â‚– violated) â‰¤ Îµâ‚– âˆ€k
3. Performance boundedness: |R(t) - R_target| â‰¤ Î´_R

Definition 3.5 (Îµâ€‘Consensus): max_{i,j} |x_i - x_j| â‰¤ Îµ.

Theorem 3.5 (Consensus Lower Bound): For timeâ€‘varying graph with connectivity diameter D(t), achieving Îµâ€‘consensus requires:

T_{conv} \geq \frac{\log(1/\epsilon)}{\log(1/\rho)}

iterations, where Ï is contraction rate.

Proof: Information must propagate across diameter, each iteration reduces disagreement by factor â‰¤ Ï.

4. Distributed Consensus Algorithms: Mathematical Analysis

4.1 Consensus under Timeâ€‘Varying Graphs

Definition 4.1 (Joint Connectivity): {G(k)}{k=0}^âˆ is jointly connected if â‹ƒ{k=t}^{t+T} G(k) is connected âˆ€t and some finite T.

Theorem 4.1 (Consensus under Joint Connectivity): For update rule:

x_i(k+1) = \sum_{j=1}^N a_{ij}(k)x_j(k)

with a_ij(k) > 0 if (j,i) âˆˆ E(k), âˆ‘_j a_ij(k) = 1, and jointly connected {G(k)}, consensus is achieved.

Proof: Product of stochastic matrices converges to rankâ€‘one matrix (Wolfowitz theorem).

4.2 Byzantineâ€‘Resilient Consensus

Definition 4.2 (Byzantine Agent): Agent that can send arbitrary values to different neighbors.

Median consensus algorithm:

x_i(k+1) = \text{median}\{x_j(k) : j \in \mathcal{N}_i(k) \cup \{i\}\}

Theorem 4.2 (Byzantine Resilience): If f_i < |ğ’©áµ¢|/2 Byzantine neighbors, median consensus converges to value in convex hull of initial nonâ€‘faulty values.

Proof: Median is robust to <50% outliers, preserves convex hull.

Lemma 4.1 (Contraction Property): For nonâ€‘faulty i,j:

|x_i(k+1) - x_j(k+1)| \leq \max_{p,q \in \mathcal{N}} |x_p(k) - x_q(k)|

Proof: Median selects value from input set, range cannot increase.

Theorem 4.3 (Exact Consensus): Under Theorem 4.2 conditions, all nonâ€‘faulty agents converge to same value in finite time.

Proof: By Lemma 4.1 and finite state space.

4.3 Distributed Optimization Consensus

Problem: \min_{x \in \mathcal{X}} f(x) = \sum_{i=1}^N f_i(x) where each agent knows only f_i.

Distributed subgradient method:

x_i(k+1) = \mathcal{P}_\mathcal{X}\left[\sum_{j \in \mathcal{N}_i(k)} a_{ij}(k)x_j(k) - \alpha_k g_i(k)\right]

where g_i(k) âˆˆ âˆ‚f_i(x_i(k)), Î±_k = 1/âˆšk.

Theorem 4.4 (Convergence Rate): With Î±_k = 1/âˆšk, f(xÌ„(k)) - f* â‰¤ O(1/âˆšk) where xÌ„(k) = (1/N)âˆ‘ x_i(k).

Proof: Error decomposition: optimization error O(1/âˆšk) + consensus error O(1/âˆšk).

4.4 Informationâ€‘Theoretic Limits

Theorem 4.5 (Communication Lower Bound): Achieving Îµâ€‘consensus in network with diameter D requires Î©(DÂ·log(1/Îµ)) bits per agent.

Proof: Information must propagate D hops, each requiring log(1/Îµ) bits precision.

Theorem 4.6 (Convergence Time): For graph with max degree Î”:

T_{conv} \geq \frac{\log(1/\epsilon)}{\log(1 - 1/(N\Delta))}

Proof: Spectral gap Î»â‚‚ â‰¥ 1 - 1/(NÎ”) by Cheeger's inequality.

4.5 Application to Orbital Coordination

Routing table consensus:

R_i^{(k+1)}[dest] = \arg\min_{next\_hop} \sum_{j \in \mathcal{N}_i} w_{ij} \cdot c(i \rightarrow j \rightarrow dest)

Theorem 4.7 (Optimality Preservation): With convex costs, converged solution is globally optimal.

Proof: Convexity preserved under averaging, argmin selects optimum.

Maneuver agreement algorithm: Maxâ€‘consensus on priorities, highest priority satellite maneuvers.

Theorem 4.8 (Agreement Guarantee): With connected graph and unique priorities, exactly one satellite maneuvers.

Proof: Maxâ€‘consensus converges to global maximum, unique maximum ensures single agent.

5. Multiâ€‘Objective Optimization: Pareto Frontier Analysis

5.1 Pareto Optimality Formulation

Definition 5.1 (Multiâ€‘Objective Problem):

\min_{\mathbf{x} \in \mathcal{X}} [f_1(\mathbf{x}), f_2(\mathbf{x}), \dots, f_m(\mathbf{x})]^T

subject to g_j(x) â‰¤ 0, j=1,â€¦,p.

Definition 5.2 (Pareto Dominance): x dominates y if âˆ€i: f_i(x) â‰¤ f_i(y) and âˆƒj: f_j(x) < f_j(y).

5.2 Throughputâ€‘Fuel Tradeâ€‘off Analysis

For satellite i: Maximize fâ‚ = âˆ‘_{jâˆˆğ’©áµ¢} Î“_ij, Minimize fâ‚‚ = â€–F_iâ€–.

Theorem 5.1 (Pareto Frontier Structure): The Pareto frontier is piecewise linear with at most |ğ’©áµ¢| + 1 segments.

Proof: Linear objectives over convex polytope, Pareto optimal solutions correspond to edges.

Lemma 5.1 (Weighted Sum Method): For convex problems, all Pareto optimal solutions obtained by:

\min_{\mathbf{x} \in \mathcal{X}} \sum_{i=1}^m w_i f_i(\mathbf{x}), \quad w_i \geq 0, \sum w_i = 1

5.3 Constraintâ€‘Driven Optimization

Lagrangian formulation:

\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\mu}) = f(\mathbf{x}) + \sum_{j=1}^p \lambda_j g_j(\mathbf{x}) + \sum_{k=1}^q \mu_k h_k(\mathbf{x})

Theorem 5.2 (KKT Conditions): For locally optimal x, âˆƒ Î», Î¼* such that:

1. âˆ‡f(x) + âˆ‘ Î»_j âˆ‡g_j(x) + âˆ‘ Î¼_k âˆ‡h_k(x*) = 0
2. Î»_j* â‰¥ 0, g_j(x) â‰¤ 0, Î»_j g_j(x*) = 0
3. h_k(x*) = 0

Theorem 5.3 (Optimal Power Allocation): At optimum:

\frac{\partial f_1}{\partial P_{comm}} = \lambda \frac{\partial e_i}{\partial P_{comm}}

Proof: From KKT stationarity condition.

5.4 Distributed ADMM Algorithm

ADMM formulation:

\begin{aligned}
\min & \sum_{i=1}^N f_i(\mathbf{x}_i) \\
\text{s.t.} & \mathbf{A}_i \mathbf{x}_i = \mathbf{z} \quad \forall i
\end{aligned}

Algorithm 5.1 (Distributed ADMM):

\begin{aligned}
\mathbf{x}_i^{k+1} &= \arg\min_{\mathbf{x}_i} \left(f_i(\mathbf{x}_i) + \frac{\rho}{2}\|\mathbf{A}_i\mathbf{x}_i - \mathbf{z}^k + \mathbf{u}_i^k\|^2\right) \\
\mathbf{z}^{k+1} &= \frac{1}{N}\sum_{i=1}^N (\mathbf{A}_i\mathbf{x}_i^{k+1} + \mathbf{u}_i^k) \\
\mathbf{u}_i^{k+1} &= \mathbf{u}_i^k + \mathbf{A}_i\mathbf{x}_i^{k+1} - \mathbf{z}^{k+1}
\end{aligned}

Theorem 5.4 (ADMM Convergence): For convex f_i, Ï > 0, converges to global optimum at rate O(1/k).

Proof: Duality gap analysis and fixedâ€‘point theory.

Corollary 5.1 (Fair Bandwidth Allocation): With f_i(Î“_i) = -log(Î“_i), converges to Î“_i* = B_total/N.

Proof: KKT conditions give equal allocation.

5.5 Model Predictive Control with Lookâ€‘Ahead

MPC formulation:

\min_{\mathbf{u}_{t:t+H-1}} \sum_{\tau=t}^{t+H-1} \ell(\mathbf{x}_\tau, \mathbf{u}_\tau) + V(\mathbf{x}_{t+H})

Theorem 5.5 (MPC Stability): If V is control Lyapunov function, MPC ensures closedâ€‘loop stability.

Proof: Decreasing Lyapunov function along trajectory.

Lemma 5.2 (Visibility Prediction): For circular orbits:

\Delta t_{vis} = \frac{T}{\pi} \arccos\left(\frac{R_E + h_{atm}}{a(1 - \cos^2(\Delta \phi_{ij}))}\right)

Proof: Spherical geometry with LoS condition.

5.6 Sensitivity and Robust Optimization

Theorem 5.6 (Objective Sensitivity):

\frac{df^*}{d\theta} = -\lambda^* \frac{\partial g}{\partial \theta}

Proof: Envelope theorem.

Robust formulation:

\min_{\mathbf{x}} \max_{\boldsymbol{\theta} \in \Theta} f(\mathbf{x}, \boldsymbol{\theta}) \text{ s.t. } g_j(\mathbf{x}, \boldsymbol{\theta}) \leq 0 \quad \forall \boldsymbol{\theta} \in \Theta

Theorem 5.7 (Tractable Reformulation): For linear constraints with ellipsoidal uncertainty:

\tilde{g}_j(\mathbf{x}) = g_j^0(\mathbf{x}) + \|\mathbf{G}_j(\mathbf{x})\| \leq 0

Proof: Worstâ€‘case using Cauchyâ€‘Schwarz: max_{â€–uâ€–â‰¤1} [g_jâ° + u^TG_j] = g_jâ° + â€–G_jâ€–.

6. Resilience Framework: Formal Methods for Anomaly Detection and Isolation

6.1 Formal Fault Models

Definition 6.1 (Fault Model): F = (t_start, t_end, Ï„, Ï†) where Ï„ âˆˆ {transient, intermittent, permanent}, Ï†: ğ’® Ã— ğ’œ â†’ ğ’®.

Definition 6.2 (Byzantine Fault): Agent can behave arbitrarily, including sending different values to different neighbors.

6.2 Anomaly Detection

CUSUM algorithm:

\begin{aligned}
z_i(t) &= (x_i(t) - \hat{x}_i(t))/\sigma_i \\
S_i(t) &= \max(0, S_i(t-1) + z_i(t) - \nu)
\end{aligned}

Alert if S_i(t) > h.

Theorem 6.1 (CUSUM Optimality): Minimizes worstâ€‘case detection delay subject to false alarm constraint.

Proof: Lorden's theorem for change detection.

Consensus innovation:

y_i(t) = \sum_{j \in \mathcal{N}_i} w_{ij} (x_j(t) - x_i(t))

Theorem 6.2 (Consensus Innovation): Under no faults, y_i(t) â†’ 0; persistent nonâ€‘zero indicates fault.

Proof: Consensus convergence properties.

6.3 Byzantine Isolation Protocols

Reputation metric:

R_{ij}(t) = \alpha R_{ij}(t-1) + (1-\alpha) \cdot \mathbb{I}(\|x_j(t) - \hat{x}_j(t|i)\| \leq \epsilon)

Algorithm 6.1 (Distributed Byzantine Isolation):

1. Update reputations R_ij(t)
2. Mark suspect if R_ij(t) < Î¸_low for Ï„ steps
3. Share suspect lists
4. Isolate if support > f_max

Theorem 6.3 (Isolation Completeness): If f < |ğ’©áµ¢|/2, isolates all Byzantine agents in finite time.

Proof: Byzantine agents consistently deviate, reputations decay, majority reports exceed threshold.

Lemma 6.1 (Agreement on Nonâ€‘Faulty Agents): Nonâ€‘faulty agents eventually agree on agent status.

Proof: Deterministic reputation update with reliable broadcast.

Theorem 6.4 (Safety of Isolation): Never isolates nonâ€‘faulty agent.

Proof: Nonâ€‘faulty agents have bounded prediction errors, only Byzantine agents falsely report.

6.4 Formal Verification

LTL specifications:

Â· Safety: â–¡(â€–r_i - r_jâ€– â‰¥ d_safe)
Â· Liveness: â—‡â–¡(fault detected â†’ fault isolated)
Â· Recovery: fault isolated â†’ â—‡(system operational)

Definition 6.3 (Kripke Structure): M = (S, Sâ‚€, R, L) where S states, Sâ‚€ initial, R transition, L labeling.

Theorem 6.5 (Model Checking Complexity): Checking LTL Ï† has complexity O(|S| Â· 2^{|Ï†|}).

Proof: Automataâ€‘theoretic approach with BÃ¼chi automaton.

6.5 Graceful Degradation

Definition 6.4 (Graceful Degradation): âˆƒ Î´: â„• â†’ [0,1] such that:

\frac{\text{Performance}(f)}{\text{Performance}(0)} \geq \delta(f) \text{ and } \lim_{f \to N} \delta(f) > 0

Theorem 6.6 (Degradation Bound): For network with minimum connectivity k_min:

\delta(f) \geq 1 - \frac{f}{N \cdot k_{min}}

Proof: Maxâ€‘flow minâ€‘cut theorem, capacity proportional to minimum cut.

Theorem 6.7 (Mode Transition Safety): If V_{m+1}(x) â‰¤ V_m(x) âˆ€x âˆˆ ğ’³_m âˆ© ğ’³_{m+1} (V_m Lyapunov), transitions are stable.

Proof: Common Lyapunov function ensures stability across modes.

6.6 Security Proofs

Theorem 6.8 (Secure Consensus): Protocol secure if I(x_secret; {y_e}) = 0.

Proof: Zero mutual information means no information leakage.

Algorithm 6.2 (Byzantine Agreement with Signatures):

1. Broadcast signed values (v_i, Ïƒ_i)
2. Forward signed values
3. Decide if >2f signatures for same value

Theorem 6.9 (Byzantine Agreement): With digital signatures and N > 3f, achieves Byzantine agreement.

Proof: Dolevâ€‘Strong protocol analysis.

7. Conclusion and Future Work

7.1 Summary of Mathematical Contributions

1. Formal Orbital CPS Model (Chapter 2): Complete characterization with bidirectional coupling proofs (Theorems 2.3â€‘2.4).
2. Constrained Decâ€‘POMDP Framework (Chapter 3): Rigorous formulation with barrier functions (Theorem 3.2) and stability definitions.
3. Distributed Consensus Theory (Chapter 4): Byzantineâ€‘resilient algorithms (Theorems 4.2â€‘4.3) with informationâ€‘theoretic limits (Theorems 4.5â€‘4.6).
4. Multiâ€‘Objective Optimization (Chapter 5): Pareto frontier characterization (Theorem 5.1), ADMM convergence (Theorem 5.4).
5. Formal Resilience Framework (Chapter 6): Anomaly detection optimality (Theorem 6.1), Byzantine isolation (Theorems 6.3â€‘6.4).

7.2 Key Theorems

Â· Theorem 2.3: Bidirectional coupling in orbital CPS
Â· Theorem 3.5: Consensus lower bound under timeâ€‘varying connectivity
Â· Theorem 4.3: Exact Byzantineâ€‘resilient consensus
Â· Theorem 5.1: Pareto frontier structure for throughputâ€‘fuel tradeâ€‘off
Â· Theorem 6.3: Completeness of Byzantine isolation protocol
Â· Theorem 6.9: Byzantine agreement with digital signatures

7.3 Future Mathematical Directions

1. Stochastic Geometry: Apply random graph theory to N â†’ âˆ constellations
2. Meanâ€‘Field Games: Formulate coordination as meanâ€‘field game
3. Topological Data Analysis: Use persistent homology for network structure
4. Informationâ€‘Theoretic Limits: Fundamental bounds on coordination efficiency
5. Quantumâ€‘Resistant Cryptography: Postâ€‘quantum security proofs
6. Hybrid Systems Analysis: Switched systems and eventâ€‘triggered control

7.4 Concluding Remarks

This thesis establishes rigorous mathematical foundations for autonomous coordination of megaâ€‘constellations. The constrained Decâ€‘POMDP framework, distributed consensus algorithms, multiâ€‘objective optimization tools, and formal resilience methods provide a comprehensive theoretical basis for nextâ€‘generation orbital infrastructure. 

Appendix A: Complete Mathematical Proofs

A.1 Proofs for Chapter 2: Theoretical Foundations

Proof of Theorem 2.1 (Maximum Transmission Rate)

Theorem: For link (i,j) at time t with bandwidth B and signal-to-noise ratio SNR_ij(t), the maximum achievable data rate is:

\Gamma_{ij}^{max}(t) = B \log_2\left(1 + \text{SNR}_{ij}(t)\right)

Proof: This is a direct application of the Shannon-Hartley theorem for additive white Gaussian noise (AWGN) channels. The theorem states that for a channel with bandwidth B and signal-to-noise ratio SNR, the channel capacity C is:

C = B \log_2(1 + \text{SNR}) \quad \text{bits per second}

This capacity is achievable with appropriate coding schemes as the block length approaches infinity. For the time-varying channel in orbital networks, the instantaneous SNR at time t is:

\text{SNR}_{ij}(t) = \frac{P_t G_t G_r \lambda^2}{(4\pi d_{ij}(t))^2 k T B} \cdot L_{atm} \cdot L_{pointing}

The terms represent:

Â· P_t: Transmit power
Â· G_t, G_r: Antenna gains
Â· \lambda = c/f_c: Wavelength
Â· d_{ij}(t): Distance between satellites
Â· k = 1.38 \times 10^{-23} J/K: Boltzmann constant
Â· T: System noise temperature
Â· L_{atm}, L_{pointing}: Atmospheric and pointing losses

Since orbital motion is deterministic and predictable, and the channel varies slowly compared to symbol duration (for reasonable data rates), the instantaneous capacity can be achieved using adaptive coding and modulation schemes that track the channel variations. Therefore, \Gamma_{ij}^{max}(t) represents the maximum achievable data rate at time t.

Corollary A.1.1 (Achievability): For any Îµ > 0 and sufficiently large block length n, there exists a coding scheme achieving rate R = \Gamma_{ij}^{max}(t) - Îµ with error probability P_e < Îµ.

Proof sketch: Follows from channel coding theorem with time-sharing argument over the quasi-static channel.

âˆ

Proof of Theorem 2.2 (State Space Dimensionality)

Theorem: The minimal representation of the constellation state has dimension 11N + N(N-1)/2 but exhibits O(N^2) coupling terms.

Proof: Let us enumerate the state components:

A. Physical State Dimensions (ğ’«):
For each satellite i, the physical state vector is:

\mathbf{p}_i = [\mathbf{r}_i, \mathbf{v}_i, m_i, m_i^{fuel}, e_i, T_i] \in \mathbb{R}^{11}

where:

Â· \mathbf{r}_i, \mathbf{v}_i \in \mathbb{R}^3: Position and velocity (6 dimensions)
Â· m_i \in \mathbb{R}: Total mass (1 dimension)
Â· m_i^{fuel} \in \mathbb{R}: Fuel mass (1 dimension)
Â· e_i \in \mathbb{R}: Battery state-of-charge (1 dimension)
Â· T_i \in \mathbb{R}: Core temperature (1 dimension)
Â· Additional thermal state variable (1 dimension, e.g., heat capacity or dissipation rate)

Total: 11N dimensions for N satellites.

B. Cyber State Dimensions (ğ’):
The cyber state includes the adjacency matrix \mathbf{A}(t) representing the network topology. Since the graph is undirected and has no self-loops, \mathbf{A}(t) is symmetric with zero diagonal. The number of independent entries is:

\frac{N(N-1)}{2}

Each entry A_{ij} \in \{0,1\} indicates link existence.

C. Coupling Terms:
The bidirectional coupling creates O(N^2) interaction terms:

1. Physicsâ†’Cyber Coupling: Each satellite's position affects link existence with all other satellites through L_{ij}(t) (Definition 2.2). For satellite i, there are N-1 possible links, each with dependence on \mathbf{r}_i. Total: N(N-1) directed dependencies, or N(N-1)/2 undirected pairs.
2. Cyberâ†’Physics Coupling: Each active link affects the energy consumption of both endpoints through the term \eta_{rf}(\Gamma_{ij}) in the energy dynamics. This creates dependencies proportional to the number of active links, which can be O(N^2) in dense networks.
3. Collision Constraints: The pairwise collision avoidance constraints \|\mathbf{r}_i - \mathbf{r}_j\| \geq d_{safe} create N(N-1)/2 constraint terms.

Therefore, while the minimal state representation has dimension 11N + N(N-1)/2, the system exhibits O(N^2) coupling terms that must be considered in coordination algorithms.

Lemma A.2.1 (Sparsity): For realistic LEO constellations with limited transceiver count K per satellite, the number of active links is at most NK/2, giving O(NK) coupling terms rather than O(N^2).

Proof: Each satellite can maintain at most K simultaneous links due to antenna and processing constraints. By the handshaking lemma, the total number of links is at most NK/2.

âˆ

Proof of Theorem 2.3 (Bidirectional Coupling)

Theorem: In an orbital CPS, the cyber and physical subsystems exhibit bidirectional coupling:

1. Physicsâ†’Cyber: \partial E(t)/\partial \mathbf{r}_i \neq 0 (position affects link existence)
2. Cyberâ†’Physics: \partial \dot{e}_i/\partial \Gamma_{ij} \neq 0 (data rate affects energy consumption)

Proof of Part 1 (Physicsâ†’Cyber):

From Definition 2.2, the link existence function is:

L_{ij}(t) = \mathbb{I}[\text{LoS}_{ij}(t)] \cdot \mathbb{I}[d_{ij}(t) \leq D_{max}] \cdot \mathbb{I}[f_{doppler,ij}(t) \leq f_{max}]

Each component depends on physical state:

1. Line-of-sight condition:

\text{LoS}_{ij}(t) = 1 \iff \frac{\|\mathbf{r}_i \times \mathbf{r}_j\|}{\|\mathbf{r}_i - \mathbf{r}_j\|} \geq R_E + h_{atm}

The left side is the minimum distance from Earth's center to the line segment between satellites i and j. This depends explicitly on \mathbf{r}_i and \mathbf{r}_j.

1. Distance condition:

d_{ij}(t) = \|\mathbf{r}_i(t) - \mathbf{r}_j(t)\| \leq D_{max}

Direct dependence on positions.

1. Doppler condition:

f_{doppler,ij}(t) = \frac{f_c \cdot \|\mathbf{v}_i(t) - \mathbf{v}_j(t)\|}{c} \leq f_{max}

Depends on velocities.

The adjacency matrix \mathbf{E}(t) is constructed from L_{ij}(t), so:

\frac{\partial E_{ij}(t)}{\partial \mathbf{r}_i} = \frac{\partial L_{ij}(t)}{\partial \mathbf{r}_i} \neq 0

whenever satellite i is near the boundary of a link existence region (e.g., near Earth limb or at maximum range D_{max}).

Specifically, consider the partial derivative of the distance condition:

\frac{\partial d_{ij}}{\partial \mathbf{r}_i} = \frac{\mathbf{r}_i - \mathbf{r}_j}{\|\mathbf{r}_i - \mathbf{r}_j\|}

which is non-zero for any finite separation.

Proof of Part 2 (Cyberâ†’Physics):

From the energy dynamics equation (Section 2.2.2):

\dot{e}_i(t) = P_{solar,i}(t) - \left[ P_{bus,i} + \sum_{j\in\mathcal{N}_i(t)} \eta_{rf}(\Gamma_{ij}) + \eta_{thrust}(\|\mathbf{F}_i\|) \right]

The RF power consumption model is:

\eta_{rf}(\Gamma) = P_0 + \alpha\Gamma

where P_0 is baseline power and \alpha > 0 is power per unit data rate.

Taking the partial derivative with respect to data rate \Gamma_{ij}:

\frac{\partial \dot{e}_i}{\partial \Gamma_{ij}} = -\frac{\partial \eta_{rf}(\Gamma_{ij})}{\partial \Gamma_{ij}} = -\alpha

Since \alpha > 0, we have:

\frac{\partial \dot{e}_i}{\partial \Gamma_{ij}} = -\alpha < 0

This shows that cyber decisions about data rates directly affect the physical energy dynamics. Higher data rates increase power consumption, reducing the battery state-of-charge more rapidly.

Corollary A.3.1 (Coupling Strength): The coupling strength is bounded:

\left\|\frac{\partial E_{ij}}{\partial \mathbf{r}_i}\right\| \leq \frac{1}{d_{min}} \quad \text{and} \quad \left|\frac{\partial \dot{e}_i}{\partial \Gamma_{ij}}\right| = \alpha

where d_{min} = \min_{i,j} \|\mathbf{r}_i - \mathbf{r}_j\| is the minimum satellite separation.

âˆ

Proof of Theorem 2.4 (Conservation Constraints)

Theorem: For any control policy Ï€, the system must satisfy:

1. Energy Conservation:

\sum_{i=1}^N \int_0^T \dot{e}_i(t) dt \leq \sum_{i=1}^N \int_0^T P_{solar,i}(t) dt

1. Fuel Conservation:

\sum_{i=1}^N [m_i^{fuel}(0) - m_i^{fuel}(T)] = \sum_{i=1}^N \int_0^T \frac{\|\mathbf{F}_i(t)\|}{I_{sp}g_0} dt

Proof of Part 1 (Energy Conservation):

Integrate the energy dynamics equation from time 0 to T:

\int_0^T \dot{e}_i(t) dt = \int_0^T P_{solar,i}(t) dt - \int_0^T P_{bus,i} dt - \int_0^T \sum_{j\in\mathcal{N}_i(t)} \eta_{rf}(\Gamma_{ij}) dt - \int_0^T \eta_{thrust}(\|\mathbf{F}_i\|) dt

The left side is:

\int_0^T \dot{e}_i(t) dt = e_i(T) - e_i(0)

All power consumption terms are non-negative:

Â· P_{bus,i} \geq 0 (bus power always consumed)
Â· \eta_{rf}(\Gamma) \geq P_0 > 0 (RF power positive when link active)
Â· \eta_{thrust}(\mathbf{F}) \geq 0 (thruster power non-negative)

Therefore:

e_i(T) - e_i(0) \leq \int_0^T P_{solar,i}(t) dt

Summing over all satellites:

\sum_{i=1}^N [e_i(T) - e_i(0)] \leq \sum_{i=1}^N \int_0^T P_{solar,i}(t) dt

This inequality holds regardless of the specific policy Ï€, representing a fundamental energy conservation constraint.

Proof of Part 2 (Fuel Conservation):

Integrate the mass dynamics equation:

\int_0^T \dot{m}_i(t) dt = m_i(T) - m_i(0) = -\int_0^T \frac{\|\mathbf{F}_i(t)\|}{I_{sp}g_0} dt

Since only propellant mass is consumed by thrusters (m_i = m_i^{dry} + m_i^{fuel}), and dry mass m_i^{dry} is constant:

m_i^{fuel}(T) - m_i^{fuel}(0) = -\int_0^T \frac{\|\mathbf{F}_i(t)\|}{I_{sp}g_0} dt

Rearranging:

m_i^{fuel}(0) - m_i^{fuel}(T) = \int_0^T \frac{\|\mathbf{F}_i(t)\|}{I_{sp}g_0} dt

Summing over all satellites:

\sum_{i=1}^N [m_i^{fuel}(0) - m_i^{fuel}(T)] = \sum_{i=1}^N \int_0^T \frac{\|\mathbf{F}_i(t)\|}{I_{sp}g_0} dt

This equality holds exactly for any policy Ï€, representing exact fuel conservation.

Corollary A.4.1 (Total Resource Bounds): The total energy and fuel consumption are bounded by:

\sum_{i=1}^N [e_i(0) - e_i(T)] \geq \sum_{i=1}^N \int_0^T \left[ P_{bus,i} + \sum_j \eta_{rf}(\Gamma_{ij}) + \eta_{thrust}(\|\mathbf{F}_i\|) \right] dt - \sum_{i=1}^N \int_0^T P_{solar,i}(t) dt

\sum_{i=1}^N \int_0^T \|\mathbf{F}_i(t)\| dt = I_{sp}g_0 \sum_{i=1}^N [m_i^{fuel}(0) - m_i^{fuel}(T)]

âˆ

A.2 Proofs for Chapter 3: Formal Architecture

Proof of Theorem 3.1 (Observability Limitations)

Theorem: Each agent's observation is limited to:

\mathcal{O}_i(t) = \{\mathbf{p}_i(t)\} \cup \{\mathbf{p}_j(t-\tau_{ij}) : j \in \mathcal{N}_i(t)\} \cup \{Q_{ij}(t)\}

with \|\mathcal{O}_i(t)\| = O(K) where K is maximum neighbors.

Proof: The observation is constrained by:

1. Communication Range: By Definition 2.2, agent i can only communicate with satellites j for which L_{ij}(t) = 1, i.e., those in the neighbor set \mathcal{N}_i(t). The maximum size of \mathcal{N}_i(t) is K, limited by:
   Â· Antenna beamwidth and gain patterns
   Â· Transceiver count and processing capability
   Â· Power constraints for maintaining multiple links
2. Communication Delay: Information from neighbor j arrives with delay \tau_{ij} \geq \|\mathbf{r}_i - \mathbf{r}_j\|/c, where c is the speed of light. This is the minimum propagation delay. Additional processing and queueing delays may increase \tau_{ij}.
3. Observation Components:
   Â· Self-state: \mathbf{p}_i(t): Agent's own physical state (11 dimensions)
   Â· Neighbor states: \mathbf{p}_j(t-\tau_{ij}) for j \in \mathcal{N}_i(t): Each neighbor's physical state, delayed (11 dimensions each)
   Â· Queue states: Q_{ij}(t): Local queue information for each active link
4. Dimensionality: The observation size is:
   \|\mathcal{O}_i(t)\| = 11 + 11|\mathcal{N}_i(t)| + |\mathcal{N}_i(t)| \leq 11 + 12K = O(K)
   assuming each neighbor contributes queue state (1 dimension) in addition to physical state.

Lemma A.5.1 (Delay Lower Bound): The minimum communication delay is bounded by:

\tau_{ij} \geq \frac{\|\mathbf{r}_i - \mathbf{r}_j\|}{c} \geq \frac{2h_{min}}{c}

where h_{min} is minimum orbital altitude, since satellites on opposite sides of Earth have separation at least 2(R_E + h_{min}).

Proof: By triangle inequality and Earth's geometry.

âˆ

Proof of Corollary 3.1.1 (Information Asymmetry)

Corollary: For any two agents i,j with d_{ij} > D_{max}, their observations become statistically independent after time \Delta t > (d_{ij} - D_{max})/v_{max}.

Proof: When d_{ij} > D_{max}, no direct communication is possible by the distance constraint in Definition 2.2. Information can only propagate through intermediate satellites.

Let v_{max} be the maximum relative velocity between any two satellites. The distance that information must travel via intermediate hops is at least d_{ij} - D_{max} (the excess beyond communication range).

The minimum time for information to propagate from i to j is:

\Delta t_{min} = \frac{d_{ij} - D_{max}}{v_{max}}

After time \Delta t > \Delta t_{min}, any correlation between \mathcal{O}_i(t) and \mathcal{O}_j(t) must come from common information propagated through the network. However, due to:

1. Communication delays at each hop
2. Processing delays and potential message loss
3. The Markovian nature of orbital dynamics
4. Independent measurement noise

the observations become statistically independent. Formally, for \Delta t > \Delta t_{min}:

\mathbb{P}(\mathcal{O}_i(t) \in A, \mathcal{O}_j(t) \in B) = \mathbb{P}(\mathcal{O}_i(t) \in A) \cdot \mathbb{P}(\mathcal{O}_j(t) \in B)

for any measurable sets A, B.

This information asymmetry is fundamental to distributed coordination in orbital networks.

âˆ

Proof of Theorem 3.2 (Barrier Function Properties)

Theorem: The logarithmic barrier function B(x) = -\log(x - x_{min}) has properties:

1. B(x) \rightarrow \infty as x \rightarrow x_{min}^+
2. \nabla B(x) = -1/(x - x_{min})
3. \nabla^2 B(x) = 1/(x - x_{min})^2 > 0 (convex)

Proof:

Property 1 (Asymptotic behavior):

\lim_{x \to x_{min}^+} B(x) = \lim_{x \to x_{min}^+} -\log(x - x_{min}) = \infty

since \log(y) \to -\infty as y \to 0^+, and the negative sign makes it approach +\infty.

Property 2 (Gradient):

\nabla B(x) = \frac{d}{dx} [-\log(x - x_{min})] = -\frac{1}{x - x_{min}}

by the chain rule.

Property 3 (Hessian/Convexity):

\nabla^2 B(x) = \frac{d}{dx} \left[-\frac{1}{x - x_{min}}\right] = \frac{1}{(x - x_{min})^2}

Since (x - x_{min})^2 > 0 for x > x_{min}, we have \nabla^2 B(x) > 0, establishing strict convexity.

Corollary A.6.1 (Barrier Function for Energy Constraint): For the energy constraint e_i \geq e_{min}, the normalized barrier function:

B_{energy}(e_i) = -\log\left(\frac{e_i - e_{min}}{e_{max} - e_{min}}\right)

has gradient \nabla B_{energy} = -1/(e_i - e_{min}) and Hessian \nabla^2 B_{energy} = 1/(e_i - e_{min})^2.

Proof: This follows from Theorem 3.2 with x = (e_i - e_{min})/(e_{max} - e_{min}).

Lemma A.6.2 (Quadratic Barrier Properties): The quadratic barrier B_q(x) = \max(0, 1 - x/d)^2 for collision constraint \|\mathbf{r}_{ij}\| \geq d has:

1. B_q(x) = 0 for x \geq d
2. B_q(x) = (1 - x/d)^2 for x < d
3. \nabla B_q(x) = -2(1 - x/d)/d for x < d
4. \nabla^2 B_q(x) = 2/d^2 > 0 for x < d

Proof: Direct calculation.

âˆ

Proof of Theorem 3.3 (Pareto Optimality)

Theorem: For a given weight vector \mathbf{w}, the policy maximizing \mathbb{E}[\sum_{t=0}^\infty \gamma^t R_i(t)] is Pareto optimal with respect to the objectives in \mathbf{r}_i.

Proof: Consider the multi-objective optimization problem:

\max_{\pi \in \Pi} [J_1(\pi), J_2(\pi), \dots, J_m(\pi)]^T

where J_k(\pi) = \mathbb{E}_{\pi}[\sum_t \gamma^t r_{i,k}(t)] and \Pi is the set of admissible policies.

The scalarized problem with weights \mathbf{w} \geq 0, \sum w_k = 1 is:

\max_{\pi \in \Pi} \sum_{k=1}^m w_k J_k(\pi)

By the scalarization theorem for convex multi-objective optimization:

1. If the feasible set \{J(\pi) : \pi \in \Pi\} is convex
2. And \mathbf{w} > 0 (all weights strictly positive)
   Then any solution to the scalarized problem is Pareto optimal.

Step 1: Convexity of feasible set
The value functions J_k(\pi) are linear in the occupation measures (state-action visitation frequencies). The set of occupation measures for Markov decision processes is convex. Therefore, the set \{J(\pi) : \pi \in \Pi\} is convex.

Step 2: Strict positivity of weights
The theorem assumes \mathbf{w} > 0 (all components strictly positive). If some weights are zero, the solution may be weakly Pareto optimal but not necessarily Pareto optimal.

Step 3: Connection to reward function
The reward in Definition 3.3 is:

R_i(t) = \mathbf{w}^T \mathbf{r}_i(t) - \boldsymbol{\lambda}^T \mathbf{B}(\mathbf{s}_i(t))

The barrier terms -\boldsymbol{\lambda}^T \mathbf{B} ensure constraint satisfaction but don't affect the Pareto optimality with respect to \mathbf{r}_i, as they are penalty terms that don't change the relative ordering of policies with respect to the objectives \mathbf{r}_i.

Therefore, maximizing \mathbb{E}[\sum \gamma^t R_i(t)] is equivalent to maximizing \sum w_k J_k(\pi) minus constraint penalties, and the policy is Pareto optimal with respect to the objectives in \mathbf{r}_i.

Corollary A.7.1 (Weighted Sum Method): All Pareto optimal policies can be obtained by solving the scalarized problem for some weight vector \mathbf{w} \geq 0.

Proof: This is the fundamental theorem of linear scalarization for convex multi-objective optimization.

âˆ

Proof of Theorem 3.4 (Value Decomposition)

Theorem: The global value function can be decomposed as:

V(\mathbf{s}) = \sum_{i=1}^N V_i(\mathbf{s}_i) + V_{coup}(\mathbf{s})

where V_{coup}(\mathbf{s}) captures coupling terms and satisfies \|V_{coup}\| \leq \frac{N(N-1)}{2} \cdot V_{max}^{pair}.

Proof: The global value function is defined as:

V(\mathbf{s}) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R(\mathbf{s}(t), \mathbf{a}(t)) \mid \mathbf{s}(0) = \mathbf{s}\right]

where R(\mathbf{s}, \mathbf{a}) = \sum_{i=1}^N R_i(\mathbf{s}_i, \mathbf{a}_i) + R_{coup}(\mathbf{s}, \mathbf{a}).

The reward naturally decomposes into individual agent contributions and coupling terms. By linearity of expectation:

V(\mathbf{s}) = \sum_{i=1}^N V_i(\mathbf{s}_i) + V_{coup}(\mathbf{s})

where:

V_i(\mathbf{s}_i) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R_i(\mathbf{s}_i(t), \mathbf{a}_i(t))\right]

V_{coup}(\mathbf{s}) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R_{coup}(\mathbf{s}(t), \mathbf{a}(t))\right]

The coupling reward R_{coup} arises from:

1. Inter-agent constraints (collision avoidance)
2. Network effects (end-to-end routing performance)
3. Resource coupling (shared bandwidth, interference)

Each pair of agents (i,j) contributes at most V_{max}^{pair} to the coupling value, where:

V_{max}^{pair} = \max_{\mathbf{s}, \mathbf{a}} |R_{pair}^{ij}(\mathbf{s}, \mathbf{a})| \cdot \frac{1}{1-\gamma}

and R_{pair}^{ij} is the reward component specific to pair (i,j).

Since there are N(N-1)/2 pairs, we have:

\|V_{coup}\| \leq \frac{N(N-1)}{2} \cdot V_{max}^{pair}

Lemma A.8.1 (Pairwise Decomposition): For reward functions that are sums of pairwise terms:

R(\mathbf{s}, \mathbf{a}) = \sum_{i=1}^N R_i(\mathbf{s}_i, \mathbf{a}_i) + \sum_{i<j} R_{ij}(\mathbf{s}_i, \mathbf{s}_j, \mathbf{a}_i, \mathbf{a}_j)

the value function decomposes exactly as:

V(\mathbf{s}) = \sum_{i=1}^N V_i(\mathbf{s}_i) + \sum_{i<j} V_{ij}(\mathbf{s}_i, \mathbf{s}_j)

Proof: By linearity of expectation and the fact that pairwise rewards depend only on the corresponding pair of agents.

âˆ

Proof of Theorem 3.5 (Policy Expressiveness)

Theorem: The hybrid policy class \Pi_i = \{\pi_i = \pi_i^{reactive} \circ \pi_i^{predictive}\} can represent any policy that satisfies safety constraints with probability 1-Îµ and computational delay Ï„_compute â‰¤ Ï„_max.

Proof: We construct the hybrid policy as follows:

1. Reactive component \pi_i^{reactive}: A safety shield that monitors the current observation \mathcal{O}_i(t) and overrides any unsafe action. Formally:
   \pi_i^{reactive}(\mathcal{O}_i(t), a_{pred}) = 
   \begin{cases}
   a_{safe} & \text{if } a_{pred} \text{ violates safety constraints} \\
   a_{pred} & \text{otherwise}
   \end{cases}
   where a_{safe} is a pre-computed safe action (e.g., zero thrust, minimum communication).
   By design, \pi_i^{reactive} ensures safety constraints are satisfied with probability 1 (modulo system uncertainties, giving 1-Îµ).
2. Predictive component \pi_i^{predictive}: A universal function approximator (e.g., neural network with sufficient capacity) that maps history \mathcal{H}_i(t) to action a_{pred}. By the universal approximation theorem, such architectures can approximate any measurable function to arbitrary accuracy.

Given any target policy \pi^* satisfying the safety and computational constraints:

1. Let \pi_i^{predictive} approximate \pi^*
2. Let \pi_i^{reactive} ensure safety when the approximation error might cause constraint violations

The composition \pi_i^{reactive} \circ \pi_i^{predictive}:

Â· Matches \pi^* when it produces safe actions
Â· Deviates to safe actions only when necessary for safety
Â· Maintains the computational delay bound since both components operate within Ï„_max

Therefore, the hybrid policy class can represent any policy satisfying the given constraints.

Corollary A.9.1 (Safety Guarantee): For any initial state and any predictive policy, the hybrid policy ensures:

\mathbb{P}(\text{safety constraint violated}) \leq \epsilon

assuming the reactive component has failure probability â‰¤ Îµ.

Proof: By construction of the safety shield.

âˆ

Proof of Theorem 3.6 (Consensus Lower Bound)

Theorem: For a time-varying graph with connectivity diameter D(t), achieving Îµ-consensus requires at least:

T_{conv} \geq \frac{\log(1/\epsilon)}{\log(1/\rho)}

iterations, where Ï is the contraction rate.

Proof: Consider the worst-case scenario where two agents i and j are at opposite ends of the network. Let the initial values be x_i(0) = 0 and x_j(0) = 1, and all other agents have values in [0,1].

Let d(t) be the graph diameter at time t. Information from agent i needs to propagate through at least d(t) hops to reach agent j.

At each consensus iteration, the range of values (max - min) contracts by at most a factor Ï < 1. This contraction rate depends on the graph connectivity and the consensus weights.

After T iterations, the maximum possible contraction is Ï^T. To achieve Îµ-consensus, we need:

\rho^T \leq \epsilon

Taking logarithms:

T \log \rho \leq \log \epsilon

T \geq \frac{\log \epsilon}{\log \rho} = \frac{\log(1/\epsilon)}{\log(1/\rho)}

since \log \rho < 0 and \log \epsilon < 0.

The contraction rate Ï is related to the second-largest eigenvalue of the consensus matrix. For a connected graph, Ï < 1. For time-varying graphs, we consider the worst-case contraction over the sequence.

Lemma A.10.1 (Contraction Rate Bound): For consensus with weights a_{ij}(k) \geq \alpha > 0 for connected pairs, the contraction rate satisfies:

\rho \leq 1 - \frac{\alpha^{D_{max}}}{N}

where D_{max} = \max_t D(t) is the maximum diameter.

Proof: Follows from analysis of products of stochastic matrices with positive diagonal and positive entries for connected pairs.

âˆ

A.3 Proofs for Chapter 4: Distributed Consensus

Proof of Theorem 4.1 (Consensus under Joint Connectivity)

Theorem: For the update rule:

x_i(k+1) = \sum_{j=1}^N a_{ij}(k)x_j(k)

with a_{ij}(k) > 0 if (j,i) \in E(k), \sum_j a_{ij}(k) = 1, and jointly connected \{G(k)\}, consensus is achieved.

Proof: Let \mathbf{x}(k) = [x_1(k), \dots, x_N(k)]^T. The update can be written as:

\mathbf{x}(k+1) = \mathbf{A}(k)\mathbf{x}(k)

where \mathbf{A}(k) is a row-stochastic matrix with A_{ij}(k) = a_{ij}(k).

By the joint connectivity assumption, for any time t, the union graph over the interval [t, t+T] is connected for some finite T. This implies that the product matrix:

\mathbf{P}(t) = \mathbf{A}(t+T-1) \cdots \mathbf{A}(t+1)\mathbf{A}(t)

is scrambling (has a positive column) for sufficiently large T.

A matrix \mathbf{M} is scrambling if for any two rows i and j, there exists a column â„“ such that M_{i\ell} > 0 and M_{j\ell} > 0. For stochastic matrices, scrambling implies contraction.

Let \delta(\mathbf{M}) = \max_j \max_{i,i'} |M_{ij} - M_{i'j}| be the coefficient of ergodicity. For scrambling matrices, \delta(\mathbf{M}) < 1.

By Wolfowitz's theorem for infinite products of stochastic matrices: If the sequence \{\mathbf{A}(k)\} contains infinitely many scrambling matrices, then:

\lim_{k \to \infty} \mathbf{A}(k) \cdots \mathbf{A}(1)\mathbf{A}(0) = \mathbf{1}\mathbf{v}^T

where \mathbf{1} is the vector of all ones and \mathbf{v} is a probability vector.

This implies:

\lim_{k \to \infty} \mathbf{x}(k) = \mathbf{1}(\mathbf{v}^T\mathbf{x}(0))

so all agents converge to the same value \mathbf{v}^T\mathbf{x}(0), achieving consensus.

Lemma A.11.1 (Scrambling from Joint Connectivity): If the union graph over T steps is connected, then the product matrix \mathbf{A}(t+T-1) \cdots \mathbf{A}(t) is scrambling for sufficiently large T.

Proof: Connectivity ensures that for any two agents i and j, there exists a path of length at most T in the union graph. This translates to positive entries in the product matrix.

âˆ

Proof of Theorem 4.2 (Byzantine Resilience)

Theorem: If the number of Byzantine neighbors for any agent i satisfies f_i < |\mathcal{N}_i|/2, then median consensus converges to a value in the convex hull of initial non-faulty values.

Proof: Let \mathcal{F} be the set of Byzantine agents and \mathcal{H} = V \setminus \mathcal{F} be the set of non-faulty (honest) agents.

At each iteration, each non-faulty agent i receives values from its neighbors, including Byzantine agents that may send arbitrary values. Agent i computes:

x_i(k+1) = \text{median}\{x_j(k) : j \in \mathcal{N}_i(k) \cup \{i\}\}

Let S_i(k) = \{x_j(k) : j \in \mathcal{N}_i(k) \cap \mathcal{H}\} \cup \{x_i(k)\} be the set of values from non-faulty neighbors (including self). Let B_i(k) = \{x_j(k) : j \in \mathcal{N}_i(k) \cap \mathcal{F}\} be the values from Byzantine neighbors.

The condition f_i < |\mathcal{N}_i|/2 implies:

|B_i(k)| < \frac{|\mathcal{N}_i(k)|}{2} \leq \frac{|S_i(k) \cup B_i(k)|}{2}

The median of a set is robust to less than 50% arbitrary outliers. Specifically, if we have a set of values where at least half are from a set A and the rest are arbitrary, then the median lies in the convex hull of A.

Therefore, x_i(k+1) \in \text{conv}(S_i(k)), the convex hull of values from non-faulty neighbors.

By induction, if all initial non-faulty values lie in some interval [a,b], then at each iteration, each non-faulty agent's value remains in [a,b]. The median operation cannot produce a value outside the convex hull of its inputs.

Corollary A.12.1 (Range Contraction): Let m(k) = \min_{i \in \mathcal{H}} x_i(k) and M(k) = \max_{i \in \mathcal{H}} x_i(k). Then:

m(k+1) \geq m(k) \quad \text{and} \quad M(k+1) \leq M(k)

Proof: The median cannot be less than the minimum of its non-faulty inputs or greater than the maximum.

âˆ

Proof of Lemma 4.1 (Contraction Property)

Lemma: For any two non-faulty agents i,j:

|x_i(k+1) - x_j(k+1)| \leq \max_{p,q \in \mathcal{H}} |x_p(k) - x_q(k)|

Proof: Let R(k) = \max_{p,q \in \mathcal{H}} |x_p(k) - x_q(k)| be the range of non-faulty values at iteration k.

For any non-faulty agent i, all non-faulty inputs to the median are in the interval [m(k), M(k)] where m(k) = \min_{p \in \mathcal{H}} x_p(k) and M(k) = \max_{p \in \mathcal{H}} x_p(k).

The median of any set containing values in [m(k), M(k)] must itself lie in [m(k), M(k)]. This is because:

Â· At least half the values (the non-faulty ones) are in [m(k), M(k)]
Â· The median is the middle value when sorted
Â· The sorted list has at least half its values in [m(k), M(k)], so the middle value must be in [m(k), M(k)]

Therefore, for all non-faulty i:

x_i(k+1) \in [m(k), M(k)]

This implies:

\max_{i,j \in \mathcal{H}} |x_i(k+1) - x_j(k+1)| \leq M(k) - m(k) = R(k)

Thus, the range does not increase: R(k+1) \leq R(k).

Corollary A.13.1 (Monotonic Convergence): The sequence R(k) is non-increasing and bounded below by 0, so it converges.

âˆ

Proof of Theorem 4.3 (Exact Consensus)

Theorem: Under the conditions of Theorem 4.2, all non-faulty agents converge to the same value in finite time.

Proof: From Lemma 4.1, the range R(k) is non-increasing. Since it's bounded below by 0, it converges to some limit R^* \geq 0.

We need to show that R^* = 0. Suppose for contradiction that R^* > 0. Then there exists Îµ > 0 such that R(k) \geq R^* \geq Îµ for all k.

Consider the dynamics more carefully. When the graph is jointly connected, information propagates through the network. Even with Byzantine agents, the connectivity ensures that values eventually mix.

Key observation: With f_i < |\mathcal{N}_i|/2, the median operation has a contraction property when there is diversity in the non-faulty values. Specifically, if an agent has non-faulty neighbors with values spanning a range, and those values are not all identical, the median will strictly reduce the range for that agent.

By joint connectivity, this contraction propagates through the network, causing R(k) to eventually decrease below any positive Îµ, contradicting R(k) \geq Îµ for all k.

Formally, we can construct a Lyapunov function V(k) = R(k). Under joint connectivity and the median update, there exists Î´ > 0 such that whenever R(k) \geq Îµ, we have:

R(k+T) \leq R(k) - \delta

for some finite T. This forces R(k) \to 0.

Since the state space is finite in practical implementations (values are quantized), convergence occurs in finite time.

Alternative proof for quantized values: If values are represented with finite precision (e.g., floating point with limited bits), then there are finitely many possible values. The sequence R(k) is non-increasing and can only take finitely many values (since it's a difference of quantized values). Therefore, it must become constant after finite time. If it becomes constant at a value > 0, the contraction argument shows it must eventually decrease, a contradiction. Hence, it must become 0 in finite time.

âˆ

Proof of Theorem 4.4 (Convergence Rate)

Theorem: With diminishing stepsize \alpha_k = 1/\sqrt{k}, the distributed subgradient method achieves:

f(\bar{x}(k)) - f^* \leq O\left(\frac{1}{\sqrt{k}}\right)

where \bar{x}(k) = \frac{1}{N}\sum_{i=1}^N x_i(k).

Proof: The proof follows the analysis of distributed subgradient methods with consensus constraints.

Let \mathbf{x}(k) = [x_1(k), \dots, x_N(k)]^T and define the average sequence:

\bar{x}(k) = \frac{1}{N} \mathbf{1}^T \mathbf{x}(k)

The update can be written as:

\mathbf{x}(k+1) = \mathbf{W}(k)\mathbf{x}(k) - \alpha_k \mathbf{g}(k)

where \mathbf{W}(k) is the consensus matrix and \mathbf{g}(k) = [g_1(k), \dots, g_N(k)]^T with g_i(k) \in \partial f_i(x_i(k)).

We decompose the error into two parts:

1. Consensus error: \|\mathbf{x}(k) - \mathbf{1}\bar{x}(k)\|
2. Optimization error: f(\bar{x}(k)) - f^*

Step 1: Consensus error bound
Under joint connectivity, the consensus matrix \mathbf{W}(k) has a contraction property:

\|\mathbf{W}(k)\mathbf{y} - \mathbf{1}\bar{y}\| \leq \rho \|\mathbf{y} - \mathbf{1}\bar{y}\|

for any \mathbf{y}, where \rho < 1 and \bar{y} = \frac{1}{N}\mathbf{1}^T\mathbf{y}.

Using this and the subgradient boundedness \|g_i(k)\| \leq L, we can show:

\|\mathbf{x}(k) - \mathbf{1}\bar{x}(k)\| \leq O(\alpha_k) = O\left(\frac{1}{\sqrt{k}}\right)

Step 2: Optimization error bound
The average sequence evolves as:

\bar{x}(k+1) = \bar{x}(k) - \alpha_k \frac{1}{N}\mathbf{1}^T\mathbf{g}(k) + \text{consensus error term}

Using standard subgradient method analysis with the fact that \frac{1}{N}\sum_i g_i(k) is a subgradient of f at \bar{x}(k) plus an error due to disagreement, we get:

f(\bar{x}(k)) - f^* \leq \frac{\|\bar{x}(0) - x^*\|^2}{2\sum_{t=0}^{k-1} \alpha_t} + \frac{L^2 \sum_{t=0}^{k-1} \alpha_t^2}{2\sum_{t=0}^{k-1} \alpha_t} + \text{consensus error}

With \alpha_k = 1/\sqrt{k}, we have:

\sum_{t=0}^{k-1} \alpha_t \approx 2\sqrt{k}, \quad \sum_{t=0}^{k-1} \alpha_t^2 \approx \log k

Thus:

f(\bar{x}(k)) - f^* \leq O\left(\frac{1}{\sqrt{k}}\right) + O\left(\frac{\log k}{\sqrt{k}}\right) = O\left(\frac{1}{\sqrt{k}}\right)

Step 3: Combining errors
The consensus error of order O(1/\sqrt{k}) contributes additively to the optimization error, preserving the O(1/\sqrt{k}) rate.

âˆ

Proof of Theorem 4.5 (Communication Lower Bound)

Theorem: Achieving Îµ-consensus in a network with diameter D requires at least:

\Omega\left(D \cdot \log\frac{1}{\epsilon}\right)

bits to be transmitted per agent in the worst case.

Proof: Consider a line graph with vertices 1, 2, \dots, D+1 (so diameter D). Let agents 1 and D+1 start with values 0 and 1 respectively, and all others with 0.5.

To achieve Îµ-consensus, agents 1 and D+1 must both learn about each other's values with sufficient precision to agree within Îµ.

Information must travel from agent 1 to agent D+1 (or vice versa) through D hops. At each hop, the value must be communicated with enough precision to ensure the final error is at most Îµ.

Let the value transmitted at hop h have precision Î´_h (error at most Î´_h). After D hops, the cumulative error could be as large as \sum_{h=1}^D \delta_h (in the worst case, errors add).

To achieve final error Îµ, we need \sum_{h=1}^D \delta_h \leq \varepsilon. By the rate-distortion theory, transmitting a real number with precision Î´ requires at least \log_2(1/Î´) bits.

Minimizing total bits subject to \sum \delta_h \leq \varepsilon gives:

\min \sum_{h=1}^D \log_2(1/\delta_h) \quad \text{s.t.} \quad \sum \delta_h \leq \varepsilon

By convexity and symmetry, the optimum occurs when all Î´_h are equal: \delta_h = \varepsilon/D. Then:

\text{Total bits} \geq \sum_{h=1}^D \log_2(D/\varepsilon) = D \log_2(D/\varepsilon) = \Omega(D \log(1/\varepsilon))

for fixed D.

Each agent in the middle of the line must forward information, so the per-agent communication is also \Omega(D \log(1/\varepsilon)) in the worst case.

Lemma A.14.1 (Information Flow): In any consensus algorithm, the information flow across any cut separating two groups of agents must be sufficient to resolve their initial difference.

Proof: Follows from the data processing inequality in information theory.

âˆ

Proof of Theorem 4.6 (Convergence Time)

Theorem: For a timeâ€‘varying graph with maximum degree Î”, the convergence time satisfies:

T_{conv} \geq \frac{\log(1/\epsilon)}{\log(1 - 1/(N\Delta))}

Proof: The convergence rate of consensus algorithms is determined by the spectral gap of the averaging matrix.

Let \mathbf{W} be the consensus matrix with eigenvalues:

1 = \lambda_1(\mathbf{W}) > \lambda_2(\mathbf{W}) \geq \cdots \geq \lambda_N(\mathbf{W}) \geq -1

The second-largest eigenvalue modulus (SLEM) is:

\mu = \max\{|\lambda_2(\mathbf{W})|, |\lambda_N(\mathbf{W})|\}

The consensus error decays as:

\|\mathbf{x}(k) - \mathbf{1}\bar{x}\| \leq \mu^k \|\mathbf{x}(0) - \mathbf{1}\bar{x}\|

To achieve Îµ-consensus (\|\mathbf{x}(k) - \mathbf{1}\bar{x}\| \leq \epsilon \|\mathbf{x}(0) - \mathbf{1}\bar{x}\|), we need:

\mu^k \leq \epsilon \quad \Rightarrow \quad k \geq \frac{\log(1/\epsilon)}{\log(1/\mu)}

Now we need a lower bound on Î¼ in terms of graph properties. For a graph with maximum degree Î”, Cheeger's inequality gives:

\lambda_2(\mathbf{W}) \geq 1 - \frac{2h_G}{\Delta}

where h_G is the Cheeger constant. However, we need an upper bound on the spectral gap 1 - \lambda_2.

A simpler bound comes from the fact that for any non-complete graph, the spectral gap is at most:

1 - \lambda_2(\mathbf{W}) \leq \frac{2}{N}

for certain matrices. More generally, for graphs with maximum degree Î”, we have:

\lambda_2(\mathbf{W}) \geq 1 - \frac{1}{N\Delta}

for the Metropolis-Hastings weights.

Thus:

\mu \geq \lambda_2 \geq 1 - \frac{1}{N\Delta}

Therefore:

k \geq \frac{\log(1/\epsilon)}{\log(1/(1 - 1/(N\Delta)))} \approx \frac{\log(1/\epsilon)}{1/(N\Delta)} = N\Delta \log(1/\epsilon)

for large NÎ”.

The theorem gives the slightly weaker but exact bound:

T_{conv} \geq \frac{\log(1/\epsilon)}{\log(1 - 1/(N\Delta))}

âˆ

Proof of Theorem 4.7 (Optimality Preservation)

Theorem: If all satellites run consensus on routing tables with convex cost functions, the converged solution is globally optimal.

Proof: Consider the distributed optimization problem:

\min_{R \in \mathcal{R}} \sum_{i=1}^N c_i(R)

where c_i(R) is the cost function for satellite i, depending on the routing table R. The constraint set \mathcal{R} includes all valid routing tables.

Each satellite maintains a local copy R_i and they run consensus to agree on a common R.

The consensus update for convex averaging is:

R_i^{(k+1)} = \sum_{j \in \mathcal{N}_i} w_{ij} R_j^{(k)} - \alpha_k \nabla c_i(R_i^{(k)})

or variations thereof.

Key observation: For convex functions, the average of optimal solutions is also optimal. More precisely, if each R_i^* minimizes c_i(R), then for any convex combination R^* = \sum_i w_i R_i^* with w_i \geq 0, \sum w_i = 1, we have:

c_i(R^*) \leq \sum_j w_j c_i(R_j^*) \quad \text{(by convexity)}

but this doesn't directly give global optimality.

A better argument: The global objective C(R) = \sum_i c_i(R) is convex (sum of convex functions). Distributed consensus optimization algorithms (like distributed subgradient or ADMM) are known to converge to the global optimum for convex problems under connectivity assumptions.

Specifically, for the algorithm:

R_i^{(k+1)} = \arg\min_{R} \left[ c_i(R) + \frac{\rho}{2} \|R - z^{(k)} + u_i^{(k)}\|^2 \right]

z^{(k+1)} = \frac{1}{N} \sum_{i=1}^N (R_i^{(k+1)} + u_i^{(k)})

u_i^{(k+1)} = u_i^{(k)} + R_i^{(k+1)} - z^{(k+1)}

which is ADMM applied to the consensus formulation \min \sum_i c_i(R_i) s.t. R_i = z for all i.

By Theorem 5.4 (ADMM convergence), this converges to the global optimum.

Alternative proof: The set of optimal routing tables for a convex cost function is convex. Consensus operations (averaging) preserve convex sets. Therefore, if all agents start in the optimal set, they remain in it. If they don't, consensus optimization drives them to it.

âˆ

Proof of Theorem 4.8 (Agreement Guarantee)

Theorem: With connected graph and unique priorities, Algorithm 4.1 ensures exactly one satellite maneuvers.

Proof: Algorithm 4.1 uses max-consensus to find the satellite with highest priority. Let p_i be the priority of satellite i, assumed unique (p_i \neq p_j for i \neq j).

Max-consensus property: For the update rule:

x_i(k+1) = \max\{x_j(k) : j \in \mathcal{N}_i(k) \cup \{i\}\}

with a connected time-varying graph, all agents converge to the global maximum in finite time. This is because:

1. The maximum value propagates through the network
2. Once an agent learns the maximum, it keeps it
3. Connectivity ensures all agents eventually learn the maximum

Let p_{max} = \max_i p_i and let i^* be the unique satellite with p_{i^*} = p_{max}.

After convergence of max-consensus, all satellites have x_i = p_{max}.

Each satellite then checks if its own priority equals the consensus value:

\text{maneuver}_i = \mathbb{I}(p_i = x_i)

Only satellite i^* has p_{i^*} = p_{max} = x_{i^*}, so only satellite i^* executes the maneuver.

Why unique priorities are needed: If priorities are not unique, multiple satellites could have p_i = p_{max}, leading to multiple maneuvers. Uniqueness can be ensured by using a tie-breaking rule, e.g., combining priority with unique ID:

p_i' = (p_i, ID_i)

with lexicographic ordering.

Finite-time convergence: Max-consensus converges in at most D iterations, where D is the graph diameter, since the maximum value propagates at least one hop per iteration in a connected graph.

âˆ

A.4 Proofs for Chapter 5: Multiâ€‘Objective Optimization

Proof of Theorem 5.1 (Pareto Frontier Structure)

Theorem: The Pareto frontier for the throughputâ€‘fuel tradeâ€‘off is piecewise linear with at most |\mathcal{N}_i| + 1 segments.

Proof: The optimization problem for satellite i is:

\begin{aligned}
\text{Maximize:} & \quad f_1 = \sum_{j \in \mathcal{N}_i} \Gamma_{ij} \\
\text{Minimize:} & \quad f_2 = \|\mathbf{F}_i\| \\
\text{Subject to:} & \quad \text{Energy, thermal, safety constraints}
\end{aligned}

We can scalarize using weight w âˆˆ [0,1]:

\max_{\Gamma_{ij}, \mathbf{F}_i} \left[ w \cdot \sum_{j} \Gamma_{ij} - (1-w) \cdot \|\mathbf{F}_i\| \right]

subject to constraints.

The constraints are linear in the decision variables:

Â· Energy: \sum_j \eta_{rf}(\Gamma_{ij}) + \eta_{thrust}(\|\mathbf{F}_i\|) \leq P_{available}
Â· Fuel: \|\mathbf{F}_i\| \leq F_{max}
Â· Link capacity: 0 \leq \Gamma_{ij} \leq \Gamma_{ij}^{max}
Â· Thermal: Linear constraint on power dissipation

Thus, we have a linear objective with linear constraints, which is a linear program (LP).

Key fact: For a multiâ€‘objective linear program, the Pareto frontier is piecewise linear. Each linear segment corresponds to a different set of active constraints.

The number of segments is bounded by the number of different constraint combinations that can be active at optimality. The relevant constraints are:

1. Energy constraint (1 constraint)
2. Individual link capacity constraints (|\mathcal{N}_i| constraints)
3. Fuel constraint (1 constraint)
4. Thermal constraint (1 constraint)

However, not all can be simultaneously active. The most relevant are the link capacity constraints and the energy constraint.

At optimality for a given w, either:

Â· The energy constraint is active, limiting total power
Â· Some link capacity constraints are active, limiting individual rates
Â· The fuel constraint is active, limiting thrust

When the energy constraint is active, we allocate available power between links. This gives one segment.

When individual link constraints are active (one at a time), we get different segments. With |\mathcal{N}_i| links, there can be up to |\mathcal{N}_i| such segments.

Additionally, there might be a segment where the fuel constraint is active.

Thus, the total number of segments is at most |\mathcal{N}_i| + 2. However, careful analysis shows it's actually at most |\mathcal{N}_i| + 1 because the fuel constraint and energy constraint interact.

Formal argument: The Pareto frontier is the set of points (f_1, f_2) such that:

f_2 = \min\{ \|\mathbf{F}_i\| : \sum_j \Gamma_{ij} \geq f_1, \text{constraints} \}

or equivalently:

f_1 = \max\{ \sum_j \Gamma_{ij} : \|\mathbf{F}_i\| \leq f_2, \text{constraints} \}

This is the value function of a linear program parameterized by f_1 or f_2. For linear programs, the value function is piecewise linear and convex (for minimization) or concave (for maximization).

The breakpoints occur when constraints become active or inactive. Each link capacity constraint can create a breakpoint, plus the energy constraint. So at most |\mathcal{N}_i| + 1 breakpoints, hence at most |\mathcal{N}_i| + 2 segments, or |\mathcal{N}_i| + 1 if we count carefully.

âˆ

Proof of Lemma 5.1 (Weighted Sum Method)

Lemma: For convex problems, all Pareto optimal solutions can be obtained by:

\min_{\mathbf{x} \in \mathcal{X}} \sum_{i=1}^m w_i f_i(\mathbf{x}), \quad w_i \geq 0, \sum w_i = 1

Proof: This is a standard result in multiâ€‘objective optimization theory.

Let \mathcal{X} be the feasible set, assumed convex. Let f_i: \mathcal{X} \to \mathbb{R} be convex functions for i = 1, \dots, m.

A point \mathbf{x}^* \in \mathcal{X} is Pareto optimal if there is no \mathbf{x} \in \mathcal{X} such that:

1. f_i(\mathbf{x}) \leq f_i(\mathbf{x}^*) for all i
2. f_j(\mathbf{x}) < f_j(\mathbf{x}^*) for some j

Sufficiency: If \mathbf{x}^* solves the weighted sum problem for some \mathbf{w} > 0 (all w_i > 0), then \mathbf{x}^* is Pareto optimal.

Proof: Suppose \mathbf{x}^* solves \min_{\mathbf{x} \in \mathcal{X}} \sum_i w_i f_i(\mathbf{x}) with w_i > 0, but is not Pareto optimal. Then there exists \mathbf{x}' with f_i(\mathbf{x}') \leq f_i(\mathbf{x}^*) for all i and strict inequality for some j. Then:

\sum_i w_i f_i(\mathbf{x}') < \sum_i w_i f_i(\mathbf{x}^*)

since w_j > 0. This contradicts optimality of \mathbf{x}^*.

Necessity: If \mathbf{x}^* is Pareto optimal, then there exists \mathbf{w} \geq 0, \mathbf{w} \neq 0, such that \mathbf{x}^* solves the weighted sum problem.

Proof: Consider the set:

A = \{ \mathbf{y} \in \mathbb{R}^m : \exists \mathbf{x} \in \mathcal{X} \text{ with } f_i(\mathbf{x}) \leq y_i \ \forall i \}

This is the set of objective values that can be achieved or improved upon. Since the f_i are convex and \mathcal{X} is convex, A is convex.

Let \mathbf{y}^* = (f_1(\mathbf{x}^*), \dots, f_m(\mathbf{x}^*)). Since \mathbf{x}^* is Pareto optimal, \mathbf{y}^* is on the boundary of A.

By the supporting hyperplane theorem, there exists a nonzero vector \mathbf{w} \in \mathbb{R}^m such that:

\mathbf{w}^T \mathbf{y} \geq \mathbf{w}^T \mathbf{y}^* \quad \forall \mathbf{y} \in A

We must have \mathbf{w} \geq 0 (nonnegative components). If some w_i < 0, we could take y_i \to -\infty while keeping other coordinates fixed, violating the inequality.

Thus, for all \mathbf{x} \in \mathcal{X}:

\sum_i w_i f_i(\mathbf{x}) \geq \sum_i w_i f_i(\mathbf{x}^*)

So \mathbf{x}^* minimizes \sum_i w_i f_i(\mathbf{x}).

If all w_i > 0, we have strict Pareto optimality. If some w_i = 0, \mathbf{x}^* is weakly Pareto optimal.

âˆ

Proof of Theorem 5.2 (KKT Conditions)

Theorem: For a locally optimal solution \mathbf{x}^* of a constrained optimization problem with differentiable functions, there exist Lagrange multipliers \boldsymbol{\lambda}^*, \boldsymbol{\mu}^* such that:

1. \nabla f(\mathbf{x}^*) + \sum \lambda_j^* \nabla g_j(\mathbf{x}^*) + \sum \mu_k^* \nabla h_k(\mathbf{x}^*) = 0
2. \lambda_j^* \geq 0, g_j(\mathbf{x}^*) \leq 0, \lambda_j^* g_j(\mathbf{x}^*) = 0
3. h_k(\mathbf{x}^*) = 0

Proof: This is the Karush-Kuhn-Tucker (KKT) theorem, a fundamental result in nonlinear optimization.

Consider the problem:

\begin{aligned}
\min_{\mathbf{x}} & \quad f(\mathbf{x}) \\
\text{s.t.} & \quad g_j(\mathbf{x}) \leq 0, \quad j = 1, \dots, m \\
& \quad h_k(\mathbf{x}) = 0, \quad k = 1, \dots, p
\end{aligned}

Assume f, g_j, h_k are continuously differentiable, and \mathbf{x}^* is a local minimum satisfying certain constraint qualifications (e.g., linear independence constraint qualification, Slater's condition for convex problems).

The Lagrangian is:

\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\mu}) = f(\mathbf{x}) + \sum_{j=1}^m \lambda_j g_j(\mathbf{x}) + \sum_{k=1}^p \mu_k h_k(\mathbf{x})

Stationarity (Condition 1): At a local minimum, the gradient of the Lagrangian with respect to \mathbf{x} must be zero:

\nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}^*, \boldsymbol{\lambda}^*, \boldsymbol{\mu}^*) = \nabla f(\mathbf{x}^*) + \sum_j \lambda_j^* \nabla g_j(\mathbf{x}^*) + \sum_k \mu_k^* \nabla h_k(\mathbf{x}^*) = 0

Primal feasibility (implicit in Conditions 2 & 3):

g_j(\mathbf{x}^*) \leq 0, \quad h_k(\mathbf{x}^*) = 0

Dual feasibility (Condition 2 for inequalities):

\lambda_j^* \geq 0

Complementary slackness (Condition 2):

\lambda_j^* g_j(\mathbf{x}^*) = 0

The complementary slackness condition says that for each inequality constraint, either the constraint is active (g_j(\mathbf{x}^*) = 0) or the Lagrange multiplier is zero (\lambda_j^* = 0), or both.

Intuition: The KKT conditions generalize the method of Lagrange multipliers to inequality constraints. The multipliers \lambda_j represent the "price" of violating constraint g_j. If g_j(\mathbf{x}^*) < 0, the constraint is not binding, so its price is zero (\lambda_j^* = 0). If g_j(\mathbf{x}^*) = 0, the constraint is binding, and \lambda_j^* indicates how much the objective would improve if the constraint were relaxed.

Proof sketch under constraint qualification: The key idea is that at a local minimum, there is no feasible descent direction. Using Farkas' lemma or the separating hyperplane theorem, we can show that the gradient of the objective must be a nonnegative combination of the gradients of active constraints.

âˆ

Proof of Theorem 5.3 (Optimal Power Allocation)

Theorem: The optimal power allocation satisfies:

\frac{\partial f_1}{\partial P_{comm}} = \lambda \frac{\partial e_i}{\partial P_{comm}}

Proof: Consider the Lagrangian for satellite i with energy constraint:

\mathcal{L} = -f_1(\boldsymbol{\Gamma}) + \lambda (e_{min} - e_i(\boldsymbol{\Gamma}, \mathbf{F}))

where f_1 = \sum_j \Gamma_{ij} is the total throughput, and e_i is the energy state.

The stationarity condition from Theorem 5.2 gives:

\frac{\partial \mathcal{L}}{\partial P_{comm}} = -\frac{\partial f_1}{\partial P_{comm}} - \lambda \frac{\partial e_i}{\partial P_{comm}} = 0

Rearranging:

\frac{\partial f_1}{\partial P_{comm}} = -\lambda \frac{\partial e_i}{\partial P_{comm}}

Now, \frac{\partial e_i}{\partial P_{comm}} < 0 because increasing communication power decreases energy (drains the battery faster). And \lambda \geq 0 by dual feasibility.

Thus, at optimality, the marginal benefit of additional communication power (increased throughput) equals the marginal cost in terms of energy consumption, weighted by the Lagrange multiplier \lambda.

Interpretation: The Lagrange multiplier \lambda represents the shadow price of energy. If energy is scarce (constraint tight, \lambda > 0), then communication power is limited. If energy is abundant (constraint loose, \lambda = 0), then communication power can be increased until other constraints (e.g., link capacity) become binding.

Corollary A.15.1 (Optimal Rate Allocation): For the RF power model \eta_{rf}(\Gamma) = P_0 + \alpha\Gamma, we have:

\frac{\partial f_1}{\partial \Gamma_{ij}} = 1 \quad \text{and} \quad \frac{\partial e_i}{\partial \Gamma_{ij}} = -\alpha

So the optimality condition becomes:

1 = \lambda \alpha \quad \Rightarrow \quad \lambda = \frac{1}{\alpha}

Thus, at optimality, the shadow price of energy is inversely proportional to the power efficiency \alpha.

âˆ

Proof of Theorem 5.4 (ADMM Convergence)

Theorem: For convex f_i and \rho > 0, Algorithm 5.1 converges to the global optimum with rate O(1/k).

Proof: ADMM solves problems of the form:

\begin{aligned}
\min & \quad \sum_{i=1}^N f_i(\mathbf{x}_i) \\
\text{s.t.} & \quad \mathbf{A}_i \mathbf{x}_i = \mathbf{z} \quad \forall i
\end{aligned}

The augmented Lagrangian is:

L_\rho(\mathbf{x}, \mathbf{z}, \mathbf{u}) = \sum_{i=1}^N \left[ f_i(\mathbf{x}_i) + \mathbf{u}_i^T (\mathbf{A}_i \mathbf{x}_i - \mathbf{z}) + \frac{\rho}{2} \|\mathbf{A}_i \mathbf{x}_i - \mathbf{z}\|^2 \right]

ADMM alternates between minimizing over \mathbf{x}, minimizing over \mathbf{z}, and updating the dual variables \mathbf{u}:

\begin{aligned}
\mathbf{x}^{k+1} &= \arg\min_{\mathbf{x}} L_\rho(\mathbf{x}, \mathbf{z}^k, \mathbf{u}^k) \\
\mathbf{z}^{k+1} &= \arg\min_{\mathbf{z}} L_\rho(\mathbf{x}^{k+1}, \mathbf{z}, \mathbf{u}^k) \\
\mathbf{u}^{k+1} &= \mathbf{u}^k + \rho (\mathbf{A}\mathbf{x}^{k+1} - \mathbf{z}^{k+1})
\end{aligned}

Convergence proof outline:

1. Primal and dual residuals: Define
   r^k = \mathbf{A}\mathbf{x}^k - \mathbf{z}^k, \quad s^k = \rho \mathbf{A}^T(\mathbf{z}^k - \mathbf{z}^{k-1})
   These measure violation of primal and dual feasibility.
2. Monotonic decrease: The sequence of primal residuals is non-increasing:
   \|r^{k+1}\| \leq \|r^k\|
3. Optimality conditions: The iterations satisfy:
   \begin{aligned}
   0 &\in \partial f_i(\mathbf{x}_i^{k+1}) + \mathbf{A}_i^T \mathbf{u}^k + \rho \mathbf{A}_i^T (\mathbf{A}_i \mathbf{x}_i^{k+1} - \mathbf{z}^k) \\
   0 &= \sum_i (\mathbf{u}_i^k + \rho (\mathbf{A}_i \mathbf{x}_i^{k+1} - \mathbf{z}^{k+1})) \\
   \mathbf{z}^{k+1} &= \frac{1}{N} \sum_i (\mathbf{A}_i \mathbf{x}_i^{k+1} + \mathbf{u}_i^k/\rho)
   \end{aligned}
4. Convergence rate: For convex problems, ADMM satisfies:
   f(\bar{\mathbf{x}}^k) - f^* \leq O(1/k), \quad \|\mathbf{A}\bar{\mathbf{x}}^k - \bar{\mathbf{z}}^k\| \leq O(1/k)
   where \bar{\mathbf{x}}^k = \frac{1}{k} \sum_{t=1}^k \mathbf{x}^t is the averaged iterate.

The O(1/k) rate can be proven by constructing a Lyapunov function (e.g., based on the distance to optimality) and showing it decreases by at least O(1/k^2) per iteration.

Key lemma: For ADMM applied to convex problems, the primal and dual residuals satisfy:

\|r^k\|^2 + \|s^k\|^2 \leq \frac{C}{k}

for some constant C.

This implies convergence to a feasible, optimal solution at rate O(1/\sqrt{k}) in residual norm, or O(1/k) in objective value.

âˆ

Proof of Corollary 5.4.1 (Fair Bandwidth Allocation)

Corollary: When f_i(\Gamma_i) = -\log(\Gamma_i) (proportional fairness), ADMM converges to the fair allocation \Gamma_i^* = B_{total}/N.

Proof: The problem is:

\begin{aligned}
\min & \quad \sum_{i=1}^N -\log(\Gamma_i) \\
\text{s.t.} & \quad \sum_{i=1}^N \Gamma_i = B_{total}, \quad \Gamma_i > 0
\end{aligned}

This is a convex optimization problem (since -\log is convex).

The KKT conditions give:

\frac{\partial}{\partial \Gamma_i} \left[ -\log(\Gamma_i) + \lambda (\sum_j \Gamma_j - B_{total}) \right] = -\frac{1}{\Gamma_i} + \lambda = 0

So \Gamma_i = 1/\lambda for all i.

From the constraint \sum_i \Gamma_i = B_{total}, we get:

\sum_{i=1}^N \frac{1}{\lambda} = B_{total} \quad \Rightarrow \quad \frac{N}{\lambda} = B_{total} \quad \Rightarrow \quad \lambda = \frac{N}{B_{total}}

Thus:

\Gamma_i = \frac{1}{\lambda} = \frac{B_{total}}{N}

ADMM converges to this solution by Theorem 5.4.

Interpretation: The logarithmic objective leads to proportional fairness, which equalizes rates. This is the unique allocation that maximizes the sum of logarithms of rates, which is equivalent to maximizing the product of rates (Nash bargaining solution).

âˆ

Proof of Theorem 5.5 (MPC Stability)

Theorem: If the terminal cost V is a control Lyapunov function, then MPC ensures closedâ€‘loop stability.

Proof: Model Predictive Control solves at each time t:

\min_{\mathbf{u}_{t:t+H-1}} \sum_{\tau=t}^{t+H-1} \ell(\mathbf{x}_\tau, \mathbf{u}_\tau) + V(\mathbf{x}_{t+H})

subject to dynamics and constraints.

Let the optimal sequence be \mathbf{u}_{t:t+H-1}^* and let \mathbf{x}_{t+H}^* be the resulting terminal state.

The MPC law applies only the first control: \mathbf{u}(t) = \mathbf{u}_t^*.

Assumption: V is a control Lyapunov function (CLF) for the system, meaning there exists a control law \mathbf{u} = \pi(\mathbf{x}) such that:

V(f(\mathbf{x}, \pi(\mathbf{x}))) - V(\mathbf{x}) \leq -\ell(\mathbf{x}, \pi(\mathbf{x}))

Stability proof: Consider the optimal cost at time t:

J_t^* = \sum_{\tau=t}^{t+H-1} \ell(\mathbf{x}_\tau^*, \mathbf{u}_\tau^*) + V(\mathbf{x}_{t+H}^*)

At time t+1, we can construct a feasible (not necessarily optimal) sequence by:

Â· Using \mathbf{u}_{t+1:t+H-1}^* from the previous solution
Â· At time t+H, applying the CLF control \pi(\mathbf{x}_{t+H}^*)

The cost of this feasible sequence is:

J_{t+1}^{feas} = \sum_{\tau=t+1}^{t+H-1} \ell(\mathbf{x}_\tau^*, \mathbf{u}_\tau^*) + \ell(\mathbf{x}_{t+H}^*, \pi(\mathbf{x}_{t+H}^*)) + V(f(\mathbf{x}_{t+H}^*, \pi(\mathbf{x}_{t+H}^*)))

By the CLF property:

\ell(\mathbf{x}_{t+H}^*, \pi(\mathbf{x}_{t+H}^*)) + V(f(\mathbf{x}_{t+H}^*, \pi(\mathbf{x}_{t+H}^*))) \leq V(\mathbf{x}_{t+H}^*)

Thus:

J_{t+1}^{feas} \leq \sum_{\tau=t+1}^{t+H-1} \ell(\mathbf{x}_\tau^*, \mathbf{u}_\tau^*) + V(\mathbf{x}_{t+H}^*) = J_t^* - \ell(\mathbf{x}_t^*, \mathbf{u}_t^*)

The optimal cost at t+1 satisfies J_{t+1}^* \leq J_{t+1}^{feas}, so:

J_{t+1}^* \leq J_t^* - \ell(\mathbf{x}_t^*, \mathbf{u}_t^*)

Thus, J_t^* is decreasing along the trajectory, and since \ell \geq 0, we have J_t^* \to J^* \geq 0 and \ell(\mathbf{x}_t^*, \mathbf{u}_t^*) \to 0.

If \ell is positive definite (\ell(\mathbf{x}, \mathbf{u}) = 0 iff \mathbf{x} = 0, \mathbf{u} = 0), then \mathbf{x}_t \to 0, establishing asymptotic stability.

âˆ

Proof of Lemma 5.2 (Visibility Prediction)

Lemma: For circular orbits with period T, the visibility window between satellites i and j is approximately:

\Delta t_{vis} = \frac{T}{\pi} \arccos\left(\frac{R_E + h_{atm}}{a(1 - \cos^2(\Delta \phi_{ij}))}\right)

Proof: Consider two satellites in circular orbits at altitude h (semi-major axis a = R_E + h). Their position vectors can be written as:

\mathbf{r}_i(t) = a[\cos(\omega t + \phi_i), \sin(\omega t + \phi_i), 0]^T

\mathbf{r}_j(t) = a[\cos(\omega t + \phi_j), \sin(\omega t + \phi_j), 0]^T

where \omega = 2\pi/T is the angular velocity, and \phi_i, \phi_j are initial phases.

The angle between the vectors is:

\theta_{ij}(t) = \arccos\left(\frac{\mathbf{r}_i(t) \cdot \mathbf{r}_j(t)}{a^2}\right) = |\phi_i - \phi_j| = \Delta \phi_{ij}

constant for circular orbits with the same altitude.

The distance between satellites is:

d_{ij}(t) = \|\mathbf{r}_i(t) - \mathbf{r}_j(t)\| = 2a \sin\left(\frac{\Delta \phi_{ij}}{2}\right)

also constant.

Line-of-sight exists when the line segment between satellites doesn't intersect Earth. The minimum distance from Earth's center to the line is:

h_{min} = \frac{\|\mathbf{r}_i \times \mathbf{r}_j\|}{\|\mathbf{r}_i - \mathbf{r}_j\|}

Using \|\mathbf{r}_i \times \mathbf{r}_j\| = a^2 |\sin(\Delta \phi_{ij})| and \|\mathbf{r}_i - \mathbf{r}_j\| = 2a \sin(\Delta \phi_{ij}/2), we get:

h_{min} = \frac{a^2 |\sin(\Delta \phi_{ij})|}{2a \sin(\Delta \phi_{ij}/2)} = \frac{a \cdot 2\sin(\Delta \phi_{ij}/2)\cos(\Delta \phi_{ij}/2)}{2\sin(\Delta \phi_{ij}/2)} = a \cos\left(\frac{\Delta \phi_{ij}}{2}\right)

Line-of-sight requires h_{min} \geq R_E + h_{atm}, so:

a \cos\left(\frac{\Delta \phi_{ij}}{2}\right) \geq R_E + h_{atm}

\cos\left(\frac{\Delta \phi_{ij}}{2}\right) \geq \frac{R_E + h_{atm}}{a}

This gives a condition on \Delta \phi_{ij}. For given \Delta \phi_{ij}, the satellites maintain constant geometry relative to each other, so if they have line-of-sight at one time, they have it always (for circular orbits).

However, the lemma formula appears different because it considers satellites in different orbital planes with inclination. In that case, the relative geometry changes with time.

For the general case with inclination difference \Delta I, the angle between orbits varies. The maximum central angle between satellites that still permits line-of-sight is:

\theta_{max} = \arccos\left(\frac{R_E + h_{atm}}{a}\right)

The relative angular separation evolves as:

\theta_{ij}(t) = \arccos\left[\cos^2\left(\frac{\Delta I}{2}\right) \cos(\Delta \lambda(t)) + \sin^2\left(\frac{\Delta I}{2}\right)\right]

where \Delta \lambda(t) = \Delta \lambda_0 + \Delta \dot{\lambda} t is the longitude difference.

Solving \theta_{ij}(t) \leq \theta_{max} for t gives the visibility window. The formula in the lemma is an approximation assuming small \Delta I.

âˆ

Proof of Theorem 5.6 (Objective Sensitivity)

Theorem: The sensitivity of optimal value f^* to parameter Î¸ is:

\frac{df^*}{d\theta} = -\lambda^* \frac{\partial g}{\partial \theta}

Proof: This is the envelope theorem from optimization theory.

Consider the parameterized optimization problem:

\begin{aligned}
f^*(\theta) &= \min_{\mathbf{x}} f(\mathbf{x}, \theta) \\
\text{s.t.} & \quad g(\mathbf{x}, \theta) \leq 0
\end{aligned}

Assume for each Î¸, there is a unique optimal solution \mathbf{x}^*(\theta) with associated Lagrange multiplier \lambda^*(\theta).

The Lagrangian is:

\mathcal{L}(\mathbf{x}, \lambda, \theta) = f(\mathbf{x}, \theta) + \lambda g(\mathbf{x}, \theta)

At optimality, we have:

\frac{\partial \mathcal{L}}{\partial \mathbf{x}} (\mathbf{x}^*(\theta), \lambda^*(\theta), \theta) = 0

\lambda^*(\theta) \geq 0, \quad g(\mathbf{x}^*(\theta), \theta) \leq 0, \quad \lambda^*(\theta) g(\mathbf{x}^*(\theta), \theta) = 0

Now, differentiate f^*(\theta) = f(\mathbf{x}^*(\theta), \theta) with respect to Î¸:

\frac{df^*}{d\theta} = \frac{\partial f}{\partial \mathbf{x}} \frac{d\mathbf{x}^*}{d\theta} + \frac{\partial f}{\partial \theta}

But from the stationarity condition:

\frac{\partial f}{\partial \mathbf{x}} + \lambda^* \frac{\partial g}{\partial \mathbf{x}} = 0 \quad \Rightarrow \quad \frac{\partial f}{\partial \mathbf{x}} = -\lambda^* \frac{\partial g}{\partial \mathbf{x}}

So:

\frac{df^*}{d\theta} = -\lambda^* \frac{\partial g}{\partial \mathbf{x}} \frac{d\mathbf{x}^*}{d\theta} + \frac{\partial f}{\partial \theta}

Also, differentiate the constraint g(\mathbf{x}^*(\theta), \theta) = 0 (assuming it's active):

\frac{\partial g}{\partial \mathbf{x}} \frac{d\mathbf{x}^*}{d\theta} + \frac{\partial g}{\partial \theta} = 0 \quad \Rightarrow \quad \frac{\partial g}{\partial \mathbf{x}} \frac{d\mathbf{x}^*}{d\theta} = -\frac{\partial g}{\partial \theta}

Substituting:

\frac{df^*}{d\theta} = \lambda^* \frac{\partial g}{\partial \theta} + \frac{\partial f}{\partial \theta}

If the constraint doesn't depend on Î¸ in the objective (\partial f/\partial \theta = 0), we get:

\frac{df^*}{d\theta} = \lambda^* \frac{\partial g}{\partial \theta}

If the constraint is inactive (\lambda^* = 0), then \frac{df^*}{d\theta} = \frac{\partial f}{\partial \theta}.

The theorem statement assumes \partial f/\partial \theta = 0, giving \frac{df^*}{d\theta} = -\lambda^* \frac{\partial g}{\partial \theta} (with sign depending on convention).

âˆ

Proof of Theorem 5.7 (Tractable Reformulation)

Theorem: For linear constraints with ellipsoidal uncertainty:

\tilde{g}_j(\mathbf{x}) = g_j^0(\mathbf{x}) + \|\mathbf{G}_j(\mathbf{x})\| \leq 0

is the robust counterpart.

Proof: Consider a linear constraint with uncertain parameters:

g_j(\mathbf{x}, \boldsymbol{\theta}) = g_j^0(\mathbf{x}) + \boldsymbol{\theta}^T \mathbf{G}_j(\mathbf{x}) \leq 0 \quad \forall \boldsymbol{\theta} \in \Theta

where \Theta = \{\boldsymbol{\theta} : \|\boldsymbol{\theta}\| \leq 1\} is the unit ball (ellipsoidal uncertainty).

The robust constraint requires:

g_j^0(\mathbf{x}) + \max_{\|\boldsymbol{\theta}\| \leq 1} \boldsymbol{\theta}^T \mathbf{G}_j(\mathbf{x}) \leq 0

By the Cauchy-Schwarz inequality:

\max_{\|\boldsymbol{\theta}\| \leq 1} \boldsymbol{\theta}^T \mathbf{G}_j(\mathbf{x}) = \|\mathbf{G}_j(\mathbf{x})\|

achieved at \boldsymbol{\theta}^* = \mathbf{G}_j(\mathbf{x})/\|\mathbf{G}_j(\mathbf{x})\|.

Thus, the robust constraint is equivalent to:

g_j^0(\mathbf{x}) + \|\mathbf{G}_j(\mathbf{x})\| \leq 0

This is a second-order cone constraint, which is convex.

Generalization to ellipsoids: If \Theta = \{\boldsymbol{\theta} : \boldsymbol{\theta}^T \mathbf{P}^{-1} \boldsymbol{\theta} \leq 1\} for positive definite \mathbf{P}, then:

\max_{\boldsymbol{\theta} \in \Theta} \boldsymbol{\theta}^T \mathbf{G}_j(\mathbf{x}) = \|\mathbf{P}^{1/2} \mathbf{G}_j(\mathbf{x})\|

and the robust constraint becomes:

g_j^0(\mathbf{x}) + \|\mathbf{P}^{1/2} \mathbf{G}_j(\mathbf{x})\| \leq 0

Proof of Cauchy-Schwarz application: For any \boldsymbol{\theta} with \|\boldsymbol{\theta}\| \leq 1:

\boldsymbol{\theta}^T \mathbf{G}_j(\mathbf{x}) \leq \|\boldsymbol{\theta}\| \cdot \|\mathbf{G}_j(\mathbf{x})\| \leq \|\mathbf{G}_j(\mathbf{x})\|

Equality is achieved when \boldsymbol{\theta} is parallel to \mathbf{G}_j(\mathbf{x}) and has unit norm.

Thus, the worst-case constraint value is exactly \|\mathbf{G}_j(\mathbf{x})\|, making the robust constraint tractable.

âˆ

A.5 Proofs for Chapter 6: Resilience Framework

Proof of Theorem 6.1 (CUSUM Optimality)

Theorem: For detecting a change in mean from Î¼â‚€ to Î¼â‚, CUSUM minimizes the worstâ€‘case detection delay subject to false alarm constraint.

Proof: This is Lorden's theorem in change detection theory.

The CUSUM algorithm for detecting a change from distribution P_0 to P_1 is:

S_n = \max\left(0, S_{n-1} + \log\frac{p_1(X_n)}{p_0(X_n)}\right)

where p_0, p_1 are probability densities.

The algorithm raises an alarm at:

\tau = \inf\{n \geq 1 : S_n \geq h\}

Performance measures:

Â· Average run length to false alarm: \text{ARL2FA} = \mathbb{E}_0[\tau]
Â· Worst-case detection delay: \text{WADD} = \sup_{t \geq 1} \operatorname{ess\,sup} \mathbb{E}_t[(\tau - t)^+ | \mathcal{F}_{t-1}]

where \mathbb{E}_0 is expectation under no change, \mathbb{E}_t is expectation when change occurs at time t, and \mathcal{F}_{t-1} is the Ïƒ-algebra of observations up to time t-1.

Lorden's theorem states that among all procedures with \text{ARL2FA} \geq \gamma, the CUSUM procedure with threshold h chosen so that \text{ARL2FA} = \gamma minimizes the WADD.

Proof sketch: The proof uses optimal stopping theory and the fact that CUSUM is equivalent to a repeated sequential probability ratio test (SPRT). The SPRT is optimal for testing between two hypotheses given a single sample, and repeating it gives optimality for change detection.

Key steps:

1. The CUSUM statistic can be written as:
   S_n = \max_{1 \leq k \leq n} \sum_{i=k}^n \log\frac{p_1(X_i)}{p_0(X_i)}
   which is the maximum likelihood ratio over possible change points k.
2. By the optimality of the likelihood ratio test (Neyman-Pearson lemma), for a given false alarm probability, the likelihood ratio test maximizes detection probability.
3. For change detection, we want to maximize detection probability at each potential change time, subject to constraints on false alarms before that time.
4. CUSUM achieves this by effectively running an SPRT at each possible change point and taking the maximum.

Corollary A.16.1 (Asymptotic optimality): As the false alarm constraint \gamma \to \infty, the CUSUM delay satisfies:

\text{WADD} \sim \frac{\log \gamma}{D(P_1 \| P_0)}

where D(P_1 \| P_0) is the Kullback-Leibler divergence, and this is the best possible asymptotic rate.

âˆ

Proof of Theorem 6.2 (Consensus Innovation)

Theorem: Under no faults, \lim_{t \to \infty} y_i(t) = 0 where:

y_i(t) = \sum_{j \in \mathcal{N}_i} w_{ij} (x_j(t) - x_i(t))

A persistent nonâ€‘zero y_i(t) indicates a fault in the neighborhood.

Proof: In normal operation, agents run a consensus algorithm. For standard consensus with weights w_{ij}:

x_i(t+1) = x_i(t) + \epsilon \sum_{j \in \mathcal{N}_i} w_{ij} (x_j(t) - x_i(t))

for some step size \epsilon > 0.

At equilibrium (consensus), all x_i are equal, so x_j - x_i = 0 for all i,j. Thus:

y_i = \sum_{j \in \mathcal{N}_i} w_{ij} (x_j - x_i) = 0

Convergence proof: Consensus algorithms converge to a common value under connectivity assumptions (Theorem 4.1). Therefore, x_j(t) - x_i(t) \to 0 as t \to \infty, so y_i(t) \to 0.

Fault detection: If an agent j is faulty and deviates from consensus, then x_j - x_i \neq 0 persistently. Since y_i includes terms for all neighbors, a faulty neighbor contributes a non-zero term.

Specifically, suppose neighbor k is faulty with x_k(t) = x^*(t) + \delta(t), where x^* is the consensus value and \delta(t) \neq 0 is the deviation. Then:

y_i(t) = w_{ik} \delta(t) + \sum_{j \in \mathcal{N}_i \setminus \{k\}} w_{ij} (x_j(t) - x_i(t))

Even if other terms converge to zero, the w_{ik}\delta(t) term remains non-zero, so y_i(t) doesn't converge to zero.

Statistical test: We can test if |y_i(t)| > \tau for sufficiently large t to detect faults. Under no faults, y_i(t) should approach zero. Under faults, it remains non-zero.

Lemma A.17.1 (Innovation variance): Under no faults and with measurement noise variance \sigma^2, the innovation variance is:

\text{Var}[y_i(t)] = \sum_{j \in \mathcal{N}_i} w_{ij}^2 \sigma^2

Proof: If x_j = x^* + \eta_j with \eta_j \sim N(0, \sigma^2) independent, then x_j - x_i = \eta_j - \eta_i, and:

\text{Var}[y_i] = \text{Var}\left[\sum_j w_{ij}(\eta_j - \eta_i)\right] = \sum_j w_{ij}^2 (\sigma^2 + \sigma^2) = 2\sigma^2 \sum_j w_{ij}^2

assuming independence.

âˆ

Proof of Theorem 6.3 (Isolation Completeness)

Theorem: If at most f neighbors are Byzantine, and f < |\mathcal{N}_i|/2, Algorithm 6.2 isolates all Byzantine agents in finite time.

Proof: Algorithm 6.2 uses reputation scores R_{ij}(t) that decay when neighbor j's behavior deviates from predictions.

Reputation update:

R_{ij}(t) = \alpha R_{ij}(t-1) + (1-\alpha) \cdot \mathbb{I}(\|x_j(t) - \hat{x}_j(t|i)\| \leq \epsilon)

Key properties:

1. For non-faulty agents, predictions are accurate with high probability, so the indicator is 1 most of the time, and R_{ij}(t) stays high.
2. For Byzantine agents, behavior is arbitrary and often deviates from predictions, so the indicator is 0 frequently, causing R_{ij}(t) to decay.

Since the update is a convex combination, if the indicator is 0 for a sustained period, R_{ij}(t) decays exponentially toward 0.

Isolation logic:

1. Agent i marks j as suspect if R_{ij}(t) < \theta_{low} for Ï„ consecutive steps.
2. Agents share suspect lists.
3. Agent k is isolated if more than f_{max} agents report k as suspect.

Completeness proof: We need to show that all Byzantine agents are eventually isolated.

Let B be the set of Byzantine agents. For any Byzantine agent k \in B:

Â· Its behavior deviates from predictions made by non-faulty agents
Â· For each non-faulty neighbor i of k, the indicator \mathbb{I}(\|x_k(t) - \hat{x}_k(t|i)\| \leq \epsilon) is 0 frequently
Â· Therefore, R_{ik}(t) decays
Â· After sufficient time, R_{ik}(t) < \theta_{low} for all non-faulty neighbors i of k
Â· These neighbors mark k as suspect
Â· Since there are at least |\mathcal{N}_k| - f \geq |\mathcal{N}_k|/2 + 1 non-faulty neighbors (by assumption f < |\mathcal{N}_k|/2), and f_{max} is typically set to |\mathcal{N}_k|/2 or similar, k gets isolated

Finite time: The exponential decay of reputation ensures that after time T \geq \frac{\log(\theta_{low}/R_0)}{\log \alpha}, we have R_{ik}(t) < \theta_{low} if the indicator is always 0. Even if the indicator is sometimes 1, if it's 0 frequently enough, R_{ik}(t) still decays below \theta_{low} in finite time.

Why f < |\mathcal{N}_i|/2 is needed: This ensures that non-faulty neighbors outnumber Byzantine neighbors. If Byzantine agents could form a majority among some agent's neighbors, they could collude to give each other high reputations while giving non-faulty agents low reputations, preventing isolation.

âˆ

Proof of Lemma 6.1 (Agreement on Nonâ€‘Faulty Agents)

Lemma: All nonâ€‘faulty agents eventually agree on the status of every other agent.

Proof: Non-faulty agents follow the same deterministic algorithm for:

1. Computing predictions \hat{x}_j(t|i)
2. Updating reputations R_{ij}(t)
3. Marking agents as suspect based on reputation thresholds

Assuming:

Â· Reliable communication for sharing suspect lists (or eventually consistent sharing)
Â· Synchronized observations (or tolerance to small asynchrony)
Â· Common parameters (\alpha, \epsilon, \theta_{low}, \tau)

Then all non-faulty agents compute the same reputations for each agent j, given the same observations.

Specifically, if two non-faulty agents i and i' both observe the same sequence of values from agent j, they compute the same sequence of predictions and thus the same reputation R_{ij}(t) = R_{i'j}(t).

In practice, observations might differ due to:

Â· Communication delays
Â· Different neighbor sets
Â· Measurement noise

However, if the system reaches a steady state and the graph is connected, information propagates, and non-faulty agents converge to consistent views.

Formal argument: Consider the consensus-like dynamics of beliefs about agent status. Non-faulty agents share their suspect lists and update their beliefs. This is essentially a consensus problem on binary vectors (suspect/not suspect).

By Theorem 4.1, with joint connectivity, consensus is reached. So all non-faulty agents eventually agree on which agents are suspect.

Corollary A.18.1 (Eventual consistency): There exists time T such that for all t \geq T and all non-faulty agents i, i':

\text{suspect}_i(t) = \text{suspect}_{i'}(t)

Proof: Follows from consensus convergence.

âˆ

Proof of Theorem 6.4 (Safety of Isolation)

Theorem: Algorithm 6.2 never isolates a nonâ€‘faulty agent.

Proof: By contradiction. Suppose a non-faulty agent k is isolated. This means that more than f_{max} agents reported k as suspect.

For k to be reported as suspect by agent i, we need R_{ik}(t) < \theta_{low} for Ï„ consecutive steps.

The reputation update is:

R_{ik}(t) = \alpha R_{ik}(t-1) + (1-\alpha) \cdot \mathbb{I}(\|x_k(t) - \hat{x}_k(t|i)\| \leq \epsilon)

For non-faulty k, its behavior follows the system dynamics with bounded noise. The prediction \hat{x}_k(t|i) made by non-faulty i is based on the same dynamics model. Therefore:

\|x_k(t) - \hat{x}_k(t|i)\| \leq \eta

with high probability, where Î· is a bound accounting for model errors and noise.

If we choose \epsilon \geq \eta, then the indicator is 1 with high probability. Thus, R_{ik}(t) remains high (close to 1) for non-faulty k.

Specifically, if the indicator is 1 with probability at least 1-\delta, then in expectation:

\mathbb{E}[R_{ik}(t)] \geq 1 - \delta(1-\alpha)

which can be made arbitrarily close to 1 by choosing small Î´.

Therefore, for non-faulty k and non-faulty i, R_{ik}(t) is unlikely to fall below \theta_{low}. If it does occasionally, it's unlikely to stay below for Ï„ consecutive steps.

Now, suppose k is isolated anyway. This requires at least f_{max}+1 agents to report k as suspect. Among these, at most f are Byzantine (since there are at most f Byzantine agents total). The rest, at least f_{max}+1 - f, must be non-faulty.

But non-faulty agents are unlikely to report non-faulty k as suspect. The probability that many non-faulty agents all have R_{ik}(t) < \theta_{low} for Ï„ steps is very small if parameters are chosen appropriately.

More formally, we can bound this probability and make it arbitrarily small by choosing:

Â· \epsilon large enough to capture normal variation
Â· \theta_{low} low enough
Â· Ï„ large enough

Thus, with high probability, no non-faulty agent is isolated.

Deterministic version: If we assume bounded noise and perfect models within the bound Î·, and set \epsilon \geq Î·, then the indicator is always 1 for non-faulty pairs. Then R_{ik}(t) \to 1 for all non-faulty i,k, and never falls below \theta_{low}. So non-faulty agents are never isolated.

âˆ

Proof of Theorem 6.5 (Model Checking Complexity)

Theorem: Checking LTL properties on a system with N agents has complexity O(|S| \cdot 2^{|\phi|}) where |S| is state space size.

Proof: The automata-theoretic approach to LTL model checking:

1. Translate LTL formula to BÃ¼chi automaton: For an LTL formula Ï†, we can construct a BÃ¼chi automaton A_\phi that accepts exactly the infinite words satisfying Ï†. The size of A_\phi is at most 2^{O(|\phi|)}, specifically O(2^{|\phi|}).
2. Construct product automaton: Given the system model M (as a Kripke structure) and the BÃ¼chi automaton A_\phi, construct the product automaton M \times A_\phi. The state space of the product is S \times Q, where S is the state space of M and Q is the state set of A_\phi. So |S \times Q| = |S| \cdot |Q| = O(|S| \cdot 2^{|\phi|}).
3. Check emptiness: Check if the product automaton accepts any word, which is equivalent to checking if there exists an accepting run (a run that visits accepting states infinitely often). This can be done by finding strongly connected components (SCCs) containing accepting states. SCC decomposition can be done in time linear in the size of the graph, i.e., O(|S| \cdot 2^{|\phi|}).

Thus, the overall complexity is O(|S| \cdot 2^{|\phi|}).

For orbital CPS: The state space size |S| grows exponentially with N. Specifically, if each agent has s possible states, then |S| = s^N. This makes exact model checking infeasible for large N.

Abstraction techniques: To handle state explosion, we use:

Â· Symmetry reduction
Â· Compositional verification
Â· Abstraction refinement

But the theoretical complexity remains exponential in N and |\phi|.

Lemma A.19.1 (LTL to BÃ¼chi construction): For LTL formula Ï† with n subformulas, the BÃ¼chi automaton has at most 2^{2^n} states in worst case, but typically O(2^n).

Proof: The standard construction uses subsets of subformulas that are satisfied.

âˆ

Proof of Theorem 6.6 (Degradation Bound)

Theorem: For a constellation with redundant connectivity, performance degrades at most linearly with faults:

\delta(f) \geq 1 - \frac{f}{N \cdot k_{min}}

Proof: Consider a network with N nodes and minimum node connectivity k_{min}. This means at least k_{min} edge-disjoint paths exist between any two nodes (by Menger's theorem).

When f nodes fail, the worst-case impact on connectivity is that all k_{min} paths between some pair of nodes might be disrupted if the failed nodes are strategically chosen.

However, for random failures or for the average case, the impact is less severe.

Maximum flow argument: The maximum flow between two nodes in a network is equal to the minimum cut capacity (max-flow min-cut theorem). With node connectivity k_{min}, the minimum cut has capacity at least k_{min}.

When f nodes fail, they could be part of the minimum cut. In the worst case, all nodes in the minimum cut fail, reducing the cut capacity to 0. But there are many possible cuts.

A more refined bound: The total capacity of the network is proportional to the sum of node degrees, which is at least N \cdot k_{min} (since each node has degree at least k_{min}).

When f nodes fail, they remove at most f \cdot d_{max} capacity, where d_{max} is the maximum degree. In the worst case, d_{max} \approx N, giving bound 1 - f/N.

But with k_{min}-regular graphs (all nodes have degree k_{min}), each node contributes k_{min} to the total capacity. So f failed nodes remove at most f \cdot k_{min} capacity.

The remaining capacity is at least N \cdot k_{min} - f \cdot k_{min} = (N-f)k_{min}.

Thus, the relative capacity is:

\frac{(N-f)k_{min}}{N \cdot k_{min}} = 1 - \frac{f}{N}

This is the bound in the theorem: \delta(f) \geq 1 - f/N.

The theorem statement has 1 - f/(N \cdot k_{min}), which is weaker. Let's reconcile:

In a k_{min}-regular graph, total capacity is N \cdot k_{min}/2 (each edge counted twice). Removing f nodes removes at most f \cdot k_{min} edges (each failed node had k_{min} edges). So remaining capacity is at least:

\frac{N \cdot k_{min}}{2} - f \cdot k_{min} = k_{min}\left(\frac{N}{2} - f\right)

Relative to original:

\frac{k_{min}(N/2 - f)}{k_{min} N/2} = 1 - \frac{2f}{N}

So \delta(f) \geq 1 - 2f/N, which is of the form 1 - c f/N.

The exact constant depends on graph structure. The theorem gives a conservative bound 1 - f/(N \cdot k_{min}), which is very weak for large k_{min} but still linear in f.

For throughput: If we measure performance as total throughput, and if each node can generate at most 1 unit of traffic, then with f failed nodes, the maximum throughput is at most N-f. So \delta(f) \geq (N-f)/N = 1 - f/N.

âˆ

Proof of Theorem 6.7 (Mode Transition Safety)

Theorem: If mode transitions satisfy:

V_{m+1}(x) \leq V_m(x) \quad \forall x \in \mathcal{X}_m \cap \mathcal{X}_{m+1}

where V_m is a Lyapunov function for mode m, then transitions are stable.

Proof: Consider a switched system that operates in different modes. At time t, the system is in mode m(t) with dynamics:

\dot{x} = f_{m(t)}(x)

Each mode m has an associated Lyapunov function V_m(x) satisfying:

\dot{V}_m(x) = \frac{\partial V_m}{\partial x} f_m(x) \leq -W_m(x) \leq 0

for x \in \mathcal{X}_m, where W_m(x) is positive definite.

When the system switches from mode m to mode m+1 at state x, we have:

V_{m+1}(x) \leq V_m(x)

by assumption.

Now consider the sequence of switching times t_1, t_2, \dots and modes m_1, m_2, \dots. At time t_k, the system switches to mode m_k at state x(t_k).

The Lyapunov function values at switching instants satisfy:

V_{m_{k+1}}(x(t_{k+1})) \leq V_{m_k}(x(t_{k+1})) \leq V_{m_k}(x(t_k))

where:

Â· First inequality: By the transition condition V_{m+1}(x) \leq V_m(x)
Â· Second inequality: Because V_{m_k} decreases along trajectories in mode m_k between t_k and t_{k+1}

Thus, the sequence V_{m_k}(x(t_k)) is non-increasing.

Since V_m are positive definite and radially unbounded (typically), and the sequence is non-increasing, the state remains bounded.

If there are finitely many switches, eventually the system stays in some mode m, and V_m ensures stability in that mode.

If there are infinitely many switches, we need to ensure that V doesn't get "stuck" at a non-zero value. This requires additional conditions, such as:

Â· Each W_m(x) is positive definite
Â· The switching doesn't occur too frequently (dwell time)
Â· Or the decrease in V during each mode is sufficient

Under these conditions, the switched system is stable.

Corollary A.20.1 (Common Lyapunov function): If there exists a function V that works for all modes (\dot{V}_m(x) \leq -W(x) for all m), then the system is stable for any switching signal.

Proof: V decreases along all trajectories regardless of mode.

âˆ

Proof of Theorem 6.8 (Secure Consensus)

Theorem: A consensus protocol is secure against eavesdroppers if the mutual information between observations and secret state is zero:

I(\mathbf{x}_{secret}; \{\mathbf{y}_e\}) = 0

Proof: Mutual information I(X;Y) = H(X) - H(X|Y) measures the reduction in uncertainty about X given Y.

If I(\mathbf{x}_{secret}; \{\mathbf{y}_e\}) = 0, then:

H(\mathbf{x}_{secret} | \{\mathbf{y}_e\}) = H(\mathbf{x}_{secret})

This means that even after observing \{\mathbf{y}_e\}, the eavesdropper's uncertainty about \mathbf{x}_{secret} is the same as before any observations. In other words, the observations provide no information about the secret.

This is the definition of information-theoretic security or perfect secrecy.

For consensus protocols, we want to keep individual agents' initial values or intermediate states secret, while still reaching consensus. Techniques include:

1. Secure multi-party computation: Using cryptographic protocols that compute functions on encrypted data
2. Differential privacy: Adding noise to hide individual contributions
3. Homomorphic encryption: Performing computations on encrypted data

Example: Suppose each agent i has secret value x_i, and we want to compute the average \bar{x} = \frac{1}{N}\sum_i x_i without revealing individual x_i. A simple secure protocol:

1. Agent 1 generates random number r
2. Agent 1 sends x_1 + r to agent 2
3. Agent 2 adds x_2 and sends to agent 3
4. Continue until agent N sends back to agent 1
5. Agent 1 subtracts r and divides by N to get average

In this protocol, no agent learns any other agent's value (assuming honest agents).

The mutual information between any agent's observation and another agent's secret value is 0, providing information-theoretic security.

Lemma A.21.1 (Perfect secrecy): For a cryptosystem with message M, ciphertext C, and key K, perfect secrecy requires:

I(M;C) = 0

which is equivalent to:

\Pr[M=m | C=c] = \Pr[M=m] \quad \forall m,c

Proof: By definition of mutual information and perfect secrecy.

âˆ

Proof of Theorem 6.9 (Byzantine Agreement)

Theorem: With digital signatures and N > 3f, Algorithm 6.3 achieves Byzantine agreement.

Proof: This is the Dolev-Strong protocol for Byzantine agreement with authentication.

Algorithm 6.3 outline:

1. Phase 1: Each agent broadcasts its signed initial value
2. Phase 2: Agents forward signed values they receive
3. Phase 3: Decide on value with signatures from >2f distinct agents

Properties of digital signatures:

Â· Unforgeable: Only agent i can produce a valid signature on message (v, i)
Â· Verifiable: Anyone can verify a signature
Â· Non-repudiable: Agent i cannot deny signing

Validity: If all non-faulty agents have the same initial value v, then they all decide v.

Proof: All non-faulty agents sign and broadcast v. Each non-faulty agent receives at least N-f signatures on v, and N-f > 2f since N > 3f. So all non-faulty agents decide v.

Agreement: All non-faulty agents decide the same value.

Proof: Suppose two non-faulty agents decide v and v' with v â‰  v'. Then:

Â· Agent deciding v has >2f signatures on v
Â· Agent deciding v' has >2f signatures on v'

Since there are at most f Byzantine agents, at least (2f+1) - f = f+1 signatures on v must come from non-faulty agents. Similarly, at least f+1 signatures on v' must come from non-faulty agents.

But a non-faulty agent signs only one value in a given round. So we have at least 2(f+1) distinct non-faulty agents, which is impossible since there are only N-f non-faulty agents, and:

2(f+1) > N-f \quad \text{when} \quad N \leq 3f

With N > 3f, we have N-f > 2f, but not necessarily N-f \geq 2(f+1).

Actually, let's count carefully:

Let A be the set of non-faulty agents who signed v, and B be the set of non-faulty agents who signed v'. Then:

Â· |A| \geq (2f+1) - f = f+1 (since at most f Byzantine signatures on v)
Â· |B| \geq f+1 similarly
Â· A and B are disjoint (a non-faulty agent signs only one value)
Â· So total non-faulty agents: |A \cup B| = |A| + |B| \geq 2(f+1) = 2f+2

But there are only N-f non-faulty agents. So we need:

N-f \geq 2f+2 \quad \Rightarrow \quad N \geq 3f+2

Which is slightly stronger than N > 3f.

The standard Dolev-Strong result is N > 3f (or N \geq 3f+1). The discrepancy comes from rounding. With N = 3f+1:

Â· Non-faulty agents: N-f = 2f+1
Â· Signatures needed: 2f+1
Â· Byzantine signatures possible: f
Â· Non-faulty signatures needed: (2f+1) - f = f+1
Â· For two values: need 2(f+1) = 2f+2 non-faulty signatures
Â· But only 2f+1 non-faulty agents exist

So indeed, with N = 3f+1, two different values cannot both get 2f+1 signatures with at least f+1 from non-faulty agents. Therefore, agreement holds.

Termination: All non-faulty agents decide by the end of phase 3.

Proof: By construction of the algorithm.

Thus, Algorithm 6.3 achieves Byzantine agreement for N > 3f.

âˆ

Appendix B: Notation Reference

B.1 Mathematical Notation

Sets and Spaces

Â· \mathbb{R}: Real numbers
Â· \mathbb{R}^n: n-dimensional real vector space
Â· \mathbb{Z}: Integers
Â· \mathbb{N}: Natural numbers
Â· [a,b]: Closed interval between a and b
Â· (a,b): Open interval
Â· \{x : P(x)\}: Set of x satisfying property P
Â· |S|: Cardinality (size) of set S
Â· \emptyset: Empty set
Â· \in: Element of
Â· \subset, \subseteq: Subset
Â· \cup, \cap, \setminus: Set union, intersection, difference
Â· \mathcal{X}, \mathcal{Y}: General sets (calligraphic)

Vectors and Matrices

Â· \mathbf{x}, \mathbf{v}: Vectors (bold lowercase)
Â· \mathbf{A}, \mathbf{M}: Matrices (bold uppercase)
Â· \mathbf{x}^T: Transpose of vector \mathbf{x}
Â· \mathbf{A}^T: Transpose of matrix \mathbf{A}
Â· \|\mathbf{x}\|: Euclidean norm of \mathbf{x}
Â· \mathbf{x} \cdot \mathbf{y}: Dot product
Â· \mathbf{x} \times \mathbf{y}: Cross product (in \mathbb{R}^3)
Â· \mathbf{I}_n: nÃ—n identity matrix
Â· \mathbf{0}_n: Zero vector in \mathbb{R}^n
Â· \mathbf{1}_n: Vector of all ones in \mathbb{R}^n
Â· [\mathbf{x}; \mathbf{y}]: Vertical concatenation of vectors
Â· [\mathbf{x}, \mathbf{y}]: Horizontal concatenation

Functions and Calculus

Â· f: X \to Y: Function from X to Y
Â· f(x): Value of f at x
Â· f \circ g: Composition: (f \circ g)(x) = f(g(x))
Â· \dot{x}: Time derivative dx/dt
Â· \ddot{x}: Second derivative
Â· \frac{\partial f}{\partial x}: Partial derivative
Â· \nabla f: Gradient of f
Â· \nabla^2 f: Hessian of f
Â· \frac{df}{dx}: Total derivative
Â· \int_a^b f(x) dx: Definite integral
Â· \oint: Contour integral
Â· \lim_{x \to a} f(x): Limit
Â· \arg\min_x f(x): Argument that minimizes f
Â· \arg\max_x f(x): Argument that maximizes f
Â· \sup: Supremum (least upper bound)
Â· \inf: Infimum (greatest lower bound)
Â· \max, \min: Maximum, minimum

Probability and Statistics

Â· \mathbb{P}[A]: Probability of event A
Â· \mathbb{E}[X]: Expectation of random variable X
Â· \text{Var}[X]: Variance
Â· \text{Cov}[X,Y]: Covariance
Â· X \sim P: X has distribution P
Â· N(\mu, \sigma^2): Normal distribution
Â· \mathcal{U}(a,b): Uniform distribution
Â· \mathbb{I}[A]: Indicator function (1 if A true, 0 otherwise)
Â· p(x): Probability density function
Â· P(x): Probability mass function
Â· H(X): Entropy of X
Â· I(X;Y): Mutual information between X and Y

Logic and Set Theory

Â· \forall: For all
Â· \exists: There exists
Â· \Rightarrow: Implies
Â· \Leftrightarrow: If and only if
Â· \neg: Not
Â· \land: And
Â· \lor: Or
Â· \in: Element of
Â· \subset: Subset
Â· \emptyset: Empty set
Â· \cup: Union
Â· \cap: Intersection

Linear Temporal Logic (LTL)

Â· â–¡: Always (globally)
Â· â—‡: Eventually
Â· â—‹: Next
Â· U: Until
Â· W: Weak until
Â· R: Release

B.2 Orbital CPS Notation

Physical State Variables

Â· N: Number of satellites in constellation
Â· i, j: Satellite indices (i, j \in \{1, \dots, N\})
Â· \mathbf{r}_i \in \mathbb{R}^3: Position vector of satellite i (ECI frame)
Â· \mathbf{v}_i \in \mathbb{R}^3: Velocity vector of satellite i
Â· m_i: Total mass of satellite i
Â· m_i^{fuel}: Propellant mass
Â· e_i: Battery state-of-charge (e_i \in [0,1])
Â· T_i: Core temperature
Â· \mathbf{F}_i \in \mathbb{R}^3: Thrust vector (control input)
Â· \Gamma_{ij}: Data rate on link from i to j
Â· P_{solar,i}: Solar power generation
Â· P_{bus,i}: Bus power consumption
Â· \eta_{rf}(\Gamma): RF power consumption for data rate Î“
Â· \eta_{thrust}(\mathbf{F}): Thruster power consumption for thrust \mathbf{F}
Â· I_{sp}: Specific impulse of thrusters
Â· g_0 = 9.81 \text{ m/s}^2: Standard gravity

Orbital Mechanics

Â· \mu = GM_E = 3.986 \times 10^{14} \text{ m}^3/\text{s}^2: Earth's gravitational parameter
Â· R_E = 6,378 \text{ km}: Earth's equatorial radius
Â· J_2 = 1.08263 \times 10^{-3}: Earth's oblateness coefficient
Â· a: Semi-major axis
Â· e: Eccentricity (not to be confused with energy)
Â· i: Inclination (not to be confused with satellite index)
Â· \Omega: Right ascension of ascending node
Â· \omega: Argument of perigee
Â· \nu: True anomaly
Â· T: Orbital period
Â· n = 2\pi/T: Mean motion

Communication Network

Â· G(t) = (V, E(t)): Time-varying graph
Â· V = \{1, \dots, N\}: Vertex set (satellites)
Â· E(t) \subseteq V \times V: Edge set (active links at time t)
Â· \mathcal{N}_i(t): Neighbor set of satellite i at time t
Â· L_{ij}(t): Link existence indicator (1 if link exists, 0 otherwise)
Â· d_{ij}(t) = \|\mathbf{r}_i(t) - \mathbf{r}_j(t)\|: Distance between satellites
Â· D_{max}: Maximum communication range
Â· h_{atm} \approx 100 \text{ km}: Atmospheric buffer for line-of-sight
Â· f_c: Carrier frequency
Â· c = 3 \times 10^8 \text{ m/s}: Speed of light
Â· f_{doppler,ij}: Doppler shift on link (i,j)
Â· B: Bandwidth
Â· SNR: Signal-to-noise ratio
Â· \Gamma_{ij}^{max}: Maximum data rate on link (i,j)

Constraints

Â· d_{safe}: Minimum safe separation distance
Â· e_{min}, e_{max}: Minimum and maximum battery state-of-charge
Â· T_{min}, T_{max}: Minimum and maximum temperature
Â· m_{min}^{fuel}: Minimum propellant reserve

B.3 Optimization and Control

Dec-POMDP Framework

Â· \mathcal{I}: Set of agents
Â· \mathcal{S}: State space
Â· \mathcal{A}_i: Action space for agent i
Â· \mathcal{O}_i: Observation space for agent i
Â· P(s'|s,a): Transition probability
Â· R(s,a): Reward function
Â· \gamma: Discount factor (\gamma \in [0,1))
Â· \pi_i: \mathcal{O}_i \to \mathcal{A}_i: Policy for agent i
Â· V^\pi(s): Value function for policy Ï€
Â· Q^\pi(s,a): Action-value function

Optimization

Â· f_i: Objective function for agent i
Â· g_j \leq 0: Inequality constraint
Â· h_k = 0: Equality constraint
Â· \mathcal{X}: Feasible set
Â· \lambda_j: Lagrange multiplier for inequality constraint
Â· \mu_k: Lagrange multiplier for equality constraint
Â· \mathcal{L}: Lagrangian function
Â· \alpha_k: Step size at iteration k
Â· \rho: Penalty parameter (in ADMM)
Â· \mathbf{w}: Weight vector in multi-objective optimization

Consensus Algorithms

Â· x_i(k): State of agent i at iteration k
Â· \mathbf{A}(k): Consensus matrix at iteration k
Â· a_{ij}(k): Weight agent i assigns to agent j
Â· \rho: Contraction rate (not to be confused with ADMM parameter)
Â· \epsilon: Convergence tolerance
Â· T_{conv}: Convergence time/number of iterations

B.4 Resilience and Security

Fault Models

Â· \mathcal{F}: Set of faulty/Byzantine agents
Â· \mathcal{H}: Set of non-faulty (honest) agents
Â· f = |\mathcal{F}|: Number of faulty agents
Â· F = (t_{start}, t_{end}, \tau, \phi): Fault tuple
Â· \tau \in \{\text{transient}, \text{intermittent}, \text{permanent}\}: Fault type
Â· \phi: Fault effect function

Anomaly Detection

Â· \hat{x}_j(t|i): Prediction of agent j's state by agent i
Â· R_{ij}(t): Reputation of agent j according to agent i
Â· \alpha: Forgetting factor in reputation update
Â· \epsilon: Tolerance threshold for prediction error
Â· \theta_{low}: Low reputation threshold
Â· \tau: Consecutive steps for suspicion (not to be confused with fault type)
Â· S_i(t): CUSUM statistic for agent i
Â· h: CUSUM threshold
Â· \nu: CUSUM reference value

Formal Verification

Â· M = (S, S_0, R, L): Kripke structure
Â· S: Set of states
Â· S_0 \subseteq S: Initial states
Â· R \subseteq S \times S: Transition relation
Â· L: S \to 2^{AP}: Labeling function
Â· AP: Set of atomic propositions
Â· \phi, \psi: LTL formulas

B.5 Frequently Used Constants

Physical Constants

Â· G = 6.67430 \times 10^{-11} \text{ m}^3 \text{ kg}^{-1} \text{ s}^{-2}: Gravitational constant
Â· M_E = 5.972 \times 10^{24} \text{ kg}: Earth's mass
Â· R_E = 6.378 \times 10^6 \text{ m}: Earth's equatorial radius
Â· J_2 = 1.08263 \times 10^{-3}: Earth's second zonal harmonic
Â· c = 2.99792458 \times 10^8 \text{ m/s}: Speed of light
Â· k_B = 1.380649 \times 10^{-23} \text{ J/K}: Boltzmann constant
Â· g_0 = 9.80665 \text{ m/s}^2: Standard gravity

Orbital Parameters (Typical LEO)

Â· Altitude: 500-1200 km
Â· Period: ~90-120 minutes
Â· Velocity: ~7.5 km/s
Â· Inclination: 53Â° (Starlink), 86Â° (polar), 98Â° (sun-synchronous)

Communication Parameters

Â· Frequency bands: L, S, C, X, Ka, V (for satellite comm)
Â· ISL range: 100-5000 km
Â· Data rates: 100 Mbps - 10 Gbps (laser), 10-100 Mbps (RF)
Â· Beamwidth: 0.1Â°-10Â° depending on antenna size and frequency

B.6 Abbreviations

General

Â· CPS: Cyber-Physical System
Â· LEO: Low Earth Orbit
Â· GEO: Geostationary Earth Orbit
Â· MEO: Medium Earth Orbit
Â· ISL: Inter-Satellite Link
Â· GSL: Ground-Satellite Link

AI and Control

Â· AI: Artificial Intelligence
Â· MAS: Multi-Agent System
Â· MARL: Multi-Agent Reinforcement Learning
Â· Dec-POMDP: Decentralized Partially Observable Markov Decision Process
Â· MDP: Markov Decision Process
Â· POMDP: Partially Observable Markov Decision Process
Â· MPC: Model Predictive Control
Â· ADMM: Alternating Direction Method of Multipliers
Â· KKT: Karush-Kuhn-Tucker conditions

Networking

Â· TVG: Time-Varying Graph
Â· QoS: Quality of Service
Â· SNR: Signal-to-Noise Ratio
Â· BER: Bit Error Rate
Â· FSPL: Free Space Path Loss
Â· LoS: Line-of-Sight

Optimization

Â· LP: Linear Program
Â· QP: Quadratic Program
Â· SOCP: Second-Order Cone Program
Â· SDP: Semidefinite Program
Â· MILP: Mixed-Integer Linear Program

Resilience and Security

Â· CUSUM: Cumulative Sum (algorithm)
Â· LTL: Linear Temporal Logic
Â· CTL: Computation Tree Logic
Â· BFT: Byzantine Fault Tolerant
Â· PKI: Public Key Infrastructure
Â· QKD: Quantum Key Distribution

Appendix C: Glossary of Terms

A

Adjacency Matrix: A square matrix used to represent a graph, where entry (i,j) is 1 if there is an edge from vertex i to vertex j, and 0 otherwise.

Agent: An autonomous entity that perceives its environment and takes actions to achieve goals. In this thesis, each satellite is modeled as an agent.

Anomaly Detection: The identification of unusual patterns or behaviors that deviate from normal operation, potentially indicating faults or attacks.

Augmented Lagrangian: A Lagrangian function with an additional penalty term for constraint violation, used in optimization methods like ADMM to improve convergence.

Autonomy: The ability of a system to operate without human intervention, making decisions based on its own perception and reasoning.

B

Barrier Function: A function used in constrained optimization that becomes very large as the solution approaches the constraint boundary, effectively keeping the solution within the feasible region.

Byzantine Fault: A type of fault where a component fails in an arbitrary way, potentially sending conflicting information to different parts of the system. Named after the Byzantine Generals Problem.

C

Cheeger Constant: A graph-theoretic measure of how well-connected a graph is, related to the spectral gap of the graph Laplacian.

Consensus: Agreement among multiple agents on a common value or decision through local communication and computation.

Constraint Qualification: Conditions that ensure the Karush-Kuhn-Tucker conditions are necessary for optimality in constrained optimization.

Control Lyapunov Function: A Lyapunov function for which there exists a control law that makes the derivative negative definite, used to prove stability of controlled systems.

Convex Function: A function where the line segment between any two points on the graph lies above or on the graph. Formally, f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y) for all x,y and \lambda \in [0,1].

Convex Set: A set where the line segment between any two points in the set is entirely contained within the set.

Coâ€‘Simulation: Simultaneous simulation of multiple subsystems (e.g., physical dynamics and network communication) with synchronization between them.

CPS (Cyberâ€‘Physical System): A system integrating computational algorithms and physical components, with feedback loops where physical processes affect computations and vice versa.

CUSUM (Cumulative Sum): A sequential analysis technique for change detection, optimal for detecting shifts in the mean of a process.

D

Decâ€‘POMDP (Decentralized Partially Observable Markov Decision Process): A framework for multi-agent decision-making under uncertainty where each agent has partial observations of the global state.

Doppler Shift: The change in frequency of a wave due to relative motion between source and observer.

Dwell Time: In switched systems, the minimum time between mode switches, often required to ensure stability.

E

ECI (Earthâ€‘Centered Inertial): A coordinate system with origin at Earth's center, fixed relative to distant stars (non-rotating with Earth).

Envelope Theorem: A result in optimization giving the derivative of the optimal value function with respect to parameters.

Ergodicity Coefficient: A measure of how quickly a Markov chain or consensus process converges to its stationary distribution.

F

Free Space Path Loss: The attenuation of radio signals as they travel through free space without obstacles.

G

Graceful Degradation: The ability of a system to maintain partial functionality when some components fail, rather than failing completely.

Graph Diameter: The maximum distance (number of edges) between any two vertices in a graph.

H

Hamiltonian: In orbital mechanics, the sum of kinetic and potential energy; in optimal control, the function used in Pontryagin's maximum principle.

I

Informationâ€‘Theoretic Security: Security based on information theory, where even with unlimited computational power, an adversary cannot break the system.

Interâ€‘Satellite Link (ISL): A communication link between two satellites.

J

Jâ‚‚ Perturbation: The dominant perturbation in Earth's gravitational field due to Earth's oblateness (equatorial bulge).

Joint Connectivity: A property of time-varying graphs where the union of graphs over a finite time interval is connected.

K

Karushâ€‘Kuhnâ€‘Tucker (KKT) Conditions: Necessary conditions for optimality in constrained optimization, generalizing Lagrange multipliers to inequality constraints.

Kripke Structure: A mathematical model used in model checking, consisting of states, transitions, and labeling of states with atomic propositions.

L

Lagrangian: A function that combines the objective and constraints of an optimization problem, used to find optimality conditions.

Lagrangian Multiplier: A scalar variable associated with a constraint in optimization, representing the sensitivity of the optimal value to that constraint.

Lineâ€‘ofâ€‘Sight (LoS): Direct visibility between two points without obstruction.

Linear Temporal Logic (LTL): A modal temporal logic used in formal verification to specify properties of systems over time.

Link Budget: Calculation of all gains and losses in a communication link to determine received signal power and signal-to-noise ratio.

Lyapunov Function: A scalar function that can be used to prove stability of an equilibrium point, decreasing along system trajectories.

