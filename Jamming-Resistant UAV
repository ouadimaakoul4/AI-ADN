CHAPTER 1: INTRODUCTION

1.1. The Imperative for Resilient Disaster Response Systems

Natural and man-made disasters represent a persistent global challenge, with urbanization increasing vulnerability to catastrophic events. Earthquakes, hurricanes, floods, and industrial accidents can devastate infrastructure, disrupt communication networks, and create environments where human first responders face immediate danger. The initial 72-hour "golden window" following a disaster is critical for locating and rescuing survivors, yet precisely during this period, traditional response mechanisms are often most compromised.

Urban Search and Rescue (USAR) operations exemplify these challenges, requiring rapid situational assessment in unstable, unstructured environments where buildings have collapsed, roads are impassable, and conventional communication systems have failed. The 2010 Haiti earthquake demonstrated how infrastructure collapse can paralyze coordinated response efforts, while the 2023 Turkey-Syria earthquakes highlighted the limitations of existing robotic systems in complex rubble environments.

1.2. Current Technological Limitations in USAR Robotics

Despite significant advancements in robotics over the past decade, current USAR robotic systems face fundamental limitations that restrict their effectiveness in real disaster scenarios:

1.2.1. Communication Vulnerability

Existing UAV platforms primarily rely on point-to-point wireless links (typically Wi-Fi or 4G/5G) that are:

Â· Susceptible to interference from damaged infrastructure and emergency equipment
Â· Vulnerable to intentional jamming in contested environments
Â· Limited in range, especially in urban canyons and collapsed structures
Â· Prone to congestion when multiple agencies deploy overlapping systems

1.2.2. Energy Constraints

Commercial UAVs offer limited flight durations (typically 20-40 minutes), requiring:

Â· Frequent battery swaps or returns to charging stations
Â· Dedicated support personnel for energy management
Â· Pre-planned missions rather than adaptive, extended operations
Â· Compromises between flight time and payload capacity

1.2.3. Connectivity Gaps

Single-network dependency creates operational fragility:

Â· Terrestrial networks fail in infrastructure-damaged areas
Â· Satellite communications suffer from latency and bandwidth constraints
Â· Aerial networks require dedicated infrastructure unlikely in disaster zones
Â· Handoff mechanisms between networks are crude or non-existent

1.2.4. Cognitive Limitations

Most systems operate with minimal autonomy, requiring:

Â· Constant human supervision and control
Â· Pre-programmed flight paths ill-suited to dynamic environments
Â· Limited adaptation to unexpected obstacles or conditions
Â· No collaborative intelligence between robotic units

1.3. Paradigm Shift: From Vulnerable Tools to Resilient Systems

Recent technological developments suggest a paradigm shift is possible. Three concurrent advancements create new opportunities:

First, the maturation of Space-Air-Ground Integrated Networks (SAGIN) provides a multi-layer communication architecture that could offer resilient connectivity through heterogeneous network elements working in concert.

Second, advances in radio frequency energy harvesting (RF-EH) enable devices to scavenge ambient electromagnetic energy, potentially turning interference sources into power supplies.

Third, breakthroughs in deep reinforcement learning (DRL) offer new approaches for autonomous decision-making in complex, dynamic environments without requiring explicit programming for every contingency.

1.4. Research Gap and Integration Opportunity

While each of these technologies has seen independent development, their integration remains unexplored territory. Specifically, no existing system:

1. Simultaneously addresses energy sustainability and communication security in contested environments
2. Leverages SAGIN's heterogeneity while managing its complexity through intelligent autonomy
3. Transforms threats (jamming) into resources (energy) through adaptive learning
4. Coordinates multi-agent systems in disrupted environments without centralized infrastructure

This represents a critical research gap at the intersection of wireless communications, energy systems, artificial intelligence, and disaster robotics. The integration of these domains offers the potential for systems that are not merely robust to failures but actually benefit from some aspects of environmental disruption.

1.5. Thesis Statement and Core Proposition

Thesis Statement: "A hierarchical deep reinforcement learning framework that integrates energy harvesting from jamming signals with adaptive SAGIN connectivity management enables autonomous, resilient UAV swarms capable of sustained operations in contested disaster environments, significantly outperforming current systems in throughput, energy efficiency, and mission completion rates."

Core Proposition: By reframing jamming as both a threat to mitigate and an energy resource to harvest, and by managing heterogeneous network connectivity through hierarchical learning, UAV swarms can achieve unprecedented resilience and autonomy in disaster response scenarios.

1.6. Research Objectives

This research aims to achieve five specific objectives:

1. Develop an integrated system architecture that coherently combines RF energy harvesting, SAGIN connectivity, and swarm intelligence for USAR applications.
2. Design a hierarchical deep reinforcement learning algorithm that jointly optimizes communication security, energy sustainability, and mobility management in dynamic, contested environments.
3. Establish theoretical foundations for the convergence and performance bounds of the proposed learning framework under realistic constraints.
4. Implement and validate the system through comprehensive simulation across diverse disaster scenarios and limited prototype field testing.
5. Derive practical guidelines for deployment by disaster response agencies, addressing real-world constraints including regulatory, ethical, and operational considerations.

1.7. Expected Contributions

This research will contribute to multiple domains:

1.7.1. Theoretical Contributions

Â· A novel formulation of the joint energy-communication-security optimization problem as a constrained hierarchical Markov Decision Process
Â· Convergence guarantees for the proposed HDRL algorithm in non-stationary, multi-agent environments
Â· Analytical bounds on performance metrics including throughput, energy efficiency, and interference tolerance

1.7.2. Algorithmic Innovations

Â· A new hierarchical DRL architecture specifically designed for SAGIN environments with security threats
Â· Integration of interference-aware exploration mechanisms with constrained policy optimization
Â· Swarm coordination strategies using centralized training with decentralized execution

1.7.3. Practical Advancements

Â· Open-source simulation framework for resilient autonomous systems in disaster scenarios
Â· Hardware prototype demonstrating RF energy harvesting from jamming signals
Â· Deployment guidelines and performance benchmarks for disaster response agencies

1.7.4. Cross-Domain Synthesis

Â· Integration of concepts from cognitive radio, energy harvesting, multi-agent systems, and disaster robotics
Â· Validation of "threats as resources" paradigm in wireless security
Â· Demonstration of AI-driven autonomy in safety-critical applications

1.8. Thesis Structure

This thesis is organized as follows:

Chapter 2 provides a comprehensive literature review spanning USAR robotics, energy harvesting, anti-jamming strategies, SAGIN architectures, and reinforcement learning applications.

Chapter 3 presents the system architecture and mathematical models, formalizing the integrated problem.

Chapter 4 details the hierarchical deep reinforcement learning framework design, including state, action, and reward formulations.

Chapter 5 develops the complete algorithm and provides theoretical analysis of convergence and performance.

Chapter 6 describes the simulation framework and experimental methodology.

Chapter 7 presents performance evaluation across multiple disaster scenarios and comparative analysis.

Chapter 8 addresses implementation challenges, prototyping, and field testing considerations.

Chapter 9 concludes with summary findings, limitations, and future research directions.

---

CHAPTER 2: LITERATURE REVIEW

2.1. Evolution of Urban Search and Rescue Robotics

2.1.1. Historical Development

The field of USAR robotics emerged from military applications, with early systems like the "Throwbot" and "PackBot" demonstrating potential for reconnaissance in hazardous environments. The 9/11 World Trade Center response marked a turning point, highlighting both the potential and limitations of existing robotic systems. Subsequent DARPA challenges (2004-2007) drove significant advancements in autonomy and mobility, though primarily in structured environments.

2.1.2. Current State of Practice

Modern USAR robotics employs diverse platforms:

Â· Ground robots for interior exploration of collapsed structures
Â· UAVs for aerial reconnaissance and mapping
Â· Aquatic robots for flood response
Â· Hybrid systems combining multiple modalities

The National Institute of Standards and Technology (NIST) has established standard test methods for USAR robots, creating benchmarks for mobility, sensing, and autonomy. Despite these standards, field deployments remain limited by the challenges outlined in Chapter 1.

2.1.3. Research Frontiers

Current research focuses on:

Â· Multi-robot systems for coordinated coverage
Â· Human-robot collaboration through intuitive interfaces
Â· Semantic understanding of disaster environments
Â· Resilient communication in degraded conditions

However, integrated solutions addressing communication, energy, and security simultaneously remain scarce.

2.2. Wireless Communication Challenges in Disaster Environments

2.2.1. Propagation Characteristics

Disaster environments present unique propagation challenges:

Â· Non-line-of-sight conditions in rubble and collapsed structures
Â· Time-varying channel characteristics due to structural instability
Â· Multipath effects from irregular surfaces and debris
Â· Attenuation from moisture, dust, and fire

Empirical studies following major disasters have documented severe degradation in signal strength and quality, with cellular networks often collapsing under simultaneous user loads from responders and affected populations.

2.2.2. Spectrum Management Issues

Disaster response typically involves multiple agencies using overlapping frequency bands, creating:

Â· Co-channel interference between different response teams
Â· Adjacent channel interference from high-power emergency equipment
Â· Regulatory challenges in coordinating spectrum use across jurisdictions
Â· Priority access dilemmas during life-critical operations

2.2.3. Jamming and Interference

Both intentional and unintentional interference present threats:

Â· Malicious jamming from hostile actors in contested environments
Â· Unintentional jamming from damaged electrical equipment
Â· Self-interference in multi-robot systems
Â· Cross-modulation from high-power transmitters

Traditional frequency-hopping and spread-spectrum techniques provide limited protection against adaptive jammers in dynamic environments.

2.3. Energy Harvesting for Autonomous Systems

2.3.1. RF Energy Harvesting Fundamentals

RF-EH converts electromagnetic waves to direct current through:

Â· Rectennas (rectifying antennas) for RF-to-DC conversion
Â· Impedance matching networks for efficiency optimization
Â· Power management circuits for storage and regulation

Theoretical models describe harvesting efficiency as a function of input power, frequency, and circuit design, with practical systems achieving 1-50% efficiency depending on signal strength and design sophistication.

2.3.2. Ambient RF Sources in Disaster Scenarios

Potential energy sources include:

Â· Cellular base station signals (typically -50 to -80 dBm at ground level)
Â· Satellite signals (GPS, communication satellites: -130 to -150 dBm)
Â· Emergency broadcast transmitters
Â· Jamming signals (often high-power, though intermittent)
Â· Inter-agency communication signals

The power density of these sources varies significantly by location, time, and disaster conditions, creating a stochastic energy availability profile.

2.3.3. Simultaneous Wireless Information and Power Transfer (SWIPT)

SWIPT techniques enable concurrent information decoding and energy harvesting from the same signal through:

Â· Time switching between information and energy modes
Â· Power splitting of received signal
Â· Antenna separation using dedicated harvesting elements

Recent advances in circuit design have improved SWIPT efficiency, though trade-offs remain between information rate and harvested energy.

2.4. Anti-Jamming Strategies in Wireless Networks

2.4.1. Traditional Approaches

Conventional anti-jamming techniques include:

Â· Frequency hopping spread spectrum (FHSS): Rapid channel switching according to pseudorandom sequences
Â· Direct sequence spread spectrum (DSSS): Spreading signals over wider bandwidths
Â· Beamforming and directional antennas: Spatial filtering to avoid jammers
Â· Power control: Adaptive transmission power to overcome interference

These methods assume stationary or predictable jamming patterns and often require coordination mechanisms vulnerable to disruption.

2.4.2. Cognitive Radio Approaches

Cognitive radio frameworks introduce intelligence through:

Â· Spectrum sensing to detect available bands
Â· Dynamic spectrum access to opportunistically use whitespace
Â· Machine learning to predict interference patterns
Â· Game-theoretic formulations for competitive spectrum sharing

The work of Abdolkhani et al. (2025) represents a state-of-the-art example, using DRL to balance transmission, harvesting, and power control in jammed environments. However, their approach focuses on stationary IoT devices rather than mobile robotic systems with additional constraints.

2.4.3. Limitations of Existing Methods

Current anti-jamming strategies suffer from:

Â· Reactivity rather than proactivity: Responding to rather than anticipating interference
Â· Single-agent perspectives: Ignoring multi-agent coordination opportunities
Â· Energy-agnostic designs: Treating energy as unlimited rather than constrained
Â· Network homogeneity assumptions: Designed for single-network environments

2.5. Space-Air-Ground Integrated Networks (SAGIN)

2.5.1. Architectural Components

SAGIN integrates three layers:

Â· Space segment: Satellites (GEO, MEO, LEO) providing wide-area coverage
Â· Aerial segment: UAVs, balloons, and aircraft creating ad-hoc networks
Â· Ground segment: Terrestrial base stations and ad-hoc networks

Each segment offers complementary characteristics in coverage, bandwidth, latency, and mobility support.

2.5.2. Mobility Management Challenges

SAGIN introduces complex mobility management due to:

Â· Heterogeneous handoffs between different network types
Â· Variable latency across network segments
Â· Asymmetric bandwidth for uplink vs. downlink
Â· Dynamic topology in aerial and ground segments

Traditional mobility protocols (e.g., Mobile IP) perform poorly in such heterogeneous environments.

2.5.3. State-of-the-Art Approaches

Recent research has explored:

Â· Software-defined networking for unified control planes
Â· Network function virtualization for flexible service deployment
Â· Machine learning for predictive handoff decisions
Â· Blockchain for decentralized trust management

The work of Wan et al. (2025) presents a hierarchical DRL approach for joint link selection and trajectory optimization, demonstrating significant improvements over traditional methods. However, their formulation does not address security threats or energy constraints.

2.6. Reinforcement Learning in Autonomous Systems

2.6.1. Deep Reinforcement Learning Fundamentals

DRL combines deep learning with reinforcement learning through:

Â· Value-based methods (DQN, DDQN): Learning action-value functions
Â· Policy-based methods (REINFORCE, PPO): Direct policy optimization
Â· Actor-critic methods (A3C, SAC, TD3): Combining value and policy approaches
Â· Model-based methods: Learning environment dynamics for planning

Key challenges include sample efficiency, exploration-exploitation trade-offs, and stability in non-stationary environments.

2.6.2. Multi-Agent Reinforcement Learning

Multi-agent systems introduce additional complexity:

Â· Non-stationarity: Other agents' learning changes the environment
Â· Credit assignment: Attributing outcomes to individual actions
Â· Scalability: Exponential growth of joint action spaces
Â· Communication requirements: For coordination or information sharing

Approaches include independent learners, centralized training with decentralized execution, and opponent modeling.

2.6.3. Hierarchical Reinforcement Learning

HRL addresses temporal abstraction through:

Â· Options framework: Temporally extended actions
Â· Feudal learning: Manager-worker hierarchies
Â· Goal-conditioned policies: Learning at multiple time scales
Â· Skill discovery: Automatic abstraction learning

HRL is particularly suited to problems with natural hierarchical structure, such as the link selection vs. trajectory optimization problem in SAGIN mobility management.

2.7. Synthesis: Identified Research Gaps

The literature review reveals several critical gaps:

2.7.1. Integration Gap

No existing work integrates energy harvesting, anti-jamming, and SAGIN mobility management into a unified framework. Solutions address these challenges in isolation, missing synergistic opportunities.

2.7.2. Threat-Resource Duality Gap

While Abdolkhani et al. (2025) demonstrate energy harvesting in jammed environments, they treat jamming primarily as a threat to avoid rather than a resource to exploit. The full potential of "jammers as energy sources" remains unexplored, particularly in mobile systems.

2.7.3. Hierarchy Design Gap

Wan et al. (2025) present a hierarchical approach for SAGIN mobility, but their hierarchy is manually designed rather than learned, and doesn't incorporate security or energy considerations. Adaptive hierarchy learning for integrated problems represents an open challenge.

2.7.4. Swarm Intelligence Gap

Multi-agent approaches for USAR typically focus on coverage or task allocation, neglecting the joint optimization of communication, energy, and security across the swarm. Coordinated anti-jamming and energy harvesting strategies remain unexplored.

2.7.5. Real-World Validation Gap

Most proposed solutions are evaluated in simplified simulations without realistic channel models, energy constraints, or disaster dynamics. Field validation in representative environments is notably absent.

2.8. Positioning of This Research

This thesis addresses these gaps by:

1. Integrating energy harvesting, anti-jamming, and SAGIN mobility into a unified HDRL framework
2. Reframing jamming signals as both threats to mitigate and resources to harvest
3. Designing an adaptive hierarchical learning architecture for the integrated problem
4. Developing swarm coordination mechanisms for collaborative resilience
5. Validating through high-fidelity simulation and limited field testing

By bridging these domains, this research advances the state of the art toward truly resilient autonomous systems for disaster response.

CHAPTER 3: SYSTEM ARCHITECTURE AND MATHEMATICAL MODELING

3.1. Comprehensive System Architecture

3.1.1. Overall Framework Overview

The proposed system employs a multi-layered architecture integrating physical hardware, communication networks, and intelligent algorithms. The core principle is to create a closed-loop resilient system where each component contributes to overall robustness through redundancy, adaptability, and synergy.

Architecture Components:

1. Physical Layer: UAV platforms with specialized hardware modules
2. Communication Layer: Heterogeneous SAGIN connectivity
3. Intelligence Layer: Hierarchical DRL for decision-making
4. Application Layer: USAR-specific mission execution

3.1.2. Hardware Specification and Integration

Each UAV in the swarm is equipped with the following hardware modules:

Component Specification Purpose
Platform Quadcopter with 40+ min baseline flight time Mobility platform
Processing Unit NVIDIA Jetson AGX Orin (32GB) Onboard DRL inference and sensor processing
Communication Suite Multi-band SDR (2.4GHz, 5GHz, Ku-band) Simultaneous connectivity to terrestrial, aerial, and satellite networks
RF Energy Harvester Wideband rectenna array (2-7 GHz) Ambient RF energy harvesting
Primary Sensors 4K camera with gimbal, LiDAR, thermal imaging Victim detection and mapping
Secondary Sensors IMU, GPS, altimeter, gas sensors Navigation and environmental monitoring
Power System Li-ion battery (10,000 mAh) with supercapacitor buffer Energy storage and management

Integration Architecture:
The hardware components follow a modular design with standardized interfaces(USB 3.0, PCIe, GPIO) to enable rapid configuration changes for different mission profiles. The energy harvesting circuit is integrated with the power management system through maximum power point tracking (MPPT) controllers that optimize harvesting efficiency under varying input power conditions.

3.1.3. Software Stack Design

The software architecture employs a layered approach:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Mission Control Interface          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Application Layer: USAR Functions        â”‚
â”‚  â€¢ Victim detection     â€¢ Map generation     â”‚
â”‚  â€¢ Path planning        â€¢ Swarm coordination â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Intelligence Layer: DRL Framework       â”‚
â”‚  â€¢ HDRL agent           â€¢ CTDE coordination  â”‚
â”‚  â€¢ Experience replay    â€¢ Model updates      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Communication Layer: SAGIN Management    â”‚
â”‚  â€¢ Link selection       â€¢ Handoff management â”‚
â”‚  â€¢ Protocol translation â€¢ QoS monitoring     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Hardware Abstraction Layer           â”‚
â”‚  â€¢ Device drivers       â€¢ Sensor fusion      â”‚
â”‚  â€¢ Power management     â€¢ Emergency control  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The software is implemented in ROS 2 (Robot Operating System) for modularity and real-time performance, with DRL components developed in PyTorch for flexibility in model architecture.

3.2. SAGIN Integration Model

3.2.1. Multi-Layer Network Characterization

The SAGIN model comprises three distinct but cooperative layers:

Terrestrial Network (GN):

Â· Base Station Density: Î»_g [BS/kmÂ²] following Poisson Point Process
Â· Coverage Radius: R_g = 200-500m for urban microcells
Â· Channel Model: 3GPP Urban Macro (UMa) path loss:
  PL_{g}(d) = 28.0 + 22\log_{10}(d) + 20\log_{10}(f_c) + X_\sigma
  
  where d is distance in meters, f_c is carrier frequency in GHz, and X_Ïƒ ~ ğ’©(0, ÏƒÂ²) is shadowing.

Aerial Network (AN):

Â· Deployment: UAV-mounted base stations at altitude h_a = 100-300m
Â· Channel Model: Air-to-Ground path loss with LoS probability:
  P_{LoS} = \frac{1}{1 + a \exp(-b[\frac{180}{\pi}\theta - a])}
  
  where Î¸ is elevation angle, a and b are environment-dependent constants.
Â· Path Loss: 
  PL_{a}(d) = 20\log_{10}(\frac{4\pi f_c d}{c}) + \eta_{LoS/NLoS}
  
  where Î·_LoS and Î·_NLoS represent additional losses for LoS and NLoS links.

Satellite Network (SN):

Â· Constellation: LEO satellites at altitude h_s = 550-1200km
Â· Coverage: Footprint radius R_s â‰ˆ âˆš(2R_e h_s) where R_e is Earth's radius
Â· Channel Model: Free-space path loss with atmospheric attenuation:
  PL_{s}(d) = 20\log_{10}(\frac{4\pi d}{\lambda}) + L_{atm}
  
  where L_atm includes rain, cloud, and gaseous absorption losses.

3.2.2. Heterogeneous Link Capacity Model

The achievable data rate for UAV u connected to network type n âˆˆ {g, a, s} at time t is:

\mathcal{R}_{u,n}(t) = B_n \log_2\left(1 + \frac{P_t G_t G_r |h_{u,n}(t)|^2}{PL_{n}(d_{u,n}(t)) N_0 B_n + I_{u,n}(t)}\right)

where:

Â· B_n: Allocated bandwidth for network type n
Â· P_t: Transmit power
Â· G_t, G_r: Transmit and receive antenna gains
Â· h_{u,n}(t): Small-scale fading component (Rayleigh/Rician)
Â· PL_n(d_{u,n}(t)): Path loss as defined above
Â· N_0: Noise power spectral density
Â· I_{u,n}(t): Interference from other transmitters

3.2.3. Handoff Model and Costs

Network switching incurs multiple costs:

Temporal Cost: Handoff latency Ï„_switch = Ï„_discovery + Ï„_authentication + Ï„_registration

Energy Cost: Switching power consumption E_switch = P_switch Ã— Ï„_switch

Quality Cost: Potential data loss during handoff

The total switching cost for UAV u switching from network i to j at time t is:

C_{switch}^{u}(iâ†’j,t) = Î±_1 \frac{\tau_{switch}^{iâ†’j}}{\tau_{max}} + Î±_2 \frac{E_{switch}^{iâ†’j}}{E_{max}} + Î±_3 \frac{\Delta \mathcal{R}_{u}(t)}{\mathcal{R}_{req}}

where Î±_1, Î±_2, Î±_3 are weighting coefficients, and denominators normalize each component.

3.3. Jamming and Interference Modeling

3.3.1. Jammer Classification and Behavior

Jammers are classified based on their operational characteristics:

Type I: Sweep Jammers

Â· Frequency-sweeping across a bandwidth B_j with sweep period T_s
Â· Instantaneous bandwidth B_inst << B_j
Â· Model: J_1(t,f) = P_j \cdot rect\left(\frac{f - f_0(t)}{B_{inst}}\right)
  
  where f_0(t) = f_min + (t mod T_s) Ã— (B_j/T_s)

Type II: Barrage Jammers

Â· Continuous transmission across entire bandwidth B_j
Â· Constant power spectral density
Â· Model: J_2(f) = \frac{P_j}{B_j} \text{ for } f âˆˆ [f_c - B_j/2, f_c + B_j/2]

Type III: Reactive Jammers

Â· Sense and jam only when signals are detected
Â· Reaction time Ï„_react and jamming duration T_jam
Â· Model: Markov chain with states {Sensing, Jamming} with transition probabilities based on signal detection

Type IV: Smart Jammers

Â· Learn and adapt to evasion strategies
Â· Employ reinforcement learning to maximize interference
Â· Modeled as adversarial agents in a game-theoretic framework

3.3.2. Jamming Impact Quantification

The effective interference experienced by UAV u on network n at time t is:

I_{eff}^{u,n}(t) = \int_{f_c-B/2}^{f_c+B/2} J(t,f) â‹… |H_{u,n}(f,t)|^2 â‹… A_{RF}(f) df

where:

Â· J(t,f): Jammer power spectral density
Â· H_{u,n}(f,t): Channel frequency response
Â· A_{RF}(f): RF front-end frequency response

The binary jamming indicator is:
\omega_j^u(t) = 
\begin{cases} 
1 & \text{if } \frac{I_{eff}^{u,n}(t)}{N_0 B} > \gamma_{thresh} \\
0 & \text{otherwise}
\end{cases}

where Î³_thresh is the interference-to-noise ratio threshold for declaring a link jammed.

3.3.3. Multi-Jammer Environment Modeling

In realistic disaster scenarios, multiple jammers may operate simultaneously. The aggregate interference is modeled as:

I_{total}^{u,n}(t) = \sum_{k=1}^{K} I_{eff,k}^{u,n}(t) + \sum_{l=1}^{L} I_{unint,l}^{u,n}(t)

where K intentional jammers and L unintentional interference sources contribute to the total interference power.

3.4. Energy Harvesting Mathematical Framework

3.4.1. RF Energy Harvesting Circuit Model

The harvesting circuit converts RF power P_RF to DC power P_DC through:

P_{DC} = Î·_{EH}(P_{RF}) â‹… P_{RF}

where Î·_EH is the harvesting efficiency function:

Î·_{EH}(P_{RF}) = \frac{Î·_{max}}{1 + \exp(-Î²(P_{RF} - P_{sat}))} â‹… (1 - \exp(-Î± P_{RF}))

Parameters:

Â· Î·_max: Maximum theoretical efficiency (typically 50-70%)
Â· P_sat: Input power at which efficiency saturates
Â· Î±, Î²: Circuit-specific parameters from diode I-V characteristics

3.4.2. Stochastic Energy Arrival Model

Energy arrivals from ambient RF sources follow a compound stochastic process:

E_{arrival}(t) = \sum_{i=1}^{N_{source}(t)} X_i â‹… Î´(t - Ï„_i)

where:

Â· N_source(t): Poisson process with rate Î»_source(t) varying by location and time
Â· X_i: Energy quantum from source i, distributed as truncated Pareto:
  f_X(x) = \frac{Î± x_m^Î±}{x^{Î±+1}} \text{ for } x â‰¥ x_m
Â· Ï„_i: Arrival times following inhomogeneous Poisson process

From jammers specifically, energy arrival follows a modulated process:
Î»_{jammer}(t) = Î»_0 â‹… (1 + \sin(2Ï€f_{jam} t + Ï•))


reflecting the periodic nature of many jamming waveforms.

3.4.3. Battery Dynamics with Harvesting

The battery state-of-charge (SOC) evolves as:

\frac{dSOC(t)}{dt} = \frac{1}{C_{bat}} \left( Î·_{charge} P_{EH}(t) - \frac{P_{cons}(t)}{Î·_{discharge}} \right)

where:

Â· C_bat: Battery capacity in Joules
Â· Î·_charge, Î·_discharge: Charge/discharge efficiencies
Â· P_EH(t): Harvested power (time-varying)
Â· P_cons(t) = P_prop + P_comm + P_comp: Total power consumption

Discretized for implementation:
SOC_{k+1} = SOC_k + \frac{Î”t}{C_{bat}} \left( Î·_{charge} P_{EH}[k] - \frac{P_{cons}[k]}{Î·_{discharge}} \right)

with constraints: SOC_min â‰¤ SOC_k â‰¤ SOC_max.

3.4.4. Energy-Quality Trade-off Formulation

The fundamental trade-off between communication quality and energy consumption is captured by:

\max_{P_t, f_c, BW} \mathcal{R}(P_t, f_c, BW)


subject to:P_{total}(P_t, f_c, BW) â‰¤ P_{budget}(SOC)

where P_budget(SOC) is the power budget determined by current SOC and desired mission duration.

3.5. Unified System State Representation

The complete system state for UAV u at time t is represented as a tuple:

s_u(t) = \langle s_{pos}, s_{energy}, s_{comm}, s_{jam}, s_{env} \rangle

3.5.1. Position and Mobility State

s_{pos} = [x_u, y_u, z_u, v_x, v_y, v_z, Ïˆ, Î¸, Ï•]^T


where(x,y,z) are 3D coordinates, (v_x,v_y,v_z) are velocity components, and (Ïˆ,Î¸,Ï•) are yaw, pitch, roll angles.

3.5.2. Energy State

s_{energy} = [SOC, P_{EH}, P_{cons}, T_{rem}]^T


where SOC is state-of-charge,P_EH is current harvesting power, P_cons is consumption power, and T_rem is estimated remaining mission time at current consumption.

3.5.3. Communication State

s_{comm} = [n_{curr}, \mathcal{R}_{curr}, \mathcal{R}_{req}, SNR_{curr}, CQI, N_{handoff}]^T


where n_curr is current network association,R_curr and R_req are current and required data rates, SNR_curr is signal-to-noise ratio, CQI is channel quality indicator, and N_handoff is number of handoffs performed.

3.5.4. Jamming State

s_{jam} = [Ï‰_j, I_{eff}, f_{jam}, BW_{jam}, type_{jam}, \hat{Ï„}_{jam}]^T


where Ï‰_j is jamming indicator,I_eff is effective interference, f_jam and BW_jam are estimated jammer frequency and bandwidth, type_jam is classified jammer type, and Ï„Ì‚_jam is estimated jammer periodicity.

3.5.5. Environmental State

s_{env} = [T, P, RH, vis, W_x, W_y, W_z, obs_{map}]^T


where T,P, RH are temperature, pressure, humidity; vis is visibility; W are wind components; and obs_map is local obstacle map.

The complete state vector has dimensionality 35, designed to be rich enough for informed decision-making while maintaining computational tractability for real-time inference.

---

CHAPTER 4: HIERARCHICAL DEEP REINFORCEMENT LEARNING FRAMEWORK DESIGN

4.1. Problem Formulation as Constrained Markov Decision Process

4.1.1. CMDP Formulation

The integrated problem is formulated as a Constrained Markov Decision Process (CMDP) defined by the tuple:

\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{C}, \gamma \rangle

where:

Â· State Space $\mathcal{S}$: As defined in Section 3.5
Â· Action Space $\mathcal{A}$: Hierarchical action space detailed in Section 4.1.2
Â· Transition Function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$: Unknown, learned through interaction
Â· Reward Function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$: Multi-objective reward defined in Section 4.1.3
Â· Constraint Function $\mathcal{C} = \{c_k: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\}_{k=1}^K$: K constraints for QoS, energy, safety
Â· Discount Factor $\gamma \in [0,1)$: Balances immediate vs. future rewards

4.1.2. Hierarchical Action Space Design

The action space is decomposed into two hierarchical levels:

Top-Level Actions (Discrete):
\mathcal{A}^{high} = \{a_{high}^{(1)}, a_{high}^{(2)}, \dots, a_{high}^{(M)}\}


where each high-level action is a tuple:
a_{high} = \langle n_{next}, m_{jam}, d_{EH} \rangle

Â· n_next âˆˆ {Remain, Switch to GN, Switch to AN, Switch to SN}: Network selection
Â· m_jam âˆˆ {Avoid, Counter, Harvest}: Jamming response mode
Â· d_EH âˆˆ {On, Off}: Energy harvesting decision

Low-Level Actions (Continuous):
\mathcal{A}^{low} = \mathbb{R}^4


representing:
a_{low} = [Î”v_x, Î”v_y, Î”v_z, Î”Ïˆ]^T


velocity changes in body frame and yaw adjustment.

The temporal abstraction follows: high-level actions are taken every T_high time steps (typically 1-5 seconds), while low-level actions are taken every T_low steps (typically 0.1-0.5 seconds), with T_high = N Ã— T_low.

4.1.3. Multi-Objective Reward Function Design

The reward function balances multiple competing objectives through weighted combination:

r(s,a) = \sum_{i=1}^{6} w_i r_i(s,a)

Primary Objectives:

1. Communication Quality Reward:
   r_1(s,a) = \tanh\left(\frac{\mathcal{R}_{curr} - \mathcal{R}_{req}}{\mathcal{R}_{req}}\right)
2. Energy Efficiency Reward:
   r_2(s,a) = \frac{P_{EH} - Î±_{cons} P_{cons}}{P_{norm}}
   
   where Î±_cons weights consumption relative to harvesting.
3. Jamming Avoidance Reward:
   r_3(s,a) = -\omega_j â‹… I_{norm}

Secondary Objectives:

1. Mission Progress Reward:
   r_4(s,a) = \frac{\|q_{curr} - q_{goal}\|_{t-1} - \|q_{curr} - q_{goal}\|_t}{d_{max}}
2. Stability Reward:
   r_5(s,a) = -Î²_{switch} â‹… \mathbb{1}_{switch} - Î²_{handoff} â‹… C_{switch}
3. Safety Reward:
   r_6(s,a) = - \sum_{i=1}^{N_{obs}} \exp(-d_i/d_0)

The weights w_i are dynamically adjusted using a multi-objective optimization technique based on the current mission phase and constraints.

4.1.4. Constraint Formulation

The CMDP includes the following constraints:

Quality of Service Constraint:
c_1(s,a) = \max(0, \mathcal{R}_{req} - \mathcal{R}_{curr})

Energy Sustainability Constraint:
c_2(s,a) = \max(0, E_{min} - SOC)

Interference Tolerance Constraint:
c_3(s,a) = \max(0, I_{eff} - I_{max})

Safety Constraint:
c_4(s,a) = \max(0, d_{min} - \min_i d_i)

The constrained optimization problem is:
\max_Ï€ \mathbb{E}_{Ï„âˆ¼Ï€} \left[ \sum_{t=0}^{T} Î³^t r(s_t, a_t) \right]


subject to:\mathbb{E}_{Ï„âˆ¼Ï€} \left[ \sum_{t=0}^{T} Î³^t c_k(s_t, a_t) \right] â‰¤ d_k, âˆ€k

4.2. Top-Level Agent: Link Selection and Anti-Jamming

4.2.1. Double Deep Q-Network Architecture

The top-level agent employs a Double DQN (DDQN) to address value overestimation:

Network Architecture:

```
Input: s_t (35-dim state vector)
     â†“
Fully Connected: 256 neurons, ReLU
     â†“
Fully Connected: 256 neurons, ReLU
     â†“
Fully Connected: 128 neurons, ReLU
     â†“
Output: Q(s_t, a) for each a âˆˆ A_high (12 actions)
```

DDQN Update Rule:
Î¸_{Q} â† Î¸_{Q} - Î± âˆ‡_{Î¸_{Q}} \left( y_t - Q(s_t, a_t; Î¸_{Q}) \right)^2


where target y_t is:
y_t = r_t + Î³ Q(s_{t+1}, \arg\max_a Q(s_{t+1}, a; Î¸_{Q}); Î¸_{Q}^{-})

Experience Replay: Uses prioritized experience replay (PER) with importance sampling to focus on critical transitions, particularly those involving jamming events or network failures.

4.2.2. Interference-Aware Upper Confidence Bound Exploration

To address the exploration-exploitation dilemma in jamming environments, we extend the UCB strategy:

Standard UCB: 
a_t = \arg\max_a \left[ Q(s_t, a) + c \sqrt{\frac{\ln N(s_t)}{N(s_t, a)}} \right]

Interference-Aware UCB (UCB-IA):
a_t = \arg\max_a \left[ \bar{Q}(s_t, a) + c' \sqrt{\frac{\ln N(s_t)}{N(s_t, a)}} \right]

where the adjusted Q-value is:
\bar{Q}(s_t, a) = Q(s_t, a) + Î»_{jam} \cdot \hat{r}_{jam}(a) + Î»_{EH} \cdot \hat{r}_{EH}(a)

with:

Â· $\hat{r}_{jam}(a)$: Empirical interference rate for action a
Â· $\hat{r}_{EH}(a)$: Empirical energy harvesting rate for action a
Â· $Î»_{jam}, Î»_{EH}$: Adaptive weights based on current jamming intensity and energy level

The exploration parameter c' is dynamically adjusted:
c' = c_0 \cdot (1 + \sigma_{jam} - \frac{SOC}{SOC_{max}})


encouraging more exploration when jamming is variable or energy is low.

4.2.3. Jamming Pattern Learning and Prediction

The top-level agent maintains a jamming pattern memory using a Transformer-based sequence model:

J_{pattern} = \text{Transformer}([Ï‰_j(t-T), \dots, Ï‰_j(t-1)])

This pattern memory is used to:

1. Predict future jamming occurrences: $\hat{Ï‰}_j(t+Î´t)$
2. Classify jammer type for appropriate countermeasure selection
3. Identify periodicities for proactive avoidance

The predicted jamming state is incorporated into the Q-value calculation:
Q_{jam-aware}(s,a) = Q(s,a) - Î²_{jam} \cdot P(Ï‰_j=1|s,a) \cdot \hat{I}_{eff}

4.2.4. Network Selection with Switching Cost Consideration

The network selection decision incorporates switching costs through a hysteresis mechanism:

Q_{switch}(s,a) = Q(s,a) - Î·_{switch} \cdot \frac{C_{switch}(n_{curr}â†’n_{next})}{C_{max}} \cdot \mathbb{1}_{n_{next} â‰  n_{curr}}

This formulation discourages unnecessary handoffs while still allowing them when significantly beneficial.

4.3. Lower-Level Agent: Trajectory Optimization and Energy Management

4.3.1. Constrained Soft Actor-Critic Architecture

The lower-level agent uses Constrained Soft Actor-Critic (CSAC) for continuous control with constraints:

Actor Network (Policy Ï€_Ï•):

```
Input: s_t (35-dim) concatenated with high-level action a_high (one-hot)
     â†“
Fully Connected: 256 neurons, ReLU
     â†“
Fully Connected: 256 neurons, ReLU
     â†“
Fully Connected: 128 neurons, ReLU
     â†“
Output: Î¼ (4-dim mean) and Ïƒ (4-dim std dev) for Gaussian policy
```

Critic Networks (Q-functions Î¸_1, Î¸_2):
Twin Q-networks for variance reduction,each with architecture:

```
Input: s_t âŠ• a_high âŠ• a_low
     â†“
Fully Connected: 256 neurons, ReLU
     â†“
Fully Connected: 256 neurons, ReLU
     â†“
Fully Connected: 128 neurons, ReLU
     â†“
Output: Q-value (scalar)
```

Update Rules:

Critic update:
J_Q(Î¸_i) = \mathbb{E}_{(s,a)âˆ¼D} \left[ \left( Q_{Î¸_i}(s,a) - \hat{Q}(s,a) \right)^2 \right]


where target$\hat{Q}(s,a) = r(s,a) + Î³ \left( \min_{i=1,2} Q_{Î¸_i^{-}}(s', \tilde{a}') - Î± \log Ï€_Ï•(\tilde{a}'|s') \right)$

Actor update:
J_Ï€(Ï•) = \mathbb{E}_{sâˆ¼D} \left[ Î± \log Ï€_Ï•(f_Ï•(Ïµ;s)|s) - \min_{i=1,2} Q_{Î¸_i}(s, f_Ï•(Ïµ;s)) \right]

4.3.2. Lagrangian Optimization for Constraint Satisfaction

Constraints are incorporated through Lagrangian multipliers:

Augmented Lagrangian:
\mathcal{L}(Ï€, Î») = \mathbb{E}_{Ï„âˆ¼Ï€} \left[ \sum_{t=0}^{T} Î³^t (r(s_t, a_t) - \sum_{k=1}^{K} Î»_k c_k(s_t, a_t)) \right] + \sum_{k=1}^{K} Î»_k d_k

Multiplier Update:
Î»_k â† \max(0, Î»_k + Î·_Î» ( \mathbb{E}_{Ï„âˆ¼Ï€}[c_k] - d_k ))

Adaptive Learning Rates: 
Constraint thresholds d_k are dynamically adjusted based on mission phase:
d_k(t) = d_k^{base} + Î”d_k \cdot f(t/T_{mission})

4.3.3. Energy-Aware Trajectory Optimization

The trajectory optimization incorporates energy considerations through:

Energy Cost Map: Maintains an estimate of location-dependent energy harvesting potential:
E_{map}(x,y,z) = \mathbb{E}[P_{EH} | location]

Energy-Optimal Path Planning: The reward includes an energy guidance term:
r_{energy}(s,a) = Î²_{EH} \cdot (E_{map}(q_{t+1}) - E_{map}(q_t))

This encourages movement toward energy-rich areas when battery is low.

Velocity-Power Relationship: The power consumption model for propulsion:
P_{prop}(v) = P_{hover} + \frac{1}{2} Ï C_D A â€–vâ€–^3


is incorporated into the constraint c_2(s,a)to ensure energy-efficient movement.

4.3.4. Hierarchical Action Execution Protocol

The interaction between levels follows:

```
Initialize: t = 0, Ï„_high = 0
While mission not complete:
    Observe state s_t
    
    // High-level decision (every T_high steps)
    If Ï„_high == 0:
        a_high = Ï€_high(s_t)  // From DDQN
        Ï„_high = T_high
    
    // Low-level execution (every T_low steps)
    a_low = Ï€_low(s_t, a_high)  // From CSAC
    Execute a_low for duration T_low
    
    // Update counters and state
    Ï„_high -= T_low
    t += 1
    Observe s_{t+1}, r_t, constraints
    
    // Store experience
    D_high â† (s_t, a_high, r_t, s_{t+1})
    D_low â† (s_tâŠ•a_high, a_low, r_t, s_{t+1})
    
    // Periodic updates
    If training phase:
        Update Ï€_high, Ï€_low from buffers
```

4.4. Hierarchical Coordination Mechanism

4.4.1. Temporal Abstraction Learning

Rather than fixed time scales, the hierarchy learns appropriate temporal abstraction:

Option Framework Integration: High-level actions are treated as options (temporally extended actions) with termination conditions learned alongside policies.

Option Duration Learning: The termination probability Î²(s) is learned as:
Î²_Ï•(s) = \text{sigmoid}(f_Ï•(s))


where f_Ï• is a neural network.

Intra-Option Learning: Q-values for options are updated using intra-option learning:
Q_Î©(s,Ï‰) = \mathbb{E} \left[ r(s,Ï‰) + Î³ \left( (1-Î²(s')) Q_Î©(s',Ï‰) + Î²(s') \max_{Ï‰'} Q_Î©(s',Ï‰') \right) \right]

4.4.2. Information Flow Between Levels

Bidirectional information flow enhances coordination:

Bottom-Up Information: Low-level agent provides:

Â· Feasibility feedback on high-level commands
Â· Updated estimates of constraint satisfaction
Â· Local environmental discoveries (new obstacles, RF sources)

Top-Down Information: High-level agent provides:

Â· Strategic direction (mission phases)
Â· Global constraint adjustments
Â· Learned patterns (jamming, energy availability)

This is implemented through an additional communication channel in the state representation.

4.5. Multi-Agent Extension for Swarm Coordination

4.5.1. Centralized Training with Decentralized Execution (CTDE)

The swarm employs CTDE to balance coordination with scalability:

Centralized Critic: During training, a centralized critic has access to all agents' observations:
Q_{tot}(s, a) = f_Ïˆ([s_1, s_2, ..., s_N], [a_1, a_2, ..., a_N])

Decentralized Actors: Each agent maintains its own actor network Ï€_i(a_i | s_i) using only local observations.

Value Decomposition: The joint Q-value is decomposed into individual contributions:
Q_{tot}(s, a) = \sum_{i=1}^{N} w_i(s) Q_i(s_i, a_i) + b(s)


where w_i(s)are mixing weights and b(s) is a bias term, both learned by the mixing network.

4.5.2. Swarm-Specific Modifications

Differentiated Roles: Agents assume specialized roles through role embeddings:

Â· Scouts: Emphasize exploration and jamming detection
Â· Relays: Focus on communication link maintenance
Â· Harvesters: Optimize for energy gathering in rich areas
Â· Searchers: Prioritize area coverage for victim detection

Roles are assigned dynamically based on current needs and agent capabilities.

Collision Avoidance: Implemented through:

1. Repulsive potential in reward: $r_{collision} = - \sum_{jâ‰ i} \exp(-â€–q_i - q_jâ€–^2 / Ïƒ^2)$
2. ORCA (Optimal Reciprocal Collision Avoidance) as a safety layer overriding DRL actions when collision imminent
3. Formation maintenance for coordinated movement

Communication-Aware Coordination: Agents share:

Â· Jamming detection information
Â· Energy source locations
Â· Network quality measurements
Â· Obstacle maps

Shared information is incorporated into individual state representations through an attention mechanism:
s_i' = s_i âŠ• \text{Attn}(s_i, \{s_j\}_{jâ‰ i})

4.5.3. Scalability Enhancements

For large swarms (N > 10):

Â· Hierarchical organization: Sub-swarms with local coordinators
Â· Communication pruning: Limit information sharing to spatially proximate agents
Â· Parameter sharing: All agents share actor parameters with agent-specific biases
Â· Curriculum learning: Start with single agent, gradually add agents during training

4.6. Training Methodology

4.6.1. Two-Phase Training Approach

Phase 1: Individual Skill Acquisition

Â· Train single agent in simplified environments
Â· Separate training for top-level and low-level initially
Â· Curriculum: Start with stationary, then mobile; no jamming, then jamming

Phase 2: Integrated and Multi-Agent Training

Â· Train hierarchical agent end-to-end
Â· Gradually increase environment complexity
Â· Add agents one by one for swarm training

4.6.2. Sim-to-Real Transfer Techniques

To bridge simulation-reality gap:

Â· Domain randomization: Vary physics parameters, sensor noise, communication models
Â· Adversarial training: Include worst-case scenarios
Â· Progressive neural networks: Retain simulation knowledge while adapting to real world
Â· Meta-learning: Learn to adapt quickly to new environments

4.6.3. Evaluation Metrics During Training

Monitor:

Â· Primary: Cumulative reward, constraint violations
Â· Communication: Throughput, latency, handoff frequency
Â· Energy: Net energy gain/loss, mission duration
Â· Security: Jamming success rate, interference tolerance
Â· Swarm: Coverage efficiency, collision rate

This comprehensive HDRL framework provides the algorithmic foundation for resilient, autonomous UAV swarms capable of operating in contested disaster environments while maintaining energy sustainability through intelligent harvesting and threat mitigation.

CHAPTER 5: ALGORITHM DEVELOPMENT AND THEORETICAL ANALYSIS

5.1. Unified HDRL Algorithm Development

5.1.1. Complete Algorithm Pseudocode

The unified HDRL algorithm integrates the components developed in Chapter 4 into a coherent learning and execution framework.

```
Algorithm 1: Unified HDRL Training Algorithm for Resilient UAV Swarms
Input: Environment E, Number of agents N, Training episodes M
Output: Trained policy networks {Ï€_high_i, Ï€_low_i} for i=1..N

Initialize:
    Global replay buffer D_global with capacity C
    Agent-specific replay buffers D_i for i=1..N
    High-level networks: Î¸_Q_i, Î¸_Q'_i (DDQN) for each agent
    Low-level networks: Ï•_Ï€_i, Î¸_Q1_i, Î¸_Q2_i, Î¸_Q1'_i, Î¸_Q2'_i (CSAC)
    Lagrangian multipliers Î»_k for k=1..4 constraints
    Learning rates Î±_high, Î±_low, Î±_Î»
    Exploration parameters Ïµ (decaying), c (UCB)

for episode = 1 to M do
    Reset environment E, get initial state s_0 for all agents
    Initialize t = 0, high-level action timer Ï„_high_i = 0 for all i
    
    while not terminal and t < T_max do
        for each agent i in parallel do
            // High-level decision (every T_high steps)
            if Ï„_high_i == 0 then
                // UCB-IA exploration
                With probability Ïµ: 
                    a_high_i = random from A_high
                else:
                    for each a in A_high do
                        N_a = count of action a in state cluster(s_t_i)
                        UCB_score(a) = Q(s_t_i, a; Î¸_Q_i) 
                                      + c * sqrt(ln(t+1)/(N_a+1))
                                      + Î»_jam * rÌ‚_jam(a) 
                                      + Î»_EH * rÌ‚_EH(a)
                    end for
                    a_high_i = argmax_a UCB_score(a)
                end if
                Ï„_high_i = T_high
            end if
            
            // Low-level decision (every T_low steps)
            a_low_i = Ï€_low_i(s_t_i, a_high_i; Ï•_Ï€_i) + ğ’©(0, Ïƒ_explore)
            
            // Execute combined action
            Execute [a_high_i, a_low_i] for duration T_low
        end for
        
        // Environment transition
        Observe next state s_{t+1}, rewards r_t, constraints c_t
        Calculate joint reward R_t = Î£_i r_t_i + Î²_coop * r_swarm
        
        // Store experiences
        for each agent i do
            Store (s_t_i, a_high_i, R_t, s_{t+1}_i) in D_i and D_global
            Store (s_t_iâŠ•a_high_i, a_low_i, r_t_i, s_{t+1}_i) in D_i
            Ï„_high_i = Ï„_high_i - T_low
        end for
        
        // Periodic training (every K steps)
        if t mod K == 0 then
            // Centralized training phase
            Sample batch B from D_global
            
            // Update high-level networks
            for each agent i do
                Compute target y_i = r_i + Î³ * Q'(s', argmax_a Q(s', a; Î¸_Q_i); Î¸_Q'_i)
                Update Î¸_Q_i: âˆ‡_Î¸ L(Î¸_Q_i) = E[(y_i - Q(s,a; Î¸_Q_i))^2]
                Update target network: Î¸_Q'_i â† Ï„ Î¸_Q_i + (1-Ï„) Î¸_Q'_i
            end for
            
            // Update low-level networks
            for each agent i do
                // Update critics
                Compute target QÌ‚ = r_i + Î³(min_j Q_j'(s', aÌƒ') - Î± log Ï€(aÌƒ'|s'))
                Update Î¸_Qj_i: âˆ‡_Î¸ L(Î¸_Qj_i) = E[(QÌ‚ - Q(s,a; Î¸_Qj_i))^2]
                
                // Update actor
                Compute policy loss: J_Ï€ = E[Î± log Ï€(a|s) - min_j Q_j(s,a)]
                Update Ï•_Ï€_i: âˆ‡_Ï• J_Ï€(Ï•_Ï€_i)
                
                // Update temperature Î±
                Update Î±: âˆ‡_Î± L(Î±) = E[-Î± log Ï€(a|s) - Î± H_target]
            end for
            
            // Update Lagrangian multipliers
            for each constraint k do
                Î»_k = max(0, Î»_k + Î±_Î» (E_B[c_k] - d_k))
            end for
        end if
        
        t = t + 1
    end while
    
    // Decay exploration parameters
    Ïµ = Ïµ * decay_rate
    Ïƒ_explore = Ïƒ_explore * decay_rate
    
end for
```

5.1.2. Distributed Training Implementation

The training algorithm supports both centralized and federated learning paradigms:

Centralized Training:

Â· All experiences aggregated in D_global
Â· Single parameter server updates all networks
Â· Suitable for pre-deployment training

Federated Learning Variant:

```
Algorithm 2: Federated HDRL Training
for each training round r = 1 to R do
    for each agent i in parallel do
        Download global model parameters Î¸_global
        Initialize Î¸_local_i = Î¸_global
        // Local training
        for e = 1 to E_local do
            Collect local experiences
            Update Î¸_local_i using Algorithm 1 (single agent)
        end for
        Upload model update Î”Î¸_i = Î¸_local_i - Î¸_global
    end for
    
    // Server aggregation
    Î¸_global = Î¸_global + Î·_agg * (1/N) Î£_i Î”Î¸_i
end for
```

5.1.3. Real-Time Inference Optimization

For deployment efficiency, the inference pipeline is optimized:

Model Compression:

Â· Knowledge distillation: Train smaller student networks from larger teacher
Â· Pruning: Remove redundant weights (magnitude-based, movement-based)
Â· Quantization: 8-bit fixed-point representation for edge deployment

Inference Pipeline:

```
Input: Current state s_t
     â†“
Feature Extractor CNN (shared between levels)
     â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ High-Level Branch   â”‚ Low-Level Branch
    â”‚ â€¢ 2 FC layers       â”‚ â€¢ 2 FC layers
    â”‚ â€¢ Softmax over A_highâ”‚ â€¢ Î¼, Ïƒ for Gaussian
    â”‚ â€¢ Argmax selection  â”‚ â€¢ Re-parameterization
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                      â†“
    a_high                a_low
     â†“                      â†“
    Action Execution â”€â”€â”€â”€â”€â”€â”˜
```

Latency Budget Allocation:

Â· Total inference time: â‰¤ 50ms per control cycle
Â· High-level network: â‰¤ 10ms
Â· Low-level network: â‰¤ 30ms
Â· Safety checks: â‰¤ 10ms

5.2. Convergence Analysis

5.2.1. DDQN Convergence with UCB-IA Exploration

Theorem 5.1 (DDQN-UCB Convergence): Under the following assumptions:

1. Finite state-action space with |S| = m, |A| = n
2. Bounded rewards: |r(s,a)| â‰¤ R_max
3. Stationary transition dynamics
4. UCB exploration with c_t = O(âˆš(ln t / N_t(a)))

The DDQN-UCB algorithm converges to the optimal Q-function Q* with probability 1, with the following regret bound after T steps:

Regret(T) = \sum_{t=1}^T (Q^*(s_t, a_t^*) - Q(s_t, a_t)) â‰¤ O\left(\sqrt{Tmn \ln T}\right)

Proof Sketch:

1. UCB Regret Bound: Standard UCB regret for multi-armed bandit: O(âˆš(T n ln T))
2. DDQN Convergence: Under standard conditions, DDQN converges to fixed point of Bellman optimality operator
3. Combined Analysis: Treat each state as independent bandit problem, apply union bound over states
4. Incorporating IA Modification: The interference-aware term adds bounded bias Î” â‰¤ Î”_max, yielding additional O(Î”_max T) term

Corollary 5.1.1 (Sample Complexity): To achieve Ïµ-optimal policy, the required number of samples is:

N_{samples}(Ïµ) = O\left(\frac{mn \ln(1/Ïµ)}{Ïµ^2}\right)

5.2.2. Constrained Soft Actor-Critic Convergence

Theorem 5.2 (CSAC Convergence): Consider the CSAC algorithm with:

1. Policy parameterized by Ï• with compact parameter space Î¦
2. Critic parameterized by Î¸ in compact space Î˜
3. Lipschitz continuous reward and constraint functions: ||r(s,a) - r(s',a')|| â‰¤ L_r(||s-s'|| + ||a-a'||)
4. Bounded constraint violations: |c_k(s,a)| â‰¤ C_max

Then the algorithm converges to a stationary point of the Lagrangian â„’(Ï€, Î»), satisfying the KKT conditions for the constrained optimization problem.

Proof Outline:

1. Critic Convergence: Following standard SAC analysis, the soft Q-learning converges to soft Q* under appropriate learning rates
2. Actor Improvement: The policy improvement step ensures monotonic improvement in expected return while satisfying constraints
3. Lagrangian Convergence: The dual ascent update converges to optimal Î»* under convexity assumptions
4. Joint Convergence: Alternate optimization converges to local optimum of constrained problem

Theorem 5.3 (Constraint Satisfaction Guarantee): After convergence, the policy Ï€* satisfies:

\mathbb{E}_{Ï„âˆ¼Ï€^*}[c_k(s,a)] â‰¤ d_k + Î´

where Î´ = O(1/âˆšN) depends on the number of training samples N.

5.2.3. Hierarchical Learning Stability

Theorem 5.4 (Hierarchical Policy Convergence): The two-time-scale hierarchical learning algorithm converges if:

1. High-level updates occur on slower time scale: Î±_high = o(Î±_low)
2. Low-level policy converges faster than high-level value function updates
3. Termination condition Î²(s) satisfies gradient dominance condition

Proof Sketch:
Using two-time-scale stochastic approximation theory:

1. Define ODE approximations for high and low levels
2. Show that fast subsystem (low-level) converges to stationary point given fixed high-level policy
3. Show slow subsystem (high-level) sees quasi-stationary low-level policy
4. Apply Kushner-Clark lemma for convergence of coupled stochastic processes

The resulting hierarchical policy Ï€_H = (Ï€_high, Ï€_low) satisfies the hierarchical Bellman equation:

Q_H(s, a_high) = \mathbb{E}\left[\sum_{t=0}^{Ï„-1} Î³^t r(s_t, a_low_t) + Î³^Ï„ \max_{a_high'} Q_H(s_Ï„, a_high') \right]

where Ï„ is the option termination time.

5.3. Complexity Analysis

5.3.1. Computational Complexity

Training Phase Complexity:

Â· Per iteration: O(B Â· (d_s + d_a) Â· L) where:
  Â· B: Batch size (typically 256-1024)
  Â· d_s: State dimension (35)
  Â· d_a: Action dimension (4 for low-level, 12 for high-level)
  Â· L: Network depth (â‰ˆ 10^6 parameters total)
Â· Total training: O(M Â· T Â· N Â· C_per_step) where:
  Â· M: Number of episodes (10^4-10^6)
  Â· T: Episode length (10^3-10^4)
  Â· N: Number of agents (1-32)
  Â· C_per_step: Complexity per step (â‰ˆ 10^3 FLOPs)

Inference Phase Complexity:

Â· Forward pass: O(d_s Â· L) â‰ˆ 10^5 FLOPs per agent per step
Â· Memory requirements: O(L Â· d_layerÂ²) â‰ˆ 10 MB per network

5.3.2. Space Complexity

Model Storage:

Â· High-level network: O(d_s Â· d_h1 + d_h1 Â· d_h2 + d_h2 Â· |A_high|) â‰ˆ 0.5 MB
Â· Low-level networks: O(2Â·(d_s'Â·d_h1 + d_h1Â·d_h2 + d_h2Â·1) + (d_s'Â·d_h1 + d_h1Â·d_h2 + d_h2Â·d_a)) â‰ˆ 2 MB
Â· Total per agent: â‰ˆ 2.5 MB
Â· Swarm of N agents: â‰ˆ 2.5N MB (shared parameters reduce to â‰ˆ 3 MB total)

Experience Replay:

Â· Each transition: O(d_s + d_a + 1 + d_s) â‰ˆ 100 bytes
Â· Buffer of size C: O(C Â· 100) bytes
Â· Typical C = 10^6: â‰ˆ 100 MB

5.3.3. Scalability Analysis

Theorem 5.5 (Swarm Scalability): For N agents with shared experience replay and parameter sharing, the training complexity scales as:

C_{train}(N) = O(N^{Î±}) \text{ where } Î± âˆˆ [0.5, 1.0]

depending on the degree of coordination required.

Proof:

1. Best case (Î± = 0.5): Independent agents with shared data augmentation
2. Worst case (Î± = 1.0): Fully centralized critic with joint action space
3. CTDE approach: Î± â‰ˆ 0.7 empirically observed

Communication Overhead:

Â· During training: O(N Â· d_s) per time step for centralized critic
Â· During execution: O(N_local Â· d_comm) where N_local is agents in communication range

5.4. Theoretical Performance Bounds

5.4.1. Regret Analysis in Non-Stationary Environments

Theorem 5.6 (Non-Stationary Regret Bound): In an environment with variation budget V_T = Î£_{t=1}^T ||P_t - P_{t+1}||_1 (measuring non-stationarity), the regret of the adaptive HDRL algorithm is bounded by:

Regret(T) â‰¤ O\left(\sqrt{T(mn \ln T + V_T)}\right)

Implications:

1. For stationary environments (V_T = 0): Standard âˆšT regret
2. For slowly varying environments (V_T = O(âˆšT)): Still sublinear regret
3. For adversarial jamming (V_T = O(T)): Linear regret unavoidable

Adaptation Mechanism: The algorithm maintains a change detection module:

D_{KL}(PÌ‚_{[t-w,t]} || PÌ‚_{[t-2w,t-w]}) > Ï„_{change}

Upon detection, the algorithm:

1. Increases exploration rate
2. Resets certain Q-values
3. Adjusts learning rates

5.4.2. Energy Harvesting Performance Bounds

Theorem 5.7 (Energy Neutrality Condition): Let E_harvest(t) be the harvested energy and E_consume(t) the consumed energy. The system achieves energy neutrality (infinite operation) if:

\lim_{Tâ†’âˆ} \frac{1}{T} \sum_{t=1}^T \mathbb{E}[E_{harvest}(t) - E_{consume}(t)] â‰¥ 0

Under the proposed algorithm, this condition is achieved when:

\frac{\mathbb{E}[P_{EH}]}{\mathbb{E}[P_{cons}]} â‰¥ \frac{1}{Î·_{charge} Î·_{discharge}}

where Î·_charge, Î·_discharge are battery efficiencies.

Proof: Using renewal-reward theorem and modeling energy arrivals as renewal process.

Corollary 5.7.1 (Mission Duration Bound): For given initial energy E_0 and average net energy gain Î”E, the expected mission duration is:

\mathbb{E}[T_{mission}] â‰¥ \frac{E_0}{|\mathbb{E}[Î”E]|}

with equality only when Î”E < 0 (energy deficit).

5.4.3. Communication Performance Guarantees

Theorem 5.8 (QoS Satisfaction): Under the CSAC algorithm with constraint weight Î»_QoS, the time-average QoS violation is bounded by:

\frac{1}{T} \sum_{t=1}^T \mathbb{1}_{\mathcal{R}(t) < \mathcal{R}_{req}} â‰¤ \frac{C}{\sqrt{T}} + \frac{d_{QoS}}{Î»_{QoS}}

where C is a problem-dependent constant.

Throughput Lower Bound: The minimum guaranteed throughput is:

\mathcal{R}_{min} = \mathcal{R}_{req} - O\left(\frac{1}{\sqrt{T}} + \frac{1}{Î»_{QoS}}\right)

5.4.4. Jamming Resistance Guarantee

Theorem 5.9 (Interference Tolerance): Let J_max be the maximum jamming power. The algorithm maintains communication if:

J_{max} < \frac{P_t G_t G_r |h|^2}{PL(d) \cdot (2^{\mathcal{R}_{req}/B} - 1)} - N_0 B

and the throughput degradation is bounded by:

\frac{\mathcal{R}_{jam}}{\mathcal{R}_{no-jam}} â‰¥ 1 - \frac{J_{eff}}{J_{max}} \cdot \frac{\ln(1 + J_{max}/N_0B)}{\ln(1 + SNR_{max})}

where J_eff is the effective jamming power after mitigation.

Adaptation Speed: The algorithm adapts to new jamming patterns within:

T_{adapt} = O\left(\frac{1}{Î±_{learn}} \ln\left(\frac{Î”_{Q}}{Ïµ}\right)\right)

where Î±_learn is the learning rate and Î”_Q is the Q-value change required.

5.5. Algorithm Extensions and Variants

5.5.1. Meta-Learning for Rapid Adaptation

Extension to meta-learning for quick adaptation to new disaster scenarios:

```
Algorithm 3: Meta-HDRL
Input: Distribution over environments p(E)
Output: Meta-initialized parameters Î¸_meta

Initialize Î¸_meta
for meta-iteration = 1 to M_meta do
    Sample batch of environments {E_i} ~ p(E)
    for each E_i do
        Î¸_i = Î¸_meta
        // Inner loop: Few-shot adaptation
        for step = 1 to K_inner do
            Collect experience in E_i
            Update Î¸_i with gradient descent
        end for
        Compute loss L_i(Î¸_i)
    end for
    // Outer loop: Meta-update
    Update Î¸_meta: âˆ‡_Î¸ Î£_i L_i(Î¸_i)
end for
```

5.5.2. Transfer Learning Across Disaster Types

Pre-training on source domains (earthquake simulations) and fine-tuning on target domains (flood scenarios) using:

Â· Domain adversarial training to learn domain-invariant features
Â· Gradient reversal layers for invariant representation learning
Â· Progressive networks to retain source knowledge

5.5.3. Safe Exploration Mechanisms

To ensure safety during learning:

Â· Constrained policy optimization with worst-case constraints
Â· Lyapunov-based safety filters overriding unsafe actions
Â· Bayesian uncertainty estimation for risk-aware exploration

Theorem 5.10 (Safety Guarantee): With probability at least 1-Î´, the algorithm satisfies all safety constraints during exploration if:

N_{explore}(s,a) â‰¥ \frac{1}{2Ïµ^2} \ln\left(\frac{2|S||A|}{Î´}\right)

for each state-action pair, where Ïµ is the required estimation accuracy.

---

CHAPTER 6: SIMULATION FRAMEWORK AND EXPERIMENTAL SETUP

6.1. Comprehensive Simulation Environment Design

6.1.1. Multi-Layer Simulation Architecture

The simulation framework integrates three specialized simulators through a co-simulation interface:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Co-Simulation Manager                    â”‚
â”‚  â€¢ Time synchronization     â€¢ Data exchange             â”‚
â”‚  â€¢ Event handling           â€¢ Consistency maintenance   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Robotics Sim     â”‚ â”‚ Network Simulator â”‚ â”‚   DRL Training    â”‚
    â”‚   (Gazebo/ROS)     â”‚ â”‚     (NS-3)        â”‚ â”‚    Framework      â”‚
    â”‚                    â”‚ â”‚                    â”‚ â”‚   (PyTorch)       â”‚
    â”‚â€¢ UAV dynamics      â”‚ â”‚â€¢ RF propagation    â”‚ â”‚â€¢ Neural networks  â”‚
    â”‚â€¢ Sensor models     â”‚ â”‚â€¢ Protocol stacks  â”‚ â”‚â€¢ Optimization     â”‚
    â”‚â€¢ Environment       â”‚ â”‚â€¢ Traffic models   â”‚ â”‚â€¢ Experience replayâ”‚
    â”‚â€¢ Physics engine    â”‚ â”‚â€¢ Jamming models   â”‚ â”‚                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Co-Simulation Interface: Uses HLA (High Level Architecture) standard for time synchronization and data distribution. Each simulator runs at its natural time scale with synchronization at decision epochs.

6.1.2. Robotics Simulation (Gazebo/ROS 2)

UAV Models:

Â· Platform: Custom quadcopter based on Intel Aero RTF with modified specs
Â· Dimensions: 550mm diagonal, 1.8kg mass
Â· Propulsion: 4 Ã— 900kV motors with 10Ã—4.5 propellers
Â· Battery: 6S LiPo 10000mAh with discharge curve modeling

Sensor Models:

1. Camera: Intel RealSense D455 model
   Â· Resolution: 1280Ã—720 @ 30fps
   Â· Field of view: 87Â°Ã—58Â°
   Â· Noise: Gaussian with Ïƒ = 0.5 pixel
   Â· Latency: 33ms
2. LiDAR: Velodyne Puck Lite
   Â· Channels: 16
   Â· Range: 100m
   Â· Angular resolution: 2Â°
   Â· Noise: Ïƒ_range = 0.03m, Ïƒ_angle = 0.1Â°
3. IMU: Bosch BMI088
   Â· Accelerometer: Â±8g, noise density 120Î¼g/âˆšHz
   Â· Gyroscope: Â±2000Â°/s, noise density 0.1Â°/s/âˆšHz

Physics Engine:

Â· Engine: ODE (Open Dynamics Engine) with 1ms time step
Â· Aerodynamics: Blade element theory for rotor modeling
Â· Wind: Dryden turbulence model with gusts
Â· Environment: Urban canyon effects with building downwash

6.1.3. Network Simulator Integration (NS-3)

NS-3 Modules and Configuration:

```
// Network topology configuration
NodeContainer uavNodes;
uavNodes.Create(N_UAVs);

NodeContainer groundStations;
groundStations.Create(N_GS);

NodeContainer aerialStations;  // UAV-BSs
aerialStations.Create(N_AS);

NodeContainer satellites;
satellites.Create(N_SAT);

// Channel models
Ptr<ThreeGppChannelModel> groundChannel = 
    CreateObject<ThreeGppChannelModel>();
groundChannel->SetAttribute("Scenario", StringValue("UMa"));

Ptr<SatelliteChannelModel> satelliteChannel =
    CreateObject<SatelliteChannelModel>();
satelliteChannel->SetAttribute("Frequency", DoubleValue(20e9)); // Ku-band

// Jamming models
Ptr<JammingHelper> jammerHelper = CreateObject<JammingHelper>();
jammerHelper->SetType("Type_III_Reactive");  // Reactive jammer
jammerHelper->SetPower(43.0);  // 43 dBm ERP
jammerHelper->SetBandwidth(20e6);  // 20 MHz
```

Channel Model Parameters:

Â· Path loss: Combined free-space + urban canyon model
Â· Shadowing: Log-normal with Ïƒ = 8dB (urban), 3dB (satellite)
Â· Multipath: Rayleigh fading for NLOS, Rician with K=10 for LOS
Â· Doppler: Maximum 100Hz for UAV mobility at 20m/s

Traffic Models:

Â· Control traffic: 64 byte packets @ 10Hz (512 bps per UAV)
Â· Video streaming: H.264 encoded, 2Mbps with burstiness factor 1.5
Â· Sensor data: 100 byte packets @ 5Hz (4 kbps)
Â· Emergency alerts: 256 byte packets, Poisson arrival Î» = 0.1/s

6.1.4. DRL Training Framework (PyTorch)

Network Architectures Implementation:

```python
class HierarchicalAgent(nn.Module):
    def __init__(self, state_dim, high_action_dim, low_action_dim):
        super().__init__()
        # Shared feature extractor
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        
        # High-level branch (DDQN)
        self.high_q1 = nn.Linear(128, high_action_dim)
        self.high_q2 = nn.Linear(128, high_action_dim)
        
        # Low-level branch (SAC)
        self.low_actor_mean = nn.Linear(128 + high_action_dim, low_action_dim)
        self.low_actor_logstd = nn.Linear(128 + high_action_dim, low_action_dim)
        
        self.low_critic1 = nn.Sequential(
            nn.Linear(128 + high_action_dim + low_action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
        
        self.low_critic2 = nn.Sequential(
            nn.Linear(128 + high_action_dim + low_action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
    
    def forward(self, state, high_action_onehot=None):
        features = self.feature_extractor(state)
        
        # High-level Q-values
        q1_high = self.high_q1(features)
        q2_high = self.high_q2(features)
        q_high = (q1_high + q2_high) / 2
        
        # Low-level policy
        if high_action_onehot is not None:
            policy_input = torch.cat([features, high_action_onehot], dim=-1)
            mean = self.low_actor_mean(policy_input)
            log_std = self.low_actor_logstd(policy_input)
            std = log_std.exp()
            
            # Reparameterization trick
            normal = torch.distributions.Normal(mean, std)
            action_sample = normal.rsample()
            action = torch.tanh(action_sample)
            
            # Log probability
            log_prob = normal.log_prob(action_sample)
            log_prob -= torch.log(1 - action.pow(2) + 1e-6)
            log_prob = log_prob.sum(-1, keepdim=True)
            
            return q_high, action, log_prob
            
        return q_high
```

Training Infrastructure:

Â· Hardware: 4 Ã— NVIDIA A100 80GB GPUs
Â· Parallelization: 32 environment instances running in parallel
Â· Checkpointing: Every 100 episodes, with best model retention
Â· Monitoring: TensorBoard for real-time metrics visualization

6.2. Disaster Scenario Generation

6.2.1. Urban Environment Models

City Layout Generation:

Â· Method: Procedural generation based on real city GIS data
Â· Parameters: Building density, height distribution, street width
Â· Damage modeling: Physics-based collapse simulation for different disaster types

Disaster-Specific Modifications:

1. Earthquake Scenarios:
   ```python
   class EarthquakeScenario:
       def __init__(self, magnitude, epicenter, depth):
           self.magnitude = magnitude  # 6.0-8.0 Richter
           self.epicenter = epicenter  # (x,y) coordinates
           self.depth = depth  # km
           
       def apply_damage(self, building):
           # Calculate PGA (Peak Ground Acceleration)
           distance = np.linalg.norm(building.center - self.epicenter)
           pga = 10**(0.5*self.magnitude - np.log10(distance+1) - 2)
           
           # Building collapse probability
           collapse_prob = 1 / (1 + np.exp(-10*(pga - building.resistance)))
           
           # Apply damage if collapsed
           if np.random.random() < collapse_prob:
               building.collapse_type = self._get_collapse_type(building)
               building.rubble_height = building.height * np.random.uniform(0.3, 0.8)
   ```
2. Flood Scenarios:
   Â· Water level modeling with hydrodynamic equations
   Â· Debris flow simulation
   Â· Communication tower submersion effects
3. Wildfire Scenarios:
   Â· Fire propagation model (Rothermel model)
   Â· Smoke attenuation for communication signals
   Â· Thermal updrafts affecting UAV stability

6.2.2. Infrastructure Damage Patterns

Communication Infrastructure Damage:

Â· Base stations: 30-70% failure rate depending on disaster severity
Â· Power grid: Cascading failures modeled
Â· Backhaul: Fiber cuts at 2-5 random locations

Transportation Infrastructure:

Â· Roads: 20-50% blocked by debris
Â· Bridges: 10-30% collapsed
Â· Airports: Runway damage modeled

6.2.3. Jamming Attack Scenarios

Jammer Deployment:

Â· Number: 1-5 jammers per kmÂ² in contested zones
Â· Types: Mix of Types I-IV from Section 3.3.1
Â· Mobility: 30% of jammers are mobile (vehicle-mounted)

Attack Patterns:

1. Sweep jammers: 40% of total, sweeping 20-100 MHz bands
2. Barrage jammers: 30% of total, fixed frequency high-power
3. Reactive jammers: 20% of total, reaction time 50-200ms
4. Smart jammers: 10% of total, using DRL to learn evasion patterns

Jammer Coordination:

Â· Independent operation (70% probability)
Â· Coordinated attacks (30% probability) with frequency/time coordination

6.3. Baseline Implementations

6.3.1. Implementation of Abdolkhani et al. (2025) [Paper 1]

Core Algorithm Adaptation:

```python
class Paper1Baseline:
    def __init__(self, state_dim, action_dim):
        # DDQN with UCB-IA
        self.q_network = DDQN(state_dim, action_dim)
        self.ucb_ia = UCB_IA(exploration_weight=2.0)
        
    def select_action(self, state, battery_level, jammer_status):
        # Original algorithm logic
        if jammer_status == 1 and battery_level < 0.3:
            action = 2  # Harvest energy
        else:
            # UCB-IA action selection
            q_values = self.q_network(state)
            action = self.ucb_ia.select(q_values, state)
        return action
```

Modifications for UAV Context:

Â· Added mobility considerations to state representation
Â· Extended action space to include trajectory adjustments
Â· Added handoff costs to reward function

6.3.2. Implementation of Wan et al. (2025) [Paper 2]

Hierarchical DRL Implementation:

```python
class Paper2Baseline:
    def __init__(self):
        # Top level: DDQN for link selection
        self.top_agent = DDQN(state_dim_top, action_dim_top)
        
        # Low level: CSAC for trajectory optimization
        self.low_agent = CSAC(state_dim_low, action_dim_low)
        
    def hierarchical_decision(self, state):
        # Link selection
        link_action = self.top_agent.select_action(state)
        
        # Trajectory optimization given link
        if link_action == 0:  # Remain
            traj_action = self.low_agent.select_action(state, link_action)
        else:  # Switch
            # Additional constraints during handoff
            traj_action = self.low_agent.select_action(
                state, link_action, handoff_constraint=True
            )
        return link_action, traj_action
```

Enhancements for Fair Comparison:

Â· Integrated same SAGIN models
Â· Used identical state/action spaces where possible
Â· Matched network architectures and training hyperparameters

6.3.3. Traditional Approaches

1. Q-Learning Baseline:
   Â· Tabular Q-learning with state discretization
   Â· Ïµ-greedy exploration (Ïµ = 0.1 decaying to 0.01)
   Â· Learning rate Î± = 0.1, discount Î³ = 0.99
2. Heuristic Methods:
   ```python
   class HeuristicController:
       def decide_action(self, state):
           # Rule-based decision making
           if state.battery < 0.2:
               # Low battery: find charging/harvesting
               if state.jammer_active and state.jammer_power > threshold:
                   return "harvest_from_jammer"
               else:
                   return "return_to_base"
           elif state.snr < snr_threshold:
               # Poor connection: handoff
               best_network = self.find_best_network(state)
               return f"switch_to_{best_network}"
           else:
               # Normal operation: follow trajectory
               return "continue_path"
   ```
3. Optimization-Based Baseline:
   Â· Model Predictive Control (MPC) with 5-step horizon
   Â· Mixed Integer Linear Programming formulation
   Â· Solved using Gurobi optimizer with time limit

6.4. Evaluation Metrics Definition

6.4.1. Primary Performance Metrics

Communication Performance:

1. Throughput (Mbps): 
   \mathcal{R}_{avg} = \frac{1}{T} \sum_{t=1}^T \mathcal{R}(t)
2. QoS Satisfaction Rate (%):
   QoS_{sat} = \frac{1}{T} \sum_{t=1}^T \mathbb{1}_{\mathcal{R}(t) â‰¥ \mathcal{R}_{req}} Ã— 100\%
3. Handoff Efficiency:
   Î·_{handoff} = \frac{\text{Beneficial handoffs}}{\text{Total handoffs}}
   
   where beneficial = throughput increase > 20%
4. Link Stability:
   S_{link} = \frac{\text{Mean connection duration}}{\text{Total mission time}}

Energy Performance:

1. Energy Neutrality Index:
   ENI = \frac{\sum E_{harvest}}{\sum E_{consume}}
   Â· ENI > 1: Net energy gain
   Â· ENI = 1: Energy neutral
   Â· ENI < 1: Energy deficit
2. Mission Duration Extension (%):
   Î”T = \frac{T_{with\_EH} - T_{without\_EH}}{T_{without\_EH}} Ã— 100\%
3. Harvesting Efficiency:
   Î·_{harvest} = \frac{P_{DC}}{P_{RF}} Ã— 100\%

Security Performance:

1. Jamming Success Rate (%): 
   JSR = \frac{\text{Time jammed}}{\text{Total time}} Ã— 100\%
   
   (Lower is better)
2. Interference Margin (dB):
   IM = \min_t \left( \frac{SNR(t)}{SNR_{min}} \right)
3. Adaptation Time (s): Time to recover from jamming attack

6.4.2. Secondary Evaluation Metrics

Learning Performance:

1. Convergence Speed: Episodes to reach 90% of final performance
2. Sample Efficiency: Performance vs. number of training samples
3. Generalization: Performance on unseen disaster scenarios

Swarm Performance:

1. Coverage Efficiency:
   Î·_{coverage} = \frac{\text{Area covered}}{\text{Total search area}} Ã— \frac{1}{N_{UAV}}
2. Collision Rate: Collisions per UAV-hour
3. Coordination Overhead: Communication bandwidth for coordination

Computational Performance:

1. Inference Latency (ms): Decision time per step
2. Memory Usage (MB): Model size + working memory
3. Training Time (hours): Time to convergence

6.4.3. Statistical Analysis Methods

Performance Comparison:

Â· Paired t-tests: Compare means between algorithms
Â· ANOVA: Multiple algorithm comparison
Â· Confidence intervals: 95% CI reported for all metrics
Â· Effect sizes: Cohen's d for practical significance

Robustness Evaluation:

Â· Monte Carlo simulations: 100 runs with different random seeds
Â· Sensitivity analysis: Vary key parameters Â±20%
Â· Stress testing: Extreme scenarios (90% infrastructure damage)

Visualization Methods:

1. Learning curves: Performance vs. training episodes
2. Heat maps: Spatial distribution of metrics
3. Time series: Metric evolution during mission
4. Radar charts: Multi-metric comparison

6.4.4. Scenario-Specific Evaluation

Earthquake Response Evaluation:

Â· Victim detection rate vs. time
Â· Map completeness after 1 hour
Â· Communication restoration timeline

Wildfire Monitoring Evaluation:

Â· Fire front tracking accuracy
Â· Evacuation route identification time
Â· Thermal hotspot detection rate

Flood Assessment Evaluation:

Â· Water level estimation accuracy
Â· Structural integrity assessment rate
Â· Safe zone identification speed

6.5. Experimental Protocol

6.5.1. Training Protocol

1. Pre-training: 10,000 episodes in generic environments
2. Specialization: 5,000 episodes per disaster type
3. Fine-tuning: 1,000 episodes on specific test scenarios
4. Validation: 100 episodes after each 1,000 training episodes

6.5.2. Testing Protocol

1. Warm-up: 100 steps of environment interaction before metric collection
2. Evaluation episodes: 100 episodes per test condition
3. Random seeds: 10 different seeds for statistical significance
4. Reporting: Mean Â± standard deviation for all metrics

6.5.3. Hyperparameter Settings

Parameter Value Description
High-level  
Learning rate 1e-4 Adam optimizer
Batch size 256 Experience replay
Î³ (discount) 0.99 Future reward discount
Ï„ (target update) 0.005 Soft update rate
Low-level  
Learning rate 3e-4 Actor and critic
Î± (temperature) 0.2 Initial entropy weight
Replay size 1e6 Experience buffer
Training  
Episodes 20,000 Total training
Steps per episode 2000 Maximum steps
Warm-up steps 10000 Random actions
UCB-IA  
c (exploration) 2.0 Initial exploration weight
Decay rate 0.9995 Per episode decay
Î»_jam 1.0 Jamming awareness weight
Î»_EH 0.5 Energy harvesting weight

6.6. Implementation Details and Code Availability

6.6.1. Software Stack

Â· OS: Ubuntu 20.04 LTS
Â· ROS: ROS 2 Foxy Fitzroy
Â· Simulators: Gazebo 11, NS-3.35
Â· ML Framework: PyTorch 1.12.1
Â· Language: Python 3.8, C++ 17

6.6.2. Hardware Requirements

Â· Training: 4 Ã— NVIDIA A100 80GB, 256GB RAM, 32-core CPU
Â· Testing: NVIDIA RTX 3090, 64GB RAM, 16-core CPU
Â· Deployment: NVIDIA Jetson AGX Orin, 8GB RAM

6.6.3. Reproducibility Measures

1. Random seeds: Fixed seeds for environment generation
2. Configuration files: YAML-based parameter specification
3. Docker containers: Pre-built images with all dependencies
4. Benchmark datasets: Standardized disaster scenarios

6.6.4. Code Structure

```
project_root/
â”œâ”€â”€ simulators/
â”‚   â”œâ”€â”€ gazebo/          # Robotics simulation
â”‚   â”œâ”€â”€ ns3/            # Network simulation
â”‚   â””â”€â”€ co_sim/         # Co-simulation interface
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ hdrl/           # Proposed HDRL algorithm
â”‚   â”œâ”€â”€ baselines/      # Baseline implementations
â”‚   â””â”€â”€ utils/          # Common utilities
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ scenarios/      # Disaster scenarios
â”‚   â”œâ”€â”€ configs/        # Environment configurations
â”‚   â””â”€â”€ generators/     # Scenario generators
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics/        # Evaluation metrics
â”‚   â”œâ”€â”€ visualization/  # Plotting and visualization
â”‚   â””â”€â”€ results/        # Experiment results
â””â”€â”€ docs/
    â”œâ”€â”€ api/            # Code documentation
    â””â”€â”€ tutorials/      # Usage tutorials
```

The simulation framework provides a comprehensive, reproducible experimental setup for evaluating the proposed system against state-of-the-art baselines across diverse disaster scenarios with rigorous statistical analysis.

