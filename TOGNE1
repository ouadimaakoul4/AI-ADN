Chapter 17: Multivariate Extremes and Copula Methods

17.1 Introduction to Multivariate Extreme Value Theory

17.1.1 The Challenge of Multivariate Extremes

In global systems, extremes rarely occur in isolation. A heatwave may coincide with drought and economic stress, creating compound extremes with amplified impacts. Univariate extreme value theory (Chapter 15) fails to capture these dependencies. Let \mathbf{X} = (X_1, \ldots, X_d) be a d-dimensional random vector with joint distribution F. We need to characterize:

1. Simultaneous extremes: \mathbb{P}(X_1 > x_1, \ldots, X_d > x_d) for large x_i
2. Extremal dependence: How extremes in one variable relate to extremes in others
3. Tail dependence coefficients: Measures of joint tail behavior

17.1.2 Different Types of Extremal Dependence

Definition 17.1.1 (Tail Dependence). For bivariate (X_1, X_2) with marginal distributions F_1, F_2:

1. Upper tail dependence coefficient:
   \lambda_U = \lim_{u \to 1} \mathbb{P}(F_1(X_1) > u \mid F_2(X_2) > u)
2. Lower tail dependence coefficient:
   \lambda_L = \lim_{u \to 0} \mathbb{P}(F_1(X_1) \leq u \mid F_2(X_2) \leq u)

Interpretation:

· \lambda = 0: Asymptotic independence - extremes occur independently in the limit
· \lambda > 0: Asymptotic dependence - extremes tend to occur together
· \lambda = 1: Complete dependence - extremes always occur together

17.2 Multivariate Extreme Value Distributions

17.2.1 Componentwise Maxima Approach

Let \mathbf{X}_1, \ldots, \mathbf{X}_n be i.i.d. d-dimensional vectors. Define componentwise maxima:

\mathbf{M}_n = \left(\max_{i=1}^n X_{i1}, \ldots, \max_{i=1}^n X_{id}\right)

Theorem 17.2.1 (Multivariate Fisher-Tippett-Gnedenko). If there exist sequences \mathbf{a}_n > 0 and \mathbf{b}_n such that:

\lim_{n \to \infty} \mathbb{P}\left(\frac{\mathbf{M}_n - \mathbf{b}_n}{\mathbf{a}_n} \leq \mathbf{x}\right) = G(\mathbf{x})

and G is non-degenerate, then G is a multivariate extreme value distribution (MEVD).

17.2.2 Spectral Representation

Theorem 17.2.2 (de Haan-Resnick). Any MEVD G with unit Fréchet margins (G_j(x) = \exp(-1/x) for x > 0) can be represented as:

G(\mathbf{x}) = \exp\left[-V(\mathbf{x})\right], \quad x_j > 0, \ j = 1,\ldots,d

where the exponent measure V is homogeneous of order -1:

V(t\mathbf{x}) = t^{-1} V(\mathbf{x}), \quad t > 0

and has the spectral representation:

V(\mathbf{x}) = d \int_{S_d} \max_{j=1}^d \left(\frac{w_j}{x_j}\right) H(d\mathbf{w})

Here S_d = \{\mathbf{w} \in \mathbb{R}_+^d : \|\mathbf{w}\|_1 = 1\} is the unit simplex, and H is a spectral measure satisfying:

\int_{S_d} w_j H(d\mathbf{w}) = 1, \quad j = 1,\ldots,d

17.2.3 Pickands Dependence Function (Bivariate Case)

For d = 2, the representation simplifies. Let (X_1, X_2) have unit Fréchet margins. Then:

\mathbb{P}(X_1 \leq x_1, X_2 \leq x_2) = \exp\left[-\left(\frac{1}{x_1} + \frac{1}{x_2}\right) A\left(\frac{x_2}{x_1 + x_2}\right)\right]

where A: [0,1] \to [1/2, 1] is the Pickands dependence function, convex with A(0) = A(1) = 1. The boundary cases:

· A(w) = 1: Complete dependence
· A(w) = \max(w, 1-w): Independence

The upper tail dependence coefficient relates to A:

\lambda_U = 2 - 2A(1/2)

17.3 Copula Theory for Extremes

17.3.1 Sklar's Theorem and Extreme Value Copulas

Theorem 17.3.1 (Sklar). For any multivariate distribution F with margins F_1, \ldots, F_d, there exists a copula C: [0,1]^d \to [0,1] such that:

F(x_1, \ldots, x_d) = C(F_1(x_1), \ldots, F_d(x_d))

If F is continuous, C is unique.

Definition 17.3.2 (Extreme Value Copula). A copula C is an extreme value copula if:

C(u_1^t, \ldots, u_d^t) = C^t(u_1, \ldots, u_d) \quad \forall t > 0, \ \forall \mathbf{u} \in [0,1]^d

Theorem 17.3.3. If C is an extreme value copula, then there exists a dependence function A: \Delta^{d-1} \to [1/d, 1] such that:

C(\mathbf{u}) = \exp\left[\left(\sum_{j=1}^d \log u_j\right) A\left(\frac{\log u_2}{\sum_{j=1}^d \log u_j}, \ldots, \frac{\log u_d}{\sum_{j=1}^d \log u_j}\right)\right]

17.3.2 Common Parametric Families

17.3.2.1 Gumbel-Hougaard Copula (Logistic Model)

For d = 2:

C_\theta(u, v) = \exp\left[-\left((-\log u)^\theta + (-\log v)^\theta\right)^{1/\theta}\right], \quad \theta \in [1, \infty)

· \theta = 1: Independence (C(u,v) = uv)
· \theta \to \infty: Complete dependence (C(u,v) = \min(u,v))
· Pickands function: A_\theta(w) = [w^\theta + (1-w)^\theta]^{1/\theta}
· Tail dependence: \lambda_U = 2 - 2^{1/\theta}

17.3.2.2 Husler-Reiss Copula

For d = 2:

C_\lambda(u, v) = \exp\left[-\bar{u} \Phi\left(\lambda + \frac{1}{2\lambda} \log\frac{\bar{u}}{\bar{v}}\right) - \bar{v} \Phi\left(\lambda + \frac{1}{2\lambda} \log\frac{\bar{v}}{\bar{u}}\right)\right]

where \bar{u} = -\log u, \bar{v} = -\log v, \Phi is standard normal CDF, \lambda \in [0, \infty).

· \lambda = 0: Complete dependence
· \lambda \to \infty: Independence
· Tail dependence: \lambda_U = 2 - 2\Phi(\lambda)

17.3.2.3 t-EV Copula

Based on multivariate t-distribution:

C_{\nu,R}(\mathbf{u}) = t_{\nu,R}\left(t_\nu^{-1}(u_1), \ldots, t_\nu^{-1}(u_d)\right)

where t_{\nu,R} is multivariate t CDF with \nu degrees of freedom and correlation matrix R. This is not an extreme value copula but useful for modeling tail dependence.

17.3.3 Hierarchical Archimedean Copulas

For high dimensions, use nested structures:

C(\mathbf{u}) = \phi_1^{-1}\left(\sum_{i=1}^{k_1} \phi_1 \circ \phi_{2,i}^{-1}\left(\sum_{j \in \text{group}_i} \phi_{2,i}(u_j)\right)\right)

where \phi are generator functions of Archimedean copulas.

17.4 Estimation Methods

17.4.1 Nonparametric Estimation of Dependence Function

17.4.1.1 Pickands Estimator (Bivariate)

Given i.i.d. samples (X_{i1}, X_{i2}), i = 1,\ldots,n, transform to unit Fréchet via:

Y_{ij} = -\frac{1}{\log \hat{F}_j(X_{ij})}, \quad j = 1,2

where \hat{F}_j is empirical CDF. Then for w \in (0,1):

\hat{A}_n(w) = \frac{n}{\sum_{i=1}^n \min\left(\frac{Y_{i1}}{w}, \frac{Y_{i2}}{1-w}\right)}

Theorem 17.4.1. Under regularity conditions, \sqrt{n}(\hat{A}_n - A) \Rightarrow \mathbb{G} where \mathbb{G} is a Gaussian process.

17.4.1.2 Capéraà-Fougères-Genest Estimator

Improved version using logarithms:

\hat{A}_n^{\text{CFG}}(w) = \exp\left[-\frac{1}{n} \sum_{i=1}^n \log \max\left(w Y_{i1}, (1-w) Y_{i2}\right)\right]

This estimator is asymptotically normal and more efficient than Pickands estimator.

17.4.2 Maximum Likelihood for Parametric Models

For parametric family C_\theta, transform observations to copula scale:

U_{ij} = \hat{F}_j(X_{ij}), \quad i = 1,\ldots,n, \ j = 1,\ldots,d

The log-likelihood for \theta:

\ell(\theta) = \sum_{i=1}^n \log c_\theta(U_{i1}, \ldots, U_{id})

where c_\theta = \frac{\partial^d C_\theta}{\partial u_1 \cdots \partial u_d} is the copula density.

Challenges:

1. Copula density may be complicated
2. High-dimensional integration for CDF
3. Parameter constraints

17.4.3 Composite Likelihood Methods

For high dimensions, use pairwise likelihood:

\ell_P(\theta) = \sum_{i=1}^n \sum_{j<k} \log c_{\theta}^{(jk)}(U_{ij}, U_{ik})

where c_{\theta}^{(jk)} is bivariate marginal density of C_\theta.

Theorem 17.4.2. Under regularity conditions, the maximum composite likelihood estimator \hat{\theta}_P satisfies:

\sqrt{n}(\hat{\theta}_P - \theta_0) \xrightarrow{d} N(0, G^{-1}(\theta_0))

where G(\theta) = H(\theta)J^{-1}(\theta)H(\theta) with:

· H(\theta) = \mathbb{E}[-\nabla^2 \log c_{\theta}^{(jk)}(U_j, U_k)]
· J(\theta) = \text{Cov}[\nabla \log c_{\theta}^{(jk)}(U_j, U_k)]

17.4.4 Bayesian Methods

Specify prior p(\theta) and compute posterior:

p(\theta \mid \mathbf{X}) \propto p(\theta) \prod_{i=1}^n c_\theta(U_{i1}, \ldots, U_{id})

Use Markov Chain Monte Carlo (MCMC) for computation. Advantages:

· Naturally handles parameter constraints
· Provides full uncertainty quantification
· Can incorporate expert knowledge through priors

17.5 Regular Variation and Multivariate Heavy Tails

17.5.1 Multivariate Regular Variation

Definition 17.5.1. A random vector \mathbf{X} \in \mathbb{R}^d_+ is multivariate regularly varying with index \alpha > 0 and spectral measure H on \mathbb{S}^{d-1}_+ if:

\frac{\mathbb{P}(\|\mathbf{X}\| > tx, \ \mathbf{X}/\|\mathbf{X}\| \in \cdot)}{\mathbb{P}(\|\mathbf{X}\| > t)} \xrightarrow{v} x^{-\alpha} H(\cdot) \quad \text{as } t \to \infty

where \xrightarrow{v} denotes vague convergence, \|\cdot\| is a norm, and \mathbb{S}^{d-1}_+ = \{\mathbf{x} \in \mathbb{R}^d_+ : \|\mathbf{x}\| = 1\}.

Theorem 17.5.2. If \mathbf{X} is multivariate regularly varying, then for any Borel set A \subset \mathbb{S}^{d-1}_+:

\mathbb{P}(\|\mathbf{X}\| > t, \ \mathbf{X}/\|\mathbf{X}\| \in A) \sim t^{-\alpha} L(t) H(A)

where L is slowly varying.

17.5.2 Estimation of the Spectral Measure

Empirical spectral measure: For threshold u, define:

\hat{H}_u(A) = \frac{\sum_{i=1}^n \mathbb{I}(\|\mathbf{X}_i\| > u, \ \mathbf{X}_i/\|\mathbf{X}_i\| \in A)}{\sum_{i=1}^n \mathbb{I}(\|\mathbf{X}_i\| > u)}

Smooth estimators: Use kernel smoothing on the sphere:

\hat{H}_u^{(\kappa)}(d\mathbf{w}) = \frac{\sum_{i=1}^n K_\kappa(\mathbf{w}, \mathbf{X}_i/\|\mathbf{X}_i\|) \mathbb{I}(\|\mathbf{X}_i\| > u)}{\sum_{i=1}^n \mathbb{I}(\|\mathbf{X}_i\| > u)}

where K_\kappa is a kernel on \mathbb{S}^{d-1} with bandwidth \kappa.

17.6 Measures of Extremal Dependence

17.6.1 Coefficient of Tail Dependence

For bivariate case, define χ and \bar{\chi} coefficients:

\chi(u) = \frac{2\log\mathbb{P}(F_1(X_1) > u)}{\log\mathbb{P}(F_1(X_1) > u, F_2(X_2) > u)} - 1

\bar{\chi} = \lim_{u \to 1} \chi(u)

Properties:

· \bar{\chi} = 1: Asymptotic dependence
· \bar{\chi} < 1: Asymptotic independence
· \bar{\chi} = 0: Independence in the limit

17.6.2 Extremal Coefficient

For multivariate extreme value distribution G with unit Fréchet margins, define:

\mathbb{P}(X_1 \leq x, \ldots, X_d \leq x) = \exp\left(-\frac{\theta_d}{x}\right)

where \theta_d \in [1, d] is the extremal coefficient.

· \theta_d = 1: Complete dependence
· \theta_d = d: Independence

For bivariate case with Pickands function A:

\theta_2 = 2A(1/2)

17.6.3 Tail Dependence Matrix

For d-dimensional vector, define matrix \Lambda = (\lambda_{ij}) where:

\lambda_{ij} = \lim_{u \to 1} \mathbb{P}(F_i(X_i) > u \mid F_j(X_j) > u)

This captures pairwise tail dependencies.

17.7 Modeling Asymptotic Independence

17.7.1 Ledford-Tawn Model

For asymptotically independent variables, joint tail decays faster than margins. Model:

\mathbb{P}(X_1 > x, X_2 > x) \sim \mathcal{L}(x)x^{-1/\eta} \quad \text{as } x \to \infty

where \mathcal{L} is slowly varying, and \eta \in (0, 1] is the coefficient of tail dependence:

· \eta = 1, \mathcal{L}(x) \not\to 0: Asymptotic dependence
· \eta = 1, \mathcal{L}(x) \to 0: Asymptotic independence with positive association
· \eta = 1/2: Near independence
· \eta < 1/2: Negative association

Estimation: For high threshold u, fit:

\log\mathbb{P}(X_1 > u, X_2 > u) \approx \log c - \frac{1}{\eta} \log u

via regression on exceedances.

17.7.2 Inverted Extreme Value Copulas

If C is an extreme value copula, then:

C_{\text{inv}}(u, v) = u + v - 1 + C(1-u, 1-v)

is a copula with lower tail dependence \lambda_L equal to the upper tail dependence of C, but upper tail independence.

17.8 Spatial Extremes

17.8.1 Max-Stable Processes

Definition 17.8.1. A process \{Z(s): s \in \mathcal{S}\} is max-stable if for any n \geq 1, there exist functions a_n(s) > 0, b_n(s) such that:

\left\{\frac{\max_{i=1}^n Z_i(s) - b_n(s)}{a_n(s)}: s \in \mathcal{S}\right\} \stackrel{d}{=} \{Z(s): s \in \mathcal{S}\}

where Z_i are i.i.d. copies of Z.

17.8.2 Spectral Representation of Max-Stable Processes

Theorem 17.8.2 (de Haan). Any max-stable process with unit Fréchet margins can be represented as:

Z(s) = \max_{i \geq 1} \xi_i Y_i(s), \quad s \in \mathcal{S}

where \{\xi_i\} are points of Poisson process on (0, \infty) with intensity d\Lambda(\xi) = \xi^{-2}d\xi, and \{Y_i\} are i.i.d. replicates of a nonnegative process Y with \mathbb{E}[Y(s)] = 1.

17.8.3 Common Models

1. Smith model: Y(s) = \phi(s - V) where \phi is multivariate normal density, V \sim \text{Uniform}(\mathcal{S})
2. Schlather model: Y(s) = \max(0, W(s)) where W is Gaussian process with correlation \rho(h)
3. Brown-Resnick model: Y(s) = \exp(W(s) - \gamma(s)) where W is Gaussian process with variogram \gamma(h)

17.9 Inference for High-Dimensional Extremes

17.9.1 Conditional Extreme Value Models

Model the distribution of \mathbf{X}_{-k} = (X_1, \ldots, X_{k-1}, X_{k+1}, \ldots, X_d) given X_k > u for large u.

Heffernan-Tawn model: Assume there exist normalizing functions a_k(x) \in \mathbb{R}^{d-1}, b_k(x) > 0 \in \mathbb{R}^{d-1} such that:

\mathbb{P}\left(\frac{\mathbf{X}_{-k} - a_k(X_k)}{b_k(X_k)} \leq \mathbf{z}, \ X_k - u > y \mid X_k > u\right) \to G(\mathbf{z}) \times e^{-y}

as u \to \infty, for some non-degenerate G.

17.9.2 Graphical Models for Extremes

Use conditional independence structure to reduce dimensionality. For extreme value distribution G:

X_i \perp X_j \mid X_k \ \text{in the tail} \Leftrightarrow \lambda_{ij|k} = 0

where \lambda_{ij|k} is conditional tail dependence coefficient.

Algorithm 17.9.1 (Extremal Graphical Model Learning):

1. Estimate pairwise tail dependence matrix \Lambda
2. Apply graphical model selection (e.g., graphical lasso) to \Lambda^{-1}
3. Interpret conditional independence structure

17.10 Time Series of Extremes

17.10.1 Extremal Index

For stationary sequence \{X_t\} with marginal distribution F, define extremal index \theta \in (0, 1] such that:

\mathbb{P}(M_n \leq x) \approx F^{n\theta}(x) \quad \text{for large } n, x

where M_n = \max_{t=1}^n X_t.

Interpretation: \theta^{-1} is mean cluster size of extremes.

17.10.2 Estimation Methods

1. Blocks estimator: Divide into m blocks of length r, estimate:
   \hat{\theta} = \frac{\sum_{i=1}^m \mathbb{I}(\max_{t \in \text{block}_i} X_t > u)}{n\mathbb{P}(X > u)}
2. Runs estimator: Count exceedances separated by at least r positions:
   \hat{\theta} = \frac{\sum_{t=1}^{n-r} \mathbb{I}(X_t > u, X_{t+1} \leq u, \ldots, X_{t+r} \leq u)}{\sum_{t=1}^n \mathbb{I}(X_t > u)}
3. Intervals estimator: Based on inter-exceedance times.

17.11 Applications to Global Systems

17.11.1 Compound Climate Extremes

Model joint distribution of temperature T, precipitation P, and wind speed W:

\mathbb{P}(T > t, P < p, W > w) = C_{TPW}(F_T(t), 1 - F_P(p), F_W(w))

Use asymmetric copula to capture different tail behaviors.

17.11.2 Systemic Financial Risk

For d financial institutions, model joint losses \mathbf{L} = (L_1, \ldots, L_d). Estimate systemic risk measure:

\text{CoVaR}_q^{j|i} = \text{VaR}_q(L_j \mid L_i > \text{VaR}_\alpha(L_i))

Use multivariate extreme value distribution to estimate conditional quantiles.

17.11.3 Cascading Failures in Infrastructure

Model failure times \mathbf{T} = (T_1, \ldots, T_d) of interconnected systems. Use Marshall-Olkin distribution:

\mathbb{P}(T_1 > t_1, \ldots, T_d > t_d) = \exp\left(-\sum_{S \subseteq \{1,\ldots,d\}} \lambda_S \max_{i \in S} t_i\right)

where \lambda_S \geq 0 represents shock affecting set S.

17.11.4 Climate-Economy Nexus

Model joint extremes of climate impact C and economic output E. Test hypothesis:

H_0: \lambda_{CE} = 0 \quad \text{(climate and economy extremes independent)}

vs

H_1: \lambda_{CE} > 0 \quad \text{(extremes dependent)}

Procedure:

1. Transform to uniform margins: U_t = F_C(C_t), V_t = F_E(E_t)
2. Estimate \hat{\lambda}_{CE} using estimator from Section 17.4.1
3. Test significance via bootstrap confidence intervals

17.12 Computational Implementation

17.12.1 Python Implementation of Multivariate EVT

```python
import numpy as np
from scipy import stats, optimize, spatial
from scipy.special import gammaln
import matplotlib.pyplot as plt
from sklearn.covariance import GraphicalLasso

class MultivariateExtremes:
    def __init__(self, data):
        """
        Initialize with d-dimensional data
        data: numpy array of shape (n_samples, n_features)
        """
        self.data = data
        self.n, self.d = data.shape
        
    def to_uniform_margins(self, method='rank'):
        """Transform data to uniform margins"""
        if method == 'rank':
            # Empirical CDF
            ranks = np.apply_along_axis(
                lambda x: stats.rankdata(x, method='average'),
                axis=0, 
                arr=self.data
            )
            U = ranks / (self.n + 1)
        elif method == 'gpd':
            # Fit GPD to tails, parametric to bulk
            U = np.zeros_like(self.data)
            for j in range(self.d):
                # Fit GPD to upper tail
                threshold = np.quantile(self.data[:, j], 0.95)
                exceedances = self.data[self.data[:, j] > threshold, j] - threshold
                # Fit GPD (simplified)
                xi, beta = self._fit_gpd(exceedances)
                # Transform to uniform
                probs = np.zeros(self.n)
                for i in range(self.n):
                    if self.data[i, j] <= threshold:
                        probs[i] = stats.ecdf(self.data[:, j])(self.data[i, j])
                    else:
                        probs[i] = 1 - (1 - 0.95) * (1 + xi * (self.data[i, j] - threshold) / beta) ** (-1/xi)
                U[:, j] = probs
        return U
    
    def _fit_gpd(self, exceedances):
        """Fit GPD to exceedances (simplified)"""
        # Method of moments initial estimate
        mean_ex = np.mean(exceedances)
        var_ex = np.var(exceedances)
        if var_ex > mean_ex**2:
            xi = 0.5 * (mean_ex**2 / var_ex + 1)
            beta = 0.5 * mean_ex * (mean_ex**2 / var_ex + 1)
        else:
            xi, beta = 0.1, mean_ex
        return xi, beta
    
    def pickands_estimator(self, w):
        """Estimate Pickands dependence function A(w) for bivariate case"""
        if self.d != 2:
            raise ValueError("Pickands estimator only for bivariate case")
        
        # Transform to unit Fréchet
        U = self.to_uniform_margins()
        Y = -1 / np.log(U)
        
        # Estimate A(w)
        n = self.n
        w = np.atleast_1d(w)
        A_est = np.zeros_like(w)
        
        for idx, wi in enumerate(w):
            # Capéraà-Fougères-Genest estimator
            terms = np.log(np.maximum(wi * Y[:, 0], (1-wi) * Y[:, 1]))
            A_est[idx] = np.exp(-np.mean(terms))
        
        return A_est
    
    def tail_dependence_coefficient(self, method='CFG'):
        """Estimate upper tail dependence coefficient lambda_U"""
        if self.d != 2:
            # For multivariate, return matrix
            lambda_mat = np.zeros((self.d, self.d))
            for i in range(self.d):
                for j in range(i+1, self.d):
                    bivariate_data = self.data[:, [i, j]]
                    lambda_mat[i, j] = self._bivariate_tail_dep(bivariate_data, method)
                    lambda_mat[j, i] = lambda_mat[i, j]
            return lambda_mat
        else:
            return self._bivariate_tail_dep(self.data, method)
    
    def _bivariate_tail_dep(self, data, method='CFG'):
        """Estimate tail dependence for bivariate data"""
        n = data.shape[0]
        U = self.to_uniform_margins(data=data)
        
        if method == 'empirical':
            # Empirical estimator
            u = 0.95  # threshold
            idx = (U[:, 0] > u) & (U[:, 1] > u)
            count_joint = np.sum(idx)
            count_margin = np.sum(U[:, 0] > u)
            if count_margin > 0:
                lambda_est = count_joint / count_margin
            else:
                lambda_est = 0
        
        elif method == 'CFG':
            # Using Pickands function at w=0.5
            A_half = self.pickands_estimator(0.5)
            lambda_est = 2 - 2 * A_half[0]
        
        return lambda_est
    
    def fit_ev_copula(self, family='logistic'):
        """Fit parametric extreme value copula"""
        U = self.to_uniform_margins()
        
        if family == 'logistic':
            # Gumbel-Hougaard copula
            def neg_loglik(theta):
                if theta < 1:
                    return np.inf
                # Copula density for Gumbel
                u, v = U[:, 0], U[:, 1]
                logu, logv = -np.log(u), -np.log(v)
                sum_pow = logu**theta + logv**theta
                C = np.exp(-sum_pow**(1/theta))
                
                # Log-density (simplified)
                logc = np.log(C) + np.log(sum_pow**(1/theta - 2)) + \
                       np.log((theta-1) + sum_pow**(-1/theta) * (logu*logv)**(theta-1)) - \
                       np.log(u*v) - (2 - 1/theta)*np.log(sum_pow)
                
                return -np.sum(logc)
            
            res = optimize.minimize_scalar(
                neg_loglik,
                bounds=(1, 20),
                method='bounded'
            )
            return {'family': 'logistic', 'theta': res.x, 'loglik': -res.fun}
        
        elif family == 'husler-reiss':
            # Husler-Reiss copula
            def neg_loglik(lambda_param):
                if lambda_param < 0:
                    return np.inf
                
                u, v = U[:, 0], U[:, 1]
                y1, y2 = -np.log(u), -np.log(v)
                
                # Copula function
                term1 = y1 * stats.norm.cdf(lambda_param + 0.5/lambda_param * np.log(y1/y2))
                term2 = y2 * stats.norm.cdf(lambda_param + 0.5/lambda_param * np.log(y2/y1))
                C = np.exp(-term1 - term2)
                
                # Log-density (complicated, simplified here)
                # In practice, use numerical differentiation or known formula
                logc = np.zeros_like(u)
                eps = 1e-5
                for i in range(len(u)):
                    # Numerical differentiation for density
                    C_uu = (C[i] - np.exp(-(y1[i]+eps) * stats.norm.cdf(lambda_param + 0.5/lambda_param * np.log((y1[i]+eps)/y2[i]))
                                            - y2[i] * stats.norm.cdf(lambda_param + 0.5/lambda_param * np.log(y2[i]/(y1[i]+eps))))) / eps
                    logc[i] = np.log(-C_uu)  # Simplified
                
                return -np.sum(logc)
            
            res = optimize.minimize_scalar(
                neg_loglik,
                bounds=(1e-3, 10),
                method='bounded'
            )
            return {'family': 'husler-reiss', 'lambda': res.x, 'loglik': -res.fun}
```

17.12.2 Estimation of Spectral Measure

```python
class SpectralMeasureEstimator:
    def __init__(self, data, norm='l2'):
        self.data = data
        self.n, self.d = data.shape
        self.norm = norm
        
    def estimate(self, threshold_quantile=0.95, method='empirical'):
        """Estimate spectral measure H"""
        # Compute norms
        if self.norm == 'l2':
            norms = np.linalg.norm(self.data, axis=1)
        elif self.norm == 'l1':
            norms = np.sum(np.abs(self.data), axis=1)
        
        # Threshold
        threshold = np.quantile(norms, threshold_quantile)
        exceed_idx = norms > threshold
        
        # Exceedances and angles
        exceedances = self.data[exceed_idx]
        norms_exceed = norms[exceed_idx]
        angles = exceedances / norms_exceed[:, np.newaxis]
        
        if method == 'empirical':
            # Empirical measure (point masses)
            self.H_empirical = angles
            self.weights = np.ones(len(angles)) / len(angles)
            return self.H_empirical, self.weights
        
        elif method == 'kernel':
            # Kernel density on sphere
            from sklearn.neighbors import KernelDensity
            from scipy.special import gamma
            
            # Transform to Euclidean coordinates on sphere
            # For d=3, use spherical coordinates
            if self.d == 3:
                # Convert to spherical coordinates
                theta = np.arccos(angles[:, 2])  # polar angle
                phi = np.arctan2(angles[:, 1], angles[:, 0])  # azimuthal angle
                coords = np.column_stack([theta, phi])
                
                # Fit KDE
                kde = KernelDensity(bandwidth=0.1, kernel='gaussian')
                kde.fit(coords)
                
                # Grid for evaluation
                theta_grid, phi_grid = np.meshgrid(
                    np.linspace(0, np.pi, 50),
                    np.linspace(0, 2*np.pi, 100)
                )
                grid_points = np.column_stack([theta_grid.ravel(), phi_grid.ravel()])
                
                # Evaluate density
                log_dens = kde.score_samples(grid_points)
                dens = np.exp(log_dens)
                
                # Normalize to integrate to 1 on sphere
                # Jacobian: sin(theta) for spherical coordinates
                jacobian = np.sin(grid_points[:, 0])
                integral = np.sum(dens * jacobian) * (np.pi/49) * (2*np.pi/99)
                dens_normalized = dens / integral
                
                self.kde = kde
                self.dens_grid = dens_normalized.reshape(theta_grid.shape)
                return self.dens_grid
```

17.12.3 Graphical Model for Extremes

```python
class ExtremalGraphicalModel:
    def __init__(self, data):
        self.data = data
        self.n, self.d = data.shape
        
    def estimate_tail_dependence_matrix(self, threshold=0.95):
        """Estimate matrix of tail dependence coefficients"""
        Lambda = np.zeros((self.d, self.d))
        
        for i in range(self.d):
            for j in range(i+1, self.d):
                # Estimate pairwise tail dependence
                pair_data = self.data[:, [i, j]]
                
                # Transform to uniform
                U1 = stats.rankdata(pair_data[:, 0], method='average') / (self.n + 1)
                U2 = stats.rankdata(pair_data[:, 1], method='average') / (self.n + 1)
                
                # Empirical estimator
                u = threshold
                count_joint = np.sum((U1 > u) & (U2 > u))
                count_margin = np.sum(U1 > u)
                
                if count_margin > 0:
                    lambda_ij = count_joint / count_margin
                else:
                    lambda_ij = 0
                
                Lambda[i, j] = lambda_ij
                Lambda[j, i] = lambda_ij
        
        # Diagonal: self-dependence is 1
        np.fill_diagonal(Lambda, 1.0)
        
        return Lambda
    
    def fit_graphical_lasso(self, Lambda, rho=0.1):
        """Fit graphical lasso to tail dependence matrix"""
        # Invert tail dependence matrix (with regularization for singularity)
        K = np.linalg.inv(Lambda + 0.01 * np.eye(self.d))
        
        # Apply graphical lasso
        model = GraphicalLasso(alpha=rho, max_iter=1000)
        model.fit(K)
        
        # Get precision matrix and graph structure
        precision_matrix = model.precision_
        adjacency = (np.abs(precision_matrix) > 1e-3).astype(int)
        np.fill_diagonal(adjacency, 0)
        
        return {
            'precision': precision_matrix,
            'adjacency': adjacency,
            'partial_correlations': -precision_matrix / np.sqrt(np.outer(
                np.diag(precision_matrix), np.diag(precision_matrix)
            ))
        }
    
    def plot_extremal_graph(self, adjacency, variable_names=None):
        """Plot extremal dependence graph"""
        import networkx as nx
        import matplotlib.pyplot as plt
        
        G = nx.from_numpy_array(adjacency)
        
        plt.figure(figsize=(10, 8))
        pos = nx.spring_layout(G)
        
        if variable_names is None:
            variable_names = [f'Var{i}' for i in range(self.d)]
        
        nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')
        nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.5)
        nx.draw_networkx_labels(G, pos, labels={i: variable_names[i] for i in range(self.d)})
        
        plt.title("Extremal Dependence Graph")
        plt.axis('off')
        plt.show()
```

17.13 Diagnostic Tools and Model Validation

17.13.1 Goodness-of-Fit Tests

17.13.1.1 Test for Extreme Value Dependence Structure

Test statistic (based on Pickands function):

T = n \int_0^1 [\hat{A}_n(w) - A_\theta(w)]^2 dw

where A_\theta is Pickands function for parametric family.

Procedure:

1. Estimate \theta from data
2. Compute T_{\text{obs}}
3. Generate bootstrap samples under H_0
4. Compute p-value: \mathbb{P}(T^* > T_{\text{obs}})

17.13.1.2 Test for Asymptotic Independence

Test H_0: \lambda = 0 (asymptotic independence) vs H_1: \lambda > 0

Test statistic: \hat{\lambda}_n with bootstrap confidence interval.

17.13.2 Probability-Probability Plots

Compare empirical joint exceedance probabilities with model-based:

\hat{p}_u = \frac{1}{n} \sum_{i=1}^n \mathbb{I}(U_{i1} > u, U_{i2} > u)

vs

p_u^{\text{model}} = 1 - 2u + C_\theta(u, u)

Plot \hat{p}_u vs p_u^{\text{model}} for u near 1.

17.13.3 Conditional Simulations

1. Simulate from fitted model: \mathbf{U}^* \sim C_\theta
2. Transform to original scale: X_j^* = F_j^{-1}(U_j^*)
3. Compare characteristics of simulated vs observed extremes:
   · Cluster sizes
   · Spatial patterns
   · Temporal persistence

17.14 Time-Varying Extremal Dependence

17.14.1 Non-Stationary Models

Allow copula parameters to vary with time or covariates:

C_t(\mathbf{u}) = C_{\theta(t)}(\mathbf{u})

where \theta(t) = g(\mathbf{z}_t; \beta) with covariates \mathbf{z}_t.

Example: For Gumbel copula with \theta(t) = \exp(\beta_0 + \beta_1 t), increasing \theta(t) indicates strengthening extremal dependence.

17.14.2 Estimation

Local likelihood: For time t, weight observations by kernel:

\ell_t(\beta) = \sum_{i=1}^n K_h(t-i) \log c_{\theta(i;\beta)}(U_{i1}, \ldots, U_{id})

State-space model: Model \theta_t as latent process:

\theta_t = \theta_{t-1} + \epsilon_t, \quad \epsilon_t \sim N(0, \sigma^2)

Estimate via particle filter or MCMC.

17.15 Applications to Regime Detection

17.15.1 Monitoring Extremal Dependence Changes

Algorithm 17.15.1:

1. For each time window [t-L, t], estimate tail dependence matrix \Lambda_t
2. Compute distance between consecutive matrices: d_t = \|\Lambda_t - \Lambda_{t-1}\|_F
3. Detect change when d_t exceeds threshold
4. Identify which pairs show largest changes

17.15.2 Early Warning from Dependence Strengthening

Increasing tail dependence \lambda_{ij}(t) between subsystems i and j indicates:

1. Strengthening connections
2. Increased risk of cascading failures
3. Potential approach to critical transition

Combined indicator: Use \max_{i,j} \lambda_{ij}(t) as component of VUNC's Network measure.

17.16 Challenges and Future Directions

17.16.1 Curse of Dimensionality

For high d, estimation becomes challenging. Solutions:

1. Sparse structures: Assume conditional independence
2. Factor models: \mathbf{X} = A\mathbf{Z} + \epsilon, model extremes of \mathbf{Z}
3. Hierarchical models: Tree-structured dependence

17.16.2 Non-Stationarity and Climate Change

Changing dependence structures due to:

1. Climate change: Altering spatial dependence of weather extremes
2. Globalization: Increasing economic interdependence
3. Technological integration: Creating new dependencies

Requires non-stationary multivariate extreme value models.

17.16.3 Computational Efficiency

For large n and d, need:

1. Approximate inference: Variational methods, stochastic gradient
2. Parallel computation: GPU acceleration
3. Online algorithms: Process data streams

17.16.4 Incorporating Physical Constraints

For climate extremes, incorporate:

1. Physical laws: Conservation equations, energy balance
2. Spatial constraints: Geography, ocean currents
3. Temporal constraints: Seasonality, diurnal cycles

17.17 Conclusion

Multivariate extreme value theory provides essential tools for analyzing joint extreme behavior in global systems. Key insights:

1. Tail dependence coefficients quantify how extremes co-occur
2. Extreme value copulas flexibly model dependence structures
3. Spectral measures characterize angular distribution of extremes
4. Graphical models reveal conditional independence structure
5. Time-varying dependence tracks evolving system connectivity

Implementation recommendations:

· Start with nonparametric estimates before parametric modeling
· Validate models with diagnostic plots and tests
· Account for temporal and spatial dependence
· Use Bayesian methods for uncertainty quantification

Next chapter: Chapter 18 applies these methods to detect regime changes in tail dependence structures, a key component of the VUNC framework's Network measure.

---

Mathematical Appendices for Chapter 17

A17.1 Proof of Theorem 17.2.2 (Spectral Representation)

Sketch: For unit Fréchet margins, G_j(x) = \exp(-1/x). The exponent measure V must satisfy:

1. V(t\mathbf{x}) = t^{-1} V(\mathbf{x}) (homogeneity of order -1)
2. V(\infty, \ldots, x_j, \ldots, \infty) = 1/x_j (marginal constraints)

By homogeneity, can write V(\mathbf{x}) = \int_{S_d} \max_j(w_j/x_j) H(d\mathbf{w}) for some measure H on simplex. Marginal constraints imply \int w_j H(d\mathbf{w}) = 1.

A17.2 Derivation of Pickands Estimator

Start from definition: For unit Fréchet (Y_1, Y_2):

\mathbb{P}(Y_1 > y_1, Y_2 > y_2) = \frac{1}{y_1} + \frac{1}{y_2} - V(y_1, y_2)

where V(y_1, y_2) = (1/y_1 + 1/y_2)A(y_2/(y_1+y_2)). Then:

\mathbb{P}\left(\min\left(\frac{Y_1}{w}, \frac{Y_2}{1-w}\right) > t\right) = \mathbb{P}(Y_1 > wt, Y_2 > (1-w)t) = \frac{A(w)}{t}

Thus t \cdot \mathbb{P}(\min(Y_1/w, Y_2/(1-w)) > t) \to A(w). Empirical version gives estimator.

A17.3 Regular Variation and Domain of Attraction

A distribution F is in domain of attraction of MEVD G iff:

\lim_{t \to \infty} \frac{1 - F(t\mathbf{x})}{1 - F(t\mathbf{1})} = \frac{V(\mathbf{x})}{V(\mathbf{1})}

where V is homogeneous of order -1. This is equivalent to multivariate regular variation.

A17.4 Computation of Extremal Coefficient

For MEVD G(\mathbf{x}) = \exp(-V(\mathbf{x})), evaluate at \mathbf{x} = (x, \ldots, x):

G(x, \ldots, x) = \exp\left(-\frac{\theta_d}{x}\right)

Thus \theta_d = x \cdot V(x, \ldots, x) = V(1, \ldots, 1) by homogeneity.

For bivariate case with Pickands function:

V(1, 1) = 2A(1/2) \Rightarrow \theta_2 = 2A(1/2)

A17.5 Asymptotic Variance of CFG Estimator

For Capéraà-Fougères-Genest estimator:

\sqrt{n}(\hat{A}_n^{\text{CFG}}(w) - A(w)) \xrightarrow{d} N(0, \sigma^2(w))

where

\sigma^2(w) = A^2(w) \cdot \text{Var}\left[\log \max\left(wY_1, (1-w)Y_2\right)\right]

Consistent estimator of \sigma^2(w):

\hat{\sigma}^2(w) = \hat{A}_n^{\text{CFG}}(w)^2 \cdot \frac{1}{n} \sum_{i=1}^n \left[\log \max\left(wY_{i1}, (1-w)Y_{i2}\right) + \log \hat{A}_n^{\text{CFG}}(w)\right]^2

---

Chapter 17 provides the multivariate foundation for analyzing dependencies between extreme events across global systems. The methods developed here are crucial for the Network component of the VUNC framework, quantifying how tightly coupled different subsystems become during periods of stress.


Chapter 18: Time-Varying Tail Indices and Regime Detection

18.1 The Dynamics of Tail Risk

The core hypothesis of this thesis is that systemic risk is not static. Therefore, we must move beyond stationary Extreme Value Theory (EVT) to model the temporal evolution of the tail shape parameter ξ(t). This chapter details the methodology for estimating time-varying tail indices and statistically detecting regime shifts in the extremal behavior of global subsystems.

18.2 State-Space Model for Tail Index Evolution

We model the evolution of the tail index for a univariate process Y_t as a latent stochastic process.

· Observation Equation: For a high, time-varying threshold u_t, the exceedances Z_t = Y_t - u_t | Y_t > u_t follow a Generalized Pareto Distribution (GPD) with a time-varying shape parameter:
  Z_t \sim \text{GPD}(\xi(t), \beta(t)), \quad \xi(t) > -0.5
· State Equation (Random Walk): The logit-transformed shape parameter follows a latent drift-diffusion process to ensure it stays within a plausible range (e.g., (-0.5, 1)):
  \text{logit}(\xi(t)) = \phi \cdot \text{logit}(\xi(t-1)) + \eta_t, \quad \eta_t \sim \mathcal{N}(0, \sigma_\eta^2)
  where \text{logit}(x) = \log(\frac{x + 0.5}{1 - x}) and \phi captures persistence.
· Inference via Particle Filter: The non-Gaussian state-space model is estimated using a Sequential Monte Carlo (Particle Filter) algorithm.
  1. Initialization: Draw N particles \{\xi_0^{(i)}\} from a prior distribution.
  2. Propagation: For each time t, propagate particles: \xi_t^{(i)} \sim p(\xi_t | \xi_{t-1}^{(i)}).
  3. Weighting: Compute weights w_t^{(i)} \propto p(\mathbf{z}_t | \xi_t^{(i)}) based on the GPD likelihood of observed exceedances \mathbf{z}_t.
  4. Resampling: Resample particles with probability proportional to weights to avoid degeneracy.
  5. Output: The filtered estimate is \hat{\xi}_t = \sum_{i=1}^N w_t^{(i)} \xi_t^{(i)}.

18.3 Sequential Regime Detection via Changepoint Analysis

We test for structural breaks in the tail index series \{\hat{\xi}_t\} using online changepoint detection.

· CUSUM Statistic for Tail Shifts: Define the standardized tail index \tilde{\xi}_t = (\hat{\xi}_t - \mu_0)/\sigma_0, where \mu_0, \sigma_0 are estimated during a stable baseline period. The CUSUM process is:
  S_t = \max(0, S_{t-1} + \tilde{\xi}_t - k), \quad S_0 = 0
  where k is a reference value (often 0.5). A regime shift is signaled at time \tau = \inf\{t: S_t > h\}, where h is a control threshold.
· Generalized Likelihood Ratio (GLR) Test: For a suspected changepoint at time s within a window [t-w, t], test:
  · H_0: \xi_{t-w:t} is constant.
  · H_1: \xi_{t-w:s} = \xi_1, \xi_{s+1:t} = \xi_2, with \xi_1 \neq \xi_2.
    The test statistic is twice the log-likelihood ratio between the segmented and null models. Its distribution under H_0 is approximated via bootstrap to determine significance.

18.4 Multivariate & Cross-System Regime Detection

A systemic regime transition is characterized by synchronized shifts in tail behavior across multiple subsystems. We model this using a vine copula framework.

· Vine Copula for Tail Dependence: Let \boldsymbol{\xi}_t = (\xi_t^C, \xi_t^E, \xi_t^G, \xi_t^T) be the vector of tail indices for climate, economic, geopolitical, and technological systems. Their joint distribution is constructed using a Regular Vine (R-vine), which decomposes the multivariate density into a cascade of bivariate conditional copulas:
  f(\boldsymbol{\xi}_t) = \prod_{k=1}^{d} f_k(\xi_t^k) \cdot \prod_{j=1}^{d-1} \prod_{e \in E_j} c_{j(e),k(e)|D(e)}\left(F(\xi_t^{j(e)}|\boldsymbol{\xi}_t^{D(e)}), F(\xi_t^{k(e)}|\boldsymbol{\xi}_t^{D(e)})\right)
  where c_{j,k|D} are bivariate conditional copula densities.
· Detecting Shifts in Dependence Structure: A regime change may manifest not in the marginal \xi_t but in their dependence structure. We monitor the strength of pairwise upper tail dependence coefficients \lambda^U_{ij}(t) over time using rolling windows. A significant, simultaneous increase in multiple \lambda^U_{ij}(t) indicates a transition to a tightly coupled, systemic risk regime.

18.5 Implementation: Algorithm for Real-Time Monitoring

```python
import numpy as np
from scipy import stats, optimize
from scipy.special import logit, expit

class TimeVaryingTailMonitor:
    """
    Implements a particle filter for time-varying GPD shape parameter ξ(t)
    and sequential changepoint detection.
    """
    def __init__(self, initial_xi=0.1, n_particles=1000):
        self.n_particles = n_particles
        # Initialize particles for ξ (logit-transformed)
        self.theta_particles = logit(np.full(n_particles, initial_xi))
        self.weights = np.ones(n_particles) / n_particles
        self.estimated_xi = []

    def gpd_loglikelihood(self, exceedances, xi, beta):
        """Log-likelihood for GPD."""
        if np.any(beta <= 0):
            return -np.inf
        if abs(xi) < 1e-8:  # Exponential limit
            return -len(exceedances)*np.log(beta) - np.sum(exceedances)/beta
        else:
            z = 1 + xi * exceedances / beta
            if np.any(z <= 0):
                return -np.inf
            return -len(exceedances)*np.log(beta) - (1 + 1/xi)*np.sum(np.log(z))

    def update(self, new_exceedances, phi=0.98, sigma_eta=0.1):
        """
        One update step of the particle filter.
        new_exceedances: array of exceedances over threshold at time t.
        """
        # 1. Propagate state (AR(1) in logit space)
        new_theta = phi * self.theta_particles + np.random.normal(0, sigma_eta, self.n_particles)
        new_xi = expit(new_theta)  # Transform back to (-0.5, 1) range

        # 2. Compute weights using GPD likelihood (optimizing beta for each particle)
        log_weights = np.zeros(self.n_particles)
        for i in range(self.n_particles):
            # Method of moments for initial beta estimate
            mean_ex = np.mean(new_exceedances) if len(new_exceedances) > 0 else 1.0
            beta0 = mean_ex * (1 - new_xi[i])
            # Maximize likelihood for beta given xi[i]
            res = optimize.minimize_scalar(
                lambda b: -self.gpd_loglikelihood(new_exceedances, new_xi[i], b),
                bounds=(1e-6, 100*mean_ex)
            )
            if res.success:
                log_weights[i] = -res.fun
            else:
                log_weights[i] = -np.inf

        # Stabilize and normalize weights
        max_logw = np.max(log_weights)
        if np.isfinite(max_logw):
            weights = np.exp(log_weights - max_logw)
            weights /= np.sum(weights)
        else:
            weights = np.ones(self.n_particles) / self.n_particles

        # 3. Resample if effective sample size is too low
        ess = 1.0 / np.sum(weights**2)
        if ess < self.n_particles / 2:
            indices = np.random.choice(self.n_particles, size=self.n_particles, p=weights)
            new_theta = new_theta[indices]
            new_xi = new_xi[indices]
            weights = np.ones(self.n_particles) / self.n_particles

        self.theta_particles = new_theta
        self.weights = weights
        self.estimated_xi.append(np.average(new_xi, weights=weights))
        return self.estimated_xi[-1]

    def cusum_detector(self, h=5.0, k=0.5):
        """
        Implements CUSUM detector on the standardized series of estimated ξ.
        Returns alarm times.
        """
        xi_series = np.array(self.estimated_xi)
        # Standardize based on first half of data (assumed in-control)
        split = len(xi_series) // 2
        mu0, sigma0 = np.mean(xi_series[:split]), np.std(xi_series[:split])
        standardized = (xi_series - mu0) / sigma0

        S = 0
        alarms = []
        for t, val in enumerate(standardized):
            S = max(0, S + val - k)
            if S > h:
                alarms.append(t)
                S = 0  # Reset after detection
        return alarms
```

The Uncertainty Component (Chapters 19-22): Framework Outline

Having established how to measure Variance (tail risk), we now formalize the Uncertainty component of the VUNC framework. This component quantifies the epistemic limits of our knowledge about the system's future state.

· Chapter 19: Predictive Entropy and Ensemble Forecasting
  · Core Concept: Uncertainty is measured as the Shannon entropy of the predictive distribution for key system indicators.
  · Method: Generate an ensemble of M forecasts (e.g., using different models, parameters, or initial conditions) for a horizon h. Let p_m(\mathbf{Y}_{t+h}) be the predictive density from model m. The aggregate predictive distribution is p_{\text{ens}}(\mathbf{y}) = \frac{1}{M} \sum_{m=1}^M p_m(\mathbf{y}). Predictive entropy is:
    U(t, h) = H[p_{\text{ens}}] = -\int p_{\text{ens}}(\mathbf{y}) \log p_{\text{ens}}(\mathbf{y}) d\mathbf{y}.
  · Monitoring: Increasing U(t, h) over time indicates a loss of predictability—a key signature of an approaching non-equilibrium transition.
· Chapter 20: Bayesian Model Averaging and Structural Uncertainty
  · Core Concept: Uncertainty arises not just from parameters within a model, but from not knowing which model is "correct."
  · Method: Consider a set of competing models \{\mathcal{M}_1, \ldots, \mathcal{M}_K\} (e.g., a stationary GEV vs. a non-stationary GEV with a trend). Use Bayesian Model Averaging (BMA) to combine predictions:
    p(\mathbf{Y}_{t+h} | \mathbf{Y}_{1:t}) = \sum_{k=1}^K p(\mathbf{Y}_{t+h} | \mathcal{M}_k, \mathbf{Y}_{1:t}) \cdot \underbrace{p(\mathcal{M}_k | \mathbf{Y}_{1:t})}_{\text{Model Posterior Weight}}.
  · Application: The entropy of the model posterior weights, -\sum_k p(\mathcal{M}_k | \mathbf{Y}_{1:t}) \log p(\mathcal{M}_k | \mathbf{Y}_{1:t}), measures structural uncertainty. High structural uncertainty means the data cannot decisively select among fundamentally different narratives of system dynamics.
· Chapter 21: Information Geometry of Forecast Distributions
  · Core Concept: The "distance" between forecast distributions over time quantifies the rate at which the future is becoming unknowable.
  · Method: Use the Fisher Information Metric on the statistical manifold of forecast distributions. Let the forecast be parameterized by \boldsymbol{\theta}_t. The local rate of change is:
    g_{t} = \left\| \frac{d\boldsymbol{\theta}_t}{dt} \right\|^2_{\mathcal{I}(\boldsymbol{\theta}_t)} = \left( \frac{d\boldsymbol{\theta}_t}{dt} \right)^T \mathcal{I}(\boldsymbol{\theta}_t) \left( \frac{d\boldsymbol{\theta}_t}{dt} \right)
    where \mathcal{I}(\boldsymbol{\theta}) is the Fisher information matrix. A sharp increase in g_t indicates a rapid deformation of the predictive landscape—a loss of predictive stability.
· Chapter 22: The Limits of Predictability in Coupled Systems
  · Synthesis: This chapter integrates concepts from Chapters 19-21 to formally argue that in a tightly coupled, non-equilibrium regime, predictive uncertainty grows faster than it can be reduced by more data or better models.
  · Key Mathematical Link: Relates the predictive entropy growth rate to the dominant Lyapunov exponent \lambda_1 of the coupled system (from Chapter 28):
    \frac{dU(t, h)}{dt} \gtrsim \lambda_1 \cdot U(t, h) + \sigma_{\text{coupling}}
    where \sigma_{\text{coupling}} is a positive term representing uncertainty generated by cross-system interactions. When \lambda_1 > 0, uncertainty grows exponentially, making precise long-horizon forecasting impossible.

PART IV (CONTINUED): THE VUNC FRAMEWORK - NETWORK, COMPLEXITY, AND INTEGRATION

Chapter 23: Information-Theoretic Coupling: Transfer Entropy & Causality

23.1 Beyond Correlation: Measuring Information Flow

Traditional correlation measures fail to capture directed, nonlinear dependencies between global subsystems. Transfer Entropy (TE) provides a model-free, information-theoretic measure of predictive information flow.

Definition 23.1.1 (Transfer Entropy). For two stationary processes X_t and Y_t, the TE from X to Y quantifies the reduction in uncertainty about Y's future when knowing X's past, beyond what is contained in Y's own past:

TE_{X \rightarrow Y}(k,l) = \sum p(y_{t+1}, y_t^{(k)}, x_t^{(l)}) \log \frac{p(y_{t+1} \mid y_t^{(k)}, x_t^{(l)})}{p(y_{t+1} \mid y_t^{(k)})}

where y_t^{(k)} = (y_t, y_{t-1}, \dots, y_{t-k+1}) and x_t^{(l)} = (x_t, x_{t-1}, \dots, x_{t-l+1}) are embedded past states.

23.2 Practical Estimation and Statistical Validation

23.2.1 Kraskov-Stögbauer-Grassberger (KSG) Estimator
For continuous-valued observables, the KSG estimator using k-nearest neighbors avoids binning:

\widehat{TE}_{X \rightarrow Y} = \psi(k) + \langle \psi(n_{y_t} + 1) - \psi(n_{y_{t+1}, y_t} + 1) - \psi(n_{y_t, x_t} + 1) \rangle

where \psi is the digamma function, n_{\cdot} are neighbor counts in appropriate subspaces, and \langle \cdot \rangle denotes averaging over samples.

23.2.2 Significance Testing via Surrogate Data

1. Generate surrogate time series \{x_t^{(s)}\} that preserve X's autocorrelation but break potential coupling to Y (e.g., via iterative amplitude-adjusted Fourier transform).
2. Compute \widehat{TE}^{(s)}_{X \rightarrow Y} for many surrogates.
3. True coupling is significant if \widehat{TE}_{X \rightarrow Y} > \widehat{TE}^{(s), 95\%}_{X \rightarrow Y}.

23.3 Partial Transfer Entropy & Multivariate Systems

To isolate direct coupling from indirect pathways (e.g., X \rightarrow Z \rightarrow Y), condition on other variables:

TE_{X \rightarrow Y \mid Z} = \sum p(\cdot) \log \frac{p(y_{t+1} \mid y_t^{(k)}, x_t^{(l)}, z_t^{(m)})}{p(y_{t+1} \mid y_t^{(k)}, z_t^{(m)})}

For the global system with subsystems \mathbf{S} = \{C, E, G, T\}, we compute a directed coupling matrix \mathbf{TE}(t) \in \mathbb{R}^{4 \times 4} at each time using rolling windows.

23.4 Python Implementation: TE with Significance Testing

```python
import numpy as np
from scipy.special import digamma
from scipy.stats import rankdata
from sklearn.neighbors import NearestNeighbors

class TransferEntropy:
    """Estimates Transfer Entropy using KSG estimator with significance testing."""
    
    def __init__(self, k=5, embedding=1, tau=1):
        self.k = k  # nearest neighbors
        self.embedding = embedding  # embedding dimension
        self.tau = tau  # time delay
        
    def ksgen_te(self, source, target, condition=None):
        """KSG estimator for TE (optionally conditional)."""
        n_samples = len(source) - self.embedding
        
        # Embed data
        Yf = target[self.embedding:]  # Future target
        Yp = self._embed_series(target[:-1])  # Past target
        Xp = self._embed_series(source[:-1])  # Past source
        
        if condition is not None:
            Zp = self._embed_series(condition[:-1])
            points = [Yf, Yp, Xp, Zp]
        else:
            points = [Yf, Yp, Xp]
        
        # Find k-nearest neighbors in joint space
        data = np.column_stack(points)
        nbrs = NearestNeighbors(n_neighbors=self.k+1, metric='chebyshev')
        nbrs.fit(data)
        distances, indices = nbrs.kneighbors(data)
        eps = distances[:, -1]  # Distance to k-th neighbor
        
        # Count neighbors in subspaces
        n_yf_yp = np.zeros(n_samples, dtype=int)
        n_yp_xp = np.zeros(n_samples, dtype=int)
        n_yp = np.zeros(n_samples, dtype=int)
        
        for i in range(n_samples):
            # Subspace distances
            dist_yf_yp = np.maximum(
                np.abs(Yf - Yf[i])[:, None],
                np.abs(Yp - Yp[i])
            ).max(axis=1)
            dist_yp_xp = np.maximum(
                np.abs(Yp - Yp[i])[:, None],
                np.abs(Xp - Xp[i])
            ).max(axis=1)
            dist_yp = np.abs(Yp - Yp[i]).max(axis=1)
            
            n_yf_yp[i] = np.sum(dist_yf_yp <= eps[i])
            n_yp_xp[i] = np.sum(dist_yp_xp <= eps[i])
            n_yp[i] = np.sum(dist_yp <= eps[i])
        
        # Compute TE using digamma
        te = digamma(self.k) + np.mean(digamma(n_yp + 1) 
                                       - digamma(n_yf_yp + 1) 
                                       - digamma(n_yp_xp + 1))
        return max(te, 0)  # TE is non-negative
    
    def significance_test(self, source, target, n_surrogates=100):
        """Test TE significance using phase-randomized surrogates."""
        te_obs = self.ksgen_te(source, target)
        
        te_surr = []
        for _ in range(n_surrogates):
            # Create phase-randomized surrogate
            source_surr = self._phase_randomize(source)
            te_surr.append(self.ksgen_te(source_surr, target))
        
        p_value = np.mean(te_surr >= te_obs)
        return te_obs, p_value, te_surr
    
    def _embed_series(self, series):
        """Time-delay embedding."""
        n = len(series) - (self.embedding - 1) * self.tau
        embedded = np.zeros((n, self.embedding))
        for i in range(self.embedding):
            start = i * self.tau
            end = start + n
            embedded[:, i] = series[start:end]
        return embedded
    
    def _phase_randomize(self, series):
        """Phase randomization preserving power spectrum."""
        n = len(series)
        if n % 2 == 0:
            n_half = n // 2
        else:
            n_half = (n - 1) // 2
        
        fft_vals = np.fft.fft(series)
        amplitudes = np.abs(fft_vals)
        phases = np.angle(fft_vals)
        
        # Randomize phases (keep first and last for real signal)
        random_phases = np.random.uniform(-np.pi, np.pi, n_half - 1)
        new_phases = np.zeros(n, dtype=complex)
        new_phases[0] = phases[0]  # DC component
        
        # Positive frequencies
        new_phases[1:n_half] = random_phases
        # Negative frequencies (conjugate symmetry)
        new_phases[n_half+1:] = -random_phases[::-1]
        
        if n % 2 == 0:
            new_phases[n_half] = phases[n_half]  # Nyquist frequency
        
        # Reconstruct signal
        surrogate = np.fft.ifft(amplitudes * np.exp(1j * new_phases))
        return np.real(surrogate)
```

---

Chapter 24: Network Science Approaches: Multilayer & Temporal Networks

24.1 Representing Global Systems as Multilayer Networks

Global subsystems interact heterogeneously. A multilayer network \mathcal{M} = (\mathcal{G}, \mathcal{C}) captures this, where:

· \mathcal{G} = \{G_\alpha\}_{\alpha=1}^4 are layer graphs for Climate (C), Economy (E), Geopolitics (G), Technology (T).
· \mathcal{C} = \{E_{\alpha \beta} \subseteq V_\alpha \times V_\beta\} are inter-layer couplings.

Definition 24.1.1 (Supra-Adjacency Matrix). The complete system is represented as:

\mathbf{A} = \begin{bmatrix}
\mathbf{A}_C & \mathbf{\Omega}_{CE} & \mathbf{\Omega}_{CG} & \mathbf{\Omega}_{CT} \\
\mathbf{\Omega}_{EC} & \mathbf{A}_E & \mathbf{\Omega}_{EG} & \mathbf{\Omega}_{ET} \\
\mathbf{\Omega}_{GC} & \mathbf{\Omega}_{GE} & \mathbf{A}_G & \mathbf{\Omega}_{GT} \\
\mathbf{\Omega}_{TC} & \mathbf{\Omega}_{TE} & \mathbf{\Omega}_{TG} & \mathbf{A}_T
\end{bmatrix}

where \mathbf{A}_\alpha are intra-layer adjacency matrices, and \mathbf{\Omega}_{\alpha\beta} are inter-layer coupling matrices weighted by TE.

24.2 Dynamic Community Detection & Regime Identification

24.2.1 Multilayer Modularity Maximization
Find node partitions that maximize modularity across layers and time:

Q = \frac{1}{2\mu} \sum_{ij\alpha\beta} \left[ (A_{ij\alpha} - \gamma_\alpha \frac{k_{i\alpha}k_{j\alpha}}{2m_\alpha}) \delta_{\alpha\beta} + \delta_{ij} C_{j\alpha\beta} \right] \delta(c_{i\alpha}, c_{j\beta})

where \mu is total edge weight, \gamma_\alpha resolution parameters, and C_{j\alpha\beta} inter-layer couplings.

24.2.2 Temporal Networks & Change Point Detection
For time-varying adjacency matrices \mathbf{A}(t), monitor changes in network summary statistics:

· Global efficiency: E(t) = \frac{1}{N(N-1)} \sum_{i \neq j} \frac{1}{d_{ij}(t)} where d_{ij} is shortest path.
· Spectral gap: \lambda_1(t) - \lambda_2(t) of normalized Laplacian.
· Modularity change: \Delta Q(t) = |Q(t) - Q(t-1)|.

A regime shift is signaled by simultaneous, persistent changes in multiple statistics.

24.3 Application: Tracking Cross-System Vulnerability

Algorithm 24.3.1 (Vulnerability Propagation):

1. Construct TE-weighted multilayer network \mathcal{M}(t) for time window.
2. Compute inter-layer centrality:
   ILC_i(t) = \sum_{\alpha \neq \beta} \sum_{j \in V_\beta} TE_{i_\alpha \rightarrow j_\beta}
3. Identify bridge nodes with high ILC—these transmit shocks across subsystems.
4. Simulate shock propagation via linear threshold model:
   X_i(t+1) = \begin{cases}
   1 & \text{if } \sum_j w_{ji} X_j(t) \geq \theta_i \\
   0 & \text{otherwise}
   \end{cases}
   where weights w_{ji} come from TE.

---

Chapter 25: Spectral Methods & Frequency-Domain Coupling

25.1 Spectral Transfer Entropy

While time-domain TE measures total information flow, spectral TE decomposes it by frequency:

TE_{X \rightarrow Y}(\omega) = \log \frac{S_{Y|Y^-}(\omega)}{S_{Y|Y^-, X^-}(\omega)}

where S_{Y|Y^-}(\omega) is the conditional spectral density of Y given its own past, and S_{Y|Y^-, X^-}(\omega) conditions on both pasts.

Estimation via Multivariate Autoregressive (MVAR) Models:

1. Fit MVAR model: \mathbf{z}_t = \sum_{k=1}^p \mathbf{\Phi}_k \mathbf{z}_{t-k} + \boldsymbol{\epsilon}_t
2. Compute spectral matrix: \mathbf{S}(\omega) = \mathbf{H}(\omega) \mathbf{\Sigma} \mathbf{H}^*(\omega)
3. Partial spectral density: S_{Y|Y^-, X^-}(\omega) = \mathbf{\Sigma}_{YY} - \mathbf{\Sigma}_{YX} \mathbf{\Sigma}_{XX}^{-1} \mathbf{\Sigma}_{XY}

25.2 Wavelet Coherence for Non-Stationary Signals

For non-stationary global indicators (e.g., climate oscillations, business cycles), use wavelet coherence:

R_{XY}(s, \tau) = \frac{|S(s^{-1} W_{XY}(s, \tau))|}{\sqrt{|S(s^{-1} |W_X(s, \tau)|^2) S(s^{-1} |W_Y(s, \tau)|^2)}}

where W_X(s, \tau) is continuous wavelet transform at scale s and translation \tau, and S is smoothing operator.

25.3 Implementation: Multitaper Spectral TE

```python
import mne
from mne_connectivity import spectral_connectivity_time

class SpectralCoupling:
    """Estimates frequency-domain coupling using multitaper methods."""
    
    def __init__(self, sfreq=1.0, fmin=0.01, fmax=0.5, method='mic'):
        self.sfreq = sfreq
        self.fmin = fmin
        self.fmax = fmax
        self.method = method  # 'mic', 'coh', 'plv'
    
    def compute_spectral_te(self, data, indices):
        """
        data: [n_signals, n_times]
        indices: tuple of (source_idx, target_idx)
        """
        # Compute spectral connectivity
        con = spectral_connectivity_time(
            data,
            method=self.method,
            indices=indices,
            sfreq=self.sfreq,
            fmin=self.fmin,
            fmax=self.fmax,
            faverage=True,
            mt_adaptive=True,
            n_jobs=1
        )
        
        # Extract magnitude-squared coherence
        if self.method == 'coh':
            # Convert to spectral TE approximation: TE(f) ≈ -log(1 - |Coh(f)|^2)
            coh = np.abs(con.get_data()[0])
            ste = -np.log(1 - coh**2)
        else:
            ste = np.abs(con.get_data()[0])
        
        return ste, con.freqs
    
    def detect_coupling_shifts(self, data_series, window_size=365, step=30):
        """Track spectral coupling changes over time."""
        n_times = data_series.shape[1]
        n_windows = (n_times - window_size) // step + 1
        
        coupling_evolution = []
        for w in range(n_windows):
            start = w * step
            end = start + window_size
            window_data = data_series[:, start:end]
            
            # Compute average spectral TE in low-freq band (slow dynamics)
            ste, freqs = self.compute_spectral_te(window_data, indices=(0, 1))
            low_freq_mask = (freqs >= 0.01) & (freqs <= 0.1)
            avg_coupling = np.mean(ste[low_freq_mask])
            
            coupling_evolution.append(avg_coupling)
        
        return np.array(coupling_evolution)
```

---

Chapter 26: Applications to Global System Coupling

26.1 Constructing the Global Coupling Tensor

For the four subsystems S \in \{C, E, G, T\}, compute time-varying coupling tensor:

\mathcal{TE}(t) \in \mathbb{R}^{4 \times 4 \times F}

where F is number of frequency bands (e.g., intra-day, business cycle, decadal).

Procedure:

1. For each pair (S_i, S_j), compute windowed TE TE_{ij}(t) and spectral TE STE_{ij}(t, f).
2. Apply Fisher z-transform: z_{ij}(t) = \text{atanh}(\sqrt{TE_{ij}(t)}) for variance stabilization.
3. Compute net coupling asymmetry: \Delta_{ij}(t) = TE_{ij}(t) - TE_{ji}(t).

26.2 Early Warning from Network Cohesion

Theorem 26.2.1 (Coupling-Criticality Hypothesis). As a system approaches a critical transition:

1. Average coupling \bar{TE}(t) increases.
2. Coupling heterogeneity decreases (subsystems become uniformly connected).
3. Spectral concentration increases (coupling concentrates in low frequencies).

Monitoring Protocol:

1. Compute network cohesion index:
   \mathcal{N}(t) = \frac{\bar{TE}(t)}{1 + \sigma_{TE}(t)} \times \frac{STE_{\text{low}}(t)}{STE_{\text{total}}(t)}
2. Regime alert when \mathcal{N}(t) > \mu_{\mathcal{N}} + 2\sigma_{\mathcal{N}} for 4+ weeks.

26.3 Case Study: Climate-Finance Feedback (2020-2023)

Data:

· Climate: Daily temperature anomalies (ERA5)
· Economy: S&P 500 volatility (VIX), bond spreads
· Compute rolling TE (90-day windows)

Findings:

1. Bidirectional coupling increased 40% from 2020 to 2023.
2. Extreme events (heatwaves → market stress) show highest TE.
3. Network analysis reveals finance as superspreader of shocks.

---

Chapter 27: Dynamical Complexity: Embedding & Reconstruction

27.1 Phase Space Reconstruction

Theorem 27.1.1 (Takens' Embedding). For a deterministic dynamical system with attractor dimension d_A, the delay embedding:

\mathbf{x}(t) = [y(t), y(t-\tau), y(t-2\tau), \dots, y(t-(m-1)\tau)]

with m > 2d_A generically reconstructs the attractor topology from a scalar observable y(t).

Optimal Parameter Selection:

· Time delay \tau: First minimum of mutual information I(y(t), y(t+\tau)).
· Embedding dimension m: False nearest neighbors (FNN) algorithm.

27.2 Practical Implementation with Noise

Algorithm 27.2.1 (Robust Embedding):

```python
from nolds import embed_dim, sampen, corr_dim
from scipy.signal import find_peaks

class DynamicalReconstruction:
    """Reconstructs phase space from time series."""
    
    def __init__(self, max_dim=10, max_tau=100):
        self.max_dim = max_dim
        self.max_tau = max_tau
    
    def estimate_tau(self, series, method='mutual_info'):
        if method == 'mutual_info':
            # Compute mutual information for different taus
            tau_vals = range(1, self.max_tau)
            mi_vals = [self._mutual_info(series, tau) for tau in tau_vals]
            # First minimum
            peaks, _ = find_peaks(-np.array(mi_vals))
            return tau_vals[peaks[0]] if len(peaks) > 0 else 1
        else:  # 'autocorrelation'
            acf = np.correlate(series - np.mean(series), 
                              series - np.mean(series), mode='full')
            acf = acf[len(acf)//2:]
            # First crossing of 1/e
            threshold = np.max(acf) / np.exp(1)
            crossings = np.where(acf < threshold)[0]
            return crossings[0] if len(crossings) > 0 else 1
    
    def estimate_dimension(self, series, tau):
        """Use Cao's method for embedding dimension."""
        n = len(series)
        E = np.zeros(self.max_dim - 1)
        
        for m in range(1, self.max_dim):
            # Embed with dimension m
            embedded_m = self._embed(series, m, tau)
            embedded_m1 = self._embed(series, m+1, tau)
            
            # Find nearest neighbors in m dimensions
            nbrs = NearestNeighbors(n_neighbors=2, metric='euclidean')
            nbrs.fit(embedded_m)
            distances, indices = nbrs.kneighbors(embedded_m)
            nearest_idx = indices[:, 1]  # Skip self
            
            # Compute E(m)
            a = np.linalg.norm(embedded_m1 - embedded_m1[nearest_idx], axis=1)
            b = np.linalg.norm(embedded_m - embedded_m[nearest_idx], axis=1)
            with np.errstate(divide='ignore', invalid='ignore'):
                E[m-1] = np.mean(a / b)
        
        # Find where E stops changing
        E1 = E[1:] / E[:-1]
        optimal_m = np.where(E1 <= 1.05)[0]
        return optimal_m[0] + 2 if len(optimal_m) > 0 else self.max_dim
    
    def reconstruct(self, series, m=None, tau=None):
        if tau is None:
            tau = self.estimate_tau(series)
        if m is None:
            m = self.estimate_dimension(series, tau)
        
        embedded = self._embed(series, m, tau)
        return embedded, m, tau
```

---

Chapter 28: Lyapunov Spectrum & Predictability

28.1 Estimating Lyapunov Exponents

Definition 28.1.1 (Lyapunov Exponents). For a dynamical system d\mathbf{x}/dt = \mathbf{F}(\mathbf{x}), the Lyapunov spectrum \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_d quantifies average exponential divergence/convergence of nearby trajectories:

\lambda_i = \lim_{t \to \infty} \lim_{\delta \mathbf{x}_0 \to 0} \frac{1}{t} \ln \frac{\|\delta \mathbf{x}_i(t)\|}{\|\delta \mathbf{x}_i(0)\|}

where \delta \mathbf{x}_i(t) evolves in the ith Lyapunov direction.

28.1.2 Rosenstein's Algorithm (Practical)
For reconstructed trajectory \{\mathbf{x}_j\}_{j=1}^N:

1. Find nearest neighbor \mathbf{x}_{\hat{\jmath}} for each point.
2. Track divergence: d_j(t) = \|\mathbf{x}_{j+t} - \mathbf{x}_{\hat{\jmath}+t}\|.
3. Largest LE: \lambda_1 \approx \frac{1}{t \Delta t} \langle \ln d_j(t) \rangle.

28.2 Maximum Predictability Horizon

Theorem 28.2.1 (Predictability Limit). For a system with largest LE \lambda_1 > 0, the maximum predictable horizon is:

T_{\text{pred}} \approx \frac{1}{\lambda_1} \ln \left( \frac{\Delta_{\text{max}}}{\Delta_{\text{error}}} \right)

where \Delta_{\text{max}} is attractor size, \Delta_{\text{error}} initial uncertainty.

Application to Global Systems: Compute \lambda_1(t) from VUNC's first principal component. Decreasing T_{\text{pred}}(t) indicates loss of predictability—a regime shift precursor.

---

Chapter 29: Fractal Dimensions & Scaling

29.1 Correlation Dimension

Definition 29.1.1. The correlation dimension D_2 captures the scaling of neighbor density:

D_2 = \lim_{r \to 0} \frac{\log C(r)}{\log r}, \quad C(r) = \frac{2}{N(N-1)} \sum_{i<j} \Theta(r - \|\mathbf{x}_i - \mathbf{x}_j\|)

where \Theta is Heaviside step function.

Estimation: Plot \log C(r) vs \log r, fit linear region.

29.2 Multifractal Spectrum

Complex systems often exhibit multifractality—different scaling at different moments.

29.2.1 Generalized Dimensions:

D_q = \frac{1}{q-1} \lim_{r \to 0} \frac{\log \sum_i p_i^q}{\log r}

where p_i is probability mass in box i.

29.2.2 Multifractal Spectrum:

f(\alpha) = \inf_q [q\alpha - \tau(q)], \quad \tau(q) = (q-1)D_q

Interpretation for Regimes:

· Simple system: Narrow f(\alpha) (≈monofractal).
· Complex regime: Broad f(\alpha) (multifractal).

---

Chapter 30: Entropy & Information Measures of Complexity

30.1 Sample Entropy & Regularity

Definition 30.1.1 (Sample Entropy). For embedded vectors \{\mathbf{x}_i\},

\text{SampEn}(m, r, N) = -\ln \frac{A}{B}

where:

· A = \#\{ \|\mathbf{x}_i^{m+1} - \mathbf{x}_j^{m+1}\| < r \}
· B = \#\{ \|\mathbf{x}_i^m - \mathbf{x}_j^m\| < r \}

Clinical interpretation: Lower SampEn → more regularity/less complexity.

30.2 Permutation Entropy

Advantage: Robust to noise, computationally efficient.

Algorithm:

1. For embedded vector \mathbf{x}_i = (x_i, x_{i+\tau}, \dots, x_{i+(m-1)\tau}), compute rank permutation \pi_i.
2. Estimate probabilities p(\pi) over all m! possible permutations.
3. Permutation entropy: H_p(m) = -\sum_\pi p(\pi) \log p(\pi).

Normalized: 0 \leq H_p^{\text{norm}} = H_p(m)/\log(m!) \leq 1.

30.3 Complexity-Entropy Plane

Definition 30.3.1. Plot systems in 2D plane:

· x-axis: Normalized permutation entropy H_p^{\text{norm}}.
· y-axis: Statistical complexity C = H_p^{\text{norm}} \cdot Q where Q is disequilibrium (Jensen-Shannon divergence from uniform).

Regime interpretation:

· Lower-left: Ordered, predictable (low H, low C).
· Lower-right: Random, unstructured (high H, low C).
· Upper-middle: Complex, structured (medium H, high C).

---

Chapter 31: The VUNC Vector: Integration Framework

31.1 Standardization & Weighting

For each time t, compute raw components:

· \mathbf{V}(t): Tail indices \xi_i(t) from Chapter 18.
· \mathbf{U}(t): Predictive entropies from Chapters 19-22.
· \mathbf{N}(t): Network coupling measures from Chapters 23-26.
· \mathbf{C}(t): Complexity measures from Chapters 27-30.

Standardization:

z_i(t) = \frac{x_i(t) - \mu_i^{\text{baseline}}}{\sigma_i^{\text{baseline}}}

where baseline is 2010-2019 period.

Weighting: Use entropy weighting—higher variability components get more weight:

w_i = \frac{\sigma_i}{\sum_j \sigma_j}

31.2 Mahalanobis Distance for Regime Detection

Definition 31.2.1. The VUNC anomaly score:

D^2(t) = (\mathbf{z}(t) - \boldsymbol{\mu}_0)^T \mathbf{\Sigma}_0^{-1} (\mathbf{z}(t) - \boldsymbol{\mu}_0)

where \boldsymbol{\mu}_0, \mathbf{\Sigma}_0 are mean and covariance during baseline.

Regime Classification:

· Stable: D^2(t) < \chi^2_{4, 0.95}.
· Transitional: \chi^2_{4, 0.95} \leq D^2(t) < \chi^2_{4, 0.99}.
· Non-equilibrium: D^2(t) \geq \chi^2_{4, 0.99}.

31.3 Bayesian Updating of Regime Probability

State-space model:

1. Observation model: D^2(t) \mid s_t \sim \text{Gamma}(\alpha_{s_t}, \beta_{s_t}) where s_t \in \{\text{stable}, \text{transition}, \text{non-eq}\}.
2. Transition model: s_t \sim \text{Markov}(P_{s_{t-1} \to s_t}).
3. Filtering: Update p(s_t \mid D^2_{1:t}) via Bayes' rule.

Implementation with particle filter (see Chapter 18 code).

---

Chapter 32: Computational Architecture & Real-Time Processing

32.1 Pipeline Architecture

```
Data Sources → Preprocessing → Feature Computation → VUNC Integration → Dashboard
    ↓              ↓                ↓                    ↓              ↓
Climate APIs    Cleaning        TE, LE, ξ(t)        D²(t) calc    Visual alerts
Economic feeds  Imputation      Dim estimation    Regime class    API endpoints
News streams    Normalization   Entropy measures  Bayesian update Historical views
```

32.2 Distributed Computation

```python
# Example using Dask for parallel VUNC computation
import dask.array as da
from dask.distributed import Client

class DistributedVUNC:
    def __init__(self, n_workers=4):
        self.client = Client(n_workers=n_workers)
    
    def compute_vunc_parallel(self, data_chunks):
        """Compute VUNC components in parallel."""
        # Distribute data
        data_futures = []
        for chunk in data_chunks:
            future = self.client.scatter(chunk)
            data_futures.append(future)
        
        # Parallel computation of components
        te_futures = [self.client.submit(compute_te, df) for df in data_futures]
        lyap_futures = [self.client.submit(compute_lyapunov, df) for df in data_futures]
        entropy_futures = [self.client.submit(compute_entropy, df) for df in data_futures]
        
        # Gather results
        te_results = self.client.gather(te_futures)
        lyap_results = self.client.gather(lyap_futures)
        entropy_results = self.client.gather(entropy_futures)
        
        return self._integrate_components(te_results, lyap_results, entropy_results)
```

32.3 Database Schema for VUNC Storage

```sql
CREATE TABLE vunc_metrics (
    timestamp TIMESTAMP PRIMARY KEY,
    variance_tailindices JSONB,  -- {climate: 0.2, economy: 0.3, ...}
    uncertainty_entropies JSONB, -- {predictive: 1.5, structural: 0.8}
    network_coupling JSONB,      -- {TE_matrix: [[...]], net_strength: 0.4}
    complexity_measures JSONB,   -- {lyapunov: 0.05, dimension: 2.3}
    vunc_vector REAL[4],         -- [V, U, N, C] standardized
    mahalanobis_d2 REAL,
    regime_label TEXT,           -- 'stable', 'transition', 'non-equilibrium'
    alert_level INTEGER          -- 0-4
);

CREATE INDEX idx_regime ON vunc_metrics(regime_label);
CREATE INDEX idx_alert ON vunc_metrics(alert_level);
```

---

Chapter 33: Validation & Falsification Protocols

33.1 Out-of-Sample Testing

Procedure:

1. Train: Fit all models (GPD, TE, etc.) on 2010-2019 data.
2. Validate: Tune thresholds on 2020-2022 data.
3. Test: Apply to 2023-2026 data without recalibration.
4. Benchmark: Compare against:
   · Random classifier
   · Simple threshold on volatility
   · Published early warning indicators

33.2 Adversarial Testing

Red Team Challenges:

1. Can stationary processes produce VUNC signals?
   · Test: Generate AR(1), GARCH, stochastic volatility processes.
   · Result: VUNC should not trigger on pure stationary noise.
2. Are signals robust to measurement error?
   · Test: Add 30% noise, missing data, structural breaks in measurement.
   · Requirement: Signals persist through perturbations.
3. Alternative explanations?
   · Test: Could observed patterns arise from known cycles (solar, ENSO)?
   · Control: Include cycle proxies in models.

33.3 Success Criteria

The framework succeeds if by 2027:

1. Accuracy: >80% correct regime classification (vs. expert consensus).
2. Lead time: Average 3-6 month warning before acknowledged crises.
3. False positive rate: <20% (less than 1 major false alarm per year).
4. Theoretical contribution: New coupling patterns documented & replicated.

The framework fails if:

1. No regime shift detected but major crisis occurs.
2. Multiple false alarms undermine credibility.
3. Simpler benchmarks outperform VUNC.

---

Chapter 34: Policy Implementation & Decision Support

34.1 Alert System Design

Tiered Response Protocol:

Level D² Threshold Indicators Recommended Actions
1 χ²₀.₉₀ 1/4 VUNC components elevated Increase monitoring frequency
2 χ²₀.₉₅ 2/4 elevated, λ₁ > 0 Activate inter-agency coordination
3 χ²₀.₉₉ 3/4 elevated, TE increasing Pre-position resources, public advisories
4 χ²₀.₉₉₅ All components extreme, cascading signals Emergency measures, crisis management

34.2 Dashboard for Decision Makers

Key visualizations:

1. VUNC Radar Plot: 4 axes showing standardized components.
2. Coupling Network: Animated TE network with node size = subsystem stress.
3. Regime Probability Timeline: Bayesian posterior over regimes.
4. Scenario Projections: "What-if" simulations under different interventions.

34.3 Integration with Existing Frameworks

Climate: Feed into IPCC scenarios, climate risk assessments.
Financial: Inform FSOC (Financial Stability Oversight Council) systemic risk reports.
Security: Provide input to intelligence community's strategic forecasts.
AI Governance: Guide development of "circuit breakers" for AI systems.

34.4 Ethical Guidelines

1. Transparency: All models, code, and data (except sensitive) publicly available.
2. Accountability: Clear documentation of false alarms/misses.
3. Equity: Monitor distributional impacts of warnings and responses.
4. Human-in-the-loop: Automated alerts always require human confirmation for Level 3+ responses.

---

Conclusion of Part IV: The Complete VUNC Framework

The VUNC framework operationalizes the thesis hypothesis through four rigorously defined components:

1. Variance (V): Heavy-tailed risks via time-varying EVT (Chapters 15-18).
2. Uncertainty (U): Predictive entropy and model ambiguity (Chapters 19-22).
3. Network (N): Information-theoretic coupling (Chapters 23-26).
4. Complexity (C): Dynamical systems measures (Chapters 27-30).

Their integration (Chapters 31-34) produces a real-time, falsifiable indicator of global system state. The framework is designed to either:

· Validate the non-equilibrium transition hypothesis with statistical rigor, or
· Falsify it through predefined tests, advancing science either way.

