

Part I: Foundational Models for AI System Energy

Chapter 1 – Fundamental Physical and Mathematical Foundations

1.1 Motivation and Epistemological Positioning
This thesis is grounded in universal physical and mathematical laws that are invariant across hardware generations and software paradigms. Before analyzing algorithms, we establish that artificial intelligence systems are physical processes. Every computation has an energetic cost, and every bit processed manifests as heat. Therefore, rigorous energy optimization must begin with the immutable laws of thermodynamics, electromagnetism, and information theory.

1.2 Energy, Power, and Time
The primary objective is the minimization of total energy consumption, defined as:

E = \int_0^T P(t)\, dt 

where  E  is energy in joules,  P(t)  is instantaneous power, and  T  is workload execution time.

1.3 Electrical Power and Joule Dissipation
In digital systems, electrical power is fundamentally:

P = V \cdot I 

Resistive dissipation via Joule heating is:

P_{\text{Joule}} = I^2 R \quad \text{and thus} \quad E_{\text{Joule}} = \int I^2 R \, dt 

This directly links computational activity to thermal dissipation.

1.4 Dynamic Power in CMOS Computing
The dominant source in active logic is dynamic switching power:

P_{\text{dyn}} = C_{\text{eff}} V^2 f 

where  C_{\text{eff}}  is effective switched capacitance,  V  is supply voltage, and  f  is clock frequency. This equation reveals the three primary optimization levers: voltage scaling, frequency scaling, and reduction of effective switching activity.

1.5 Leakage Power and Thermally Induced Feedback
At advanced process nodes, static leakage power becomes significant:

P_{\text{leak}} = V \cdot I_{\text{leak}}(T) 

Subthreshold leakage current  I_{\text{leak}}(T)  has a strong exponential temperature dependence (e.g.,  \propto \exp(-E_g / k_B T) ). This creates a positive thermal-electrical feedback loop: higher power raises temperature, which exponentially increases leakage, further raising power and temperature. This loop is the primary driver behind the industry's mandatory shift to liquid cooling for AI accelerator racks, which routinely operate at 40–120 kW (H100/Blackwell era) and are projected to reach 150–200+ kW for next-generation systems.

1.6 First Law of Thermodynamics (Energy Conservation)
At the server level, energy conservation states:

E_{\text{in}} = E_{\text{compute}} + E_{\text{heat}} + E_{\text{loss}} 

As mechanical work is negligible, nearly all input electrical energy converts to waste heat:

E_{\text{in}} \approx Q_{\text{heat}} 

This necessitates cooling systems, whose overhead is captured by the Power Usage Effectiveness (PUE).

1.7 Information Theory and Physical Limits
Landauer's principle establishes a thermodynamic minimum for irreversible bit erasure:

E_{\min} = k_B T \ln 2 

(≈ 2.87 × 10⁻²¹ J/bit at 300 K). While a fundamental conceptual anchor, modern servers operate 10⁸–10⁹ times above this bound. Practical dissipation is dominated by switching, data movement, and the leakage-cooling cycle, not information-theoretic limits.

1.8 Implications and Foundational Levers
AI workloads act as energy-shaping processes. The physical levers from Section 1.4 translate directly to software and hardware control parameters.

1.9 Beyond Classical CMOS: Emerging Paradigms
The established equations are invariant, but emerging paradigms reinterpret energy flows:

· Thermodynamic/Probabilistic Computing exploits thermal noise for generative AI tasks, with 2025 prototypes (e.g., Extropic's X0, Normal Computing's CN101) targeting orders-of-magnitude efficiency gains for stochastic workloads.
· Reversible/Adiabatic Logic aims to recover energy, with 2025 test chips (e.g., Vaire Computing's Ice River) demonstrating net energy recovery in simple circuits.

These paradigms operate within the invariant conservation laws laid out in this chapter.

---

Chapter 2 – Server-Level Energy Modeling for AI Workloads

2.1 Objective: Bridging Physics and Practical Measurement
This chapter builds on Chapter 1's laws to create a quantitative, server-level energy model. The goal is to express the total energy cost of an AI workload in measurable terms, explicitly incorporating thermal limits, cooling overhead, and infrastructure efficiency.

2.2 Decomposition of Server Energy Consumption
Total server energy over interval [0, T] decomposes as:

E_{\text{server}} = E_{\text{compute}} + E_{\text{memory}} + E_{\text{interconnect}} + E_{\text{cooling}} + E_{\text{overhead}} 

This additive model conserves energy and isolates major cost centers.

2.3 Computational Energy Model

E_{\text{compute}} = \int_0^T \left[ C_{\text{eff}}(t) V(t)^2 f(t) + V(t) I_{\text{leak}}(T(t)) \right] dt 

This integrates the dynamic (switching) and static (leakage) terms from Chapter 1, with time-varying voltage  V(t) , frequency  f(t) , and temperature  T(t)  reflecting real-world DVFS and thermal feedback.

2.4 Memory and Data Movement Energy
AI workloads, especially transformers, are often memory-bound. Energy is modeled as:

E_{\text{memory}} = N_{\text{mem}} \cdot E_{\text{access}} 

where  N_{\text{mem}}  is the number of transactions and  E_{\text{access}}  is the hierarchy-dependent energy per access.

2.5 Interconnect and Communication Costs
Similarly, communication energy is:

E_{\text{interconnect}} = N_{\text{bits}} \cdot E_{\text{bit}} 

where  E_{\text{bit}}  is the cost per bit transmitted, varying from on-chip networks to high-speed links like NVLink.

2.6 Thermal–Electrical Coupling at Server Level
The first-order thermal dynamics are:

C_{\text{th}} \frac{dT}{dt} = P_{\text{server}}(t) - P_{\text{cooling}}(t) 

where  P_{\text{server}}(t)  sums compute, memory, and interconnect power. This formalizes the positive feedback loop, explaining the need for aggressive cooling and throttling in high-density AI racks.

2.7 Cooling Energy and Infrastructure Efficiency (PUE)
The total facility energy is scaled by the Power Usage Effectiveness:

E_{\text{total}} = \text{PUE} \cdot E_{\text{IT}} 

where  E_{\text{IT}}  is the energy of the IT equipment itself. Efficient modern data centers achieve PUEs of 1.1–1.5, meaning a 10–50% overhead. However, supporting the 40-140kW racks of AI compute requires a fundamental shift to liquid cooling—with water's heat transfer coefficient being ~3,500x greater than air—transforming data centers into industrial-scale thermodynamic plants. Global impact is vast: data center energy was ~415 TWh in 2024 and is projected to reach ~945 TWh by 2030 under current trends, with AI-accelerated servers a primary driver.

2.8 Time, Performance, and Energy Trade-offs
The classic trade-off is captured by:

E = P_{\text{avg}} \cdot T 

Faster execution (lower T) typically requires higher voltage/frequency (higher P), while slower execution reduces dynamic power but increases leakage and cooling duration. Thermal limits often govern this optimization.

2.9 Integrated Model and Typical Energy Costs
The server-level energy is a function of control and workload variables:

E_{\text{server}} = \mathcal{F}(V(t), f(t), T(t), W(t)) 

where  W(t)  is the workload profile. This model is grounded by contemporary hardware costs:

Component Typical Energy (2025-2026) Notes
Compute (MAC) 1–10 pJ FP8/INT8 operations, dynamic dominant
HBM Access 10–20 pJ/bit High-Bandwidth Memory, critical bottleneck
DRAM Access 100–200 pJ/access Off-package memory
On-Chip/Off-Chip Interconnect 1–50 pJ/bit Ranges from network-on-chip to NVLink

2.10 Transition to Workload-Aware Modeling
This chapter provides the physical and server-level framework to quantify energy. The final step is to characterize  W(t) —the AI workload itself. Chapter 3 will therefore introduce AI workloads as structured processes that explicitly shape the parameters  V(t) ,  f(t) ,  N_{\text{mem}} , and  N_{\text{bits}} , enabling targeted, semantics-preserving energy optimization.

Chapter 3 – AI Workloads as Energy-Shaping Computational Processes

3.1 Conceptualization of AI Workloads: From Abstraction to Physical Excitation
Building on the server-level model from Chapter 2, this chapter redefines an AI workload. It is not merely an abstract algorithm but a time-varying physical excitation of an electrical-thermal system. A workload is the structured sequence of operations that, when executed, directly modulates the server's control variables V(t), f(t), and T(t) established in Chapters 1 & 2.

Let an AI workload be defined as a measurable, time-dependent function:

W(t) = \{ \mathcal{O}(t), \mathcal{M}(t), \mathcal{C}(t) \}

where:

· \mathcal{O}(t): Arithmetic/logic operations (e.g., matrix multiplies, convolutions).
· \mathcal{M}(t): Memory access patterns (e.g., loading weights, reading activations).
· \mathcal{C}(t): Communication events (e.g., gradient synchronization, tensor parallelism).

This formulation treats W(t) as the primary driver of the power terms P_{\text{compute}}, P_{\text{memory}}, and P_{\text{interconnect}} in the server model.

3.2 Mapping Workloads to Server-Level Energy Variables
The execution of W(t) dynamically shapes the server's physical state. This mapping is central to energy-aware control:

W(t) \longrightarrow \{ V(t), f(t), C_{\text{eff}}(t), N_{\text{mem}}(t), N_{\text{bits}}(t), T(t) \}

The total energy for a workload, consistent with Eq. 2.9, is:

E_W = \int_0^{T_W} \Big[ C_{\text{eff}}(t)V(t)^2f(t) + V(t)I_{\text{leak}}(T(t)) + \beta(t)E_{\text{access}} + \gamma(t)E_{\text{bit}} \Big] dt

where T_W is the workload's makespan. This makes explicit that energy is a functional of the workload trajectory, not just hardware.

3.3 Computational Intensity and Effective Switching Activity
Define the computational intensity \alpha(t) as the rate of arithmetic operations (e.g., in FLOP/s). For CMOS-based processors, the effective switched capacitance is proportional to this intensity:

C_{\text{eff}}(t) = C_0 \cdot \alpha(t) \cdot A(t)

where C_0 is a hardware constant and A(t) \in [0,1] is the hardware activity factor (fraction of transistors switching). For a dense GEMM operation in a transformer layer, A(t) may approach ~0.2-0.3; for a sparse, pruned operation, it can drop to ~0.05. This directly links algorithmic choices to dynamic power:

P_{\text{dyn}}(t) = C_0 \, \alpha(t) \, A(t) \, V(t)^2 f(t)

3.4 Memory-Bound Regimes: The Dominant AI Cost Center
For modern AI models (e.g., LLMs with >100B parameters), energy is often dominated by data movement, not computation. Define the memory access rate \beta(t).

The memory energy from Eq. 2.4 becomes time-varying:

P_{\text{mem}}(t) = \sum_{i} \beta_i(t) \cdot E_{\text{access}}^{(i)}

where i indexes the memory hierarchy (e.g., HBM, DRAM). A key workload characteristic is the operational intensity (Ops/Byte). Low intensity signifies a memory-bound phase where energy is dictated by \beta(t), creating a critical optimization target.

3.5 Communication as a Necessary Energy Overhead
In distributed training, communication for gradient synchronization or tensor parallelism introduces a substantial, often dominant, energy overhead. Define the communication bit rate \gamma(t).

The communication power from Eq. 2.5 is:

P_{\text{comm}}(t) = \gamma(t) \cdot E_{\text{bit}}

For a workload like distributed SGD, \gamma(t) exhibits a periodic, bursty pattern aligned with optimizer steps, creating regular spikes in P_{\text{comm}}(t).

3.6 Temporal Structure and Phase-Based Modeling
AI workloads are phased and non-stationary. A training iteration has distinct phases: forward pass, backward pass, weight update, and synchronization. Each phase has a unique signature in \alpha(t), \beta(t), and \gamma(t).

This structure allows for a more precise, phase-summed energy model:

E_W = \sum_{\text{phase } p} \int_{t_p}^{t_{p+1}} P_{\text{total}}(t; W_p) \, dt

where W_p defines the operational mix of phase p. This phased view is essential for granular control, such as applying different DVFS states to compute-bound versus communication-bound phases.

3.7 Workload-Induced Thermal Feedback and Leakage Amplification
The workload W(t) directly drives the thermal dynamics from Eq. 2.6:

C_{\text{th}} \frac{dT}{dt} = P_W(t) - P_{\text{cooling}}(T(t), t)

where P_W(t) = P_{\text{dyn}}(t) + P_{\text{leak}}(T(t)) + P_{\text{mem}}(t) + P_{\text{comm}}(t).

A burst of high-intensity computation (\alpha(t) spikes) causes a rapid T increase. Due to the exponential dependence of I_{\text{leak}} on T (Eq. 1.5), this creates a workload-induced leakage amplification:

P_{\text{leak}}(T) \uparrow \rightarrow P_W \uparrow \rightarrow T \uparrow \rightarrow P_{\text{leak}}(T) \uparrow \uparrow

This means two workloads with identical total FLOPs can have different total energy costs if one has a temporal structure that excites this thermal feedback loop more severely.

3.8 The Total Workload Energy Amplification Factor (\eta_W)
The effective "wall-plug" energy of a workload must account for cooling and infrastructure overheads from Chapter 2. We define the total amplification factor:

\eta_W = \text{PUE} \cdot \left(1 + \frac{\Delta E_{\text{leak}}^{\text{thermal}}}{E_W}\right)

where:

· PUE is the infrastructure overhead (constant for a facility).
· \Delta E_{\text{leak}}^{\text{thermal}} is the additional leakage energy caused by workload-induced temperature rises above a baseline.

A "thermally adversarial" workload with sustained high power can see \eta_W significantly exceed the facility's PUE, a critical insight for holistic optimization.

3.9 Workloads as Control Inputs for Energy Optimization
This formalism enables a powerful systems perspective: The workload W(t) is a control input to the server's physical system. We can pose the energy optimization problem as finding the workload execution policy \pi (governing scheduling, batching, sparsity) that minimizes total energy subject to performance (throughput, accuracy) constraints:

\min_{\pi} E_W[\pi] \quad \text{s.t.} \quad \text{Perf}(W[\pi]) \geq \text{Perf}_{\text{req}}

The policy \pi acts by modulating the time profiles of \alpha(t), \beta(t), and \gamma(t), thereby shaping V(t), f(t), and T(t) through the coupled physical model.

3.10 Summary and Transition to Optimization Frameworks
This chapter has established that:

1. AI workloads are physical processes defined by time-varying operational mixes.
2. These mixes directly control the electrical and thermal state variables of the server.
3. The total energy is a functional of the workload's trajectory through this state space, amplified by thermal and infrastructure feedback.

Chapter 4 – Energy Optimization Under Physical and Thermodynamic Constraints

4.1 Introduction: The Energy Optimization Landscape for AI

Building upon the characterization of AI workloads as energy-shaping processes (Chapter 3), this chapter formalizes the challenge of minimizing total energy consumption as a constrained optimization problem. The goal is to find control policies that dictate how hardware resources are applied to a workload  W(t)  such that the functional

E_W = \int_0^{T_W} P_{\text{total}}(t) \, dt

is minimized, subject to the immutable physical laws of Chapters 1-2 and the performance requirements of the computational task. This establishes a rigorous bridge between the physical model of the server and the algorithmic control needed for practical energy savings, directly setting the stage for the feedback control implementations of Chapter 5.

We define two canonical optimization problems that encapsulate the primary operational modes of AI infrastructure:

1. The Energy Minimization Problem (EMP): Minimize total energy  E_W  subject to a performance deadline  T_W \leq T_{\text{deadline}} . This is typical for inference servers processing requests with latency constraints.
2. The Throughput Maximization Problem (TMP): Maximize computational throughput  \mathcal{T} = N_{\text{ops}} / T_W  subject to a fixed average power budget  \overline{P} \leq P_{\text{budget}} . This is characteristic of large-scale training clusters operating under a facility-level power cap.

4.2 Mathematical Formalization of the Optimization Problem

The system's state is defined by the variables from our physical model: core temperature  T(t) , voltage  V(t) , and frequency  f(t) . The control vector  \mathbf{u}(t)  available to the optimizer consists of the settable hardware parameters and workload execution parameters:

\mathbf{u}(t) = \left[ V_{\text{set}}(t), f_{\text{set}}(t), \text{BatchSize}(t), \text{SparsityMask}(t) \right]^T.

The workload  W(t) = \{\alpha(t), \beta(t), \gamma(t)\}  acts as a time-varying disturbance. The general, continuous-time optimization problem is therefore:

Problem P1: General Constrained Energy Minimization

\begin{aligned}
\min_{\mathbf{u}(t)} \quad & E_W = \int_0^{T_W} \Big( P_{\text{dyn}}(\mathbf{u}, W) + P_{\text{leak}}(T) + P_{\text{cooling}}(T) \Big) dt \\
\text{subject to} \quad & \text{Dynamics: } C_{\text{th}} \dot{T}(t) = P_{\text{total}}(t) - P_{\text{cooling}}(T(t), \mathbf{u}(t)) \\
& \text{Performance: } \int_0^{T_W} \alpha(t; \mathbf{u}) \, dt \geq N_{\text{ops}}^{\text{(req)}} \quad \text{(or } T_W \leq T_{\text{deadline}} \text{)} \\
& \text{Electrical: } V_{\min} \leq V(t) \leq V_{\max}, \quad f_{\min} \leq f(t) \leq f_{\max} \\
& \text{Thermal: } T(t) \leq T_{\text{safe}} \\
& \text{Power: } P_{\text{inst}}(t) \leq P_{\text{max}}.
\end{aligned}

4.3 Solution Approaches and Decomposition

Solving P1 directly is intractable due to its continuous-time, non-convex, and hybrid (mixed discrete-continuous) nature. We therefore adopt a pragmatic decomposition strategy.

4.3.1 Temporal Decomposition and Model Predictive Control (MPC)
Leveraging the phased structure of AI workloads (Chapter 3.6), we decompose the timeline into windows corresponding to computational phases (e.g., a single transformer layer). Over each window  \Delta t , the workload traits  \hat{W}  are relatively predictable. This enables a Receding Horizon Control approach:

\begin{aligned}
\mathbf{u}^*(t \rightarrow t+\Delta t) = \arg \min_{\mathbf{u}} \int_{t}^{t+\Delta t} P_{\text{total}}(\tau) d\tau \\
\text{s.t. } \quad T(t+\Delta t) \leq T_{\text{target}}, \quad \text{and local constraints.}
\end{aligned}

This breaks the global problem into a sequence of tractable, short-horizon optimizations, which will be executed by the predictive supervisor in Chapter 5.

4.3.2 Hierarchical Decomposition of Control Variables
The control variables naturally separate by time scale and domain:

· Fast Timescale (µs-ms): Voltage  V  and frequency  f  are optimized for a given workload phase and thermal state. This is the classic DVFS problem.
· Slow Timescale (ms-s): Workload parameters like batch size and sparsity are optimized to shape the demand presented to the hardware, influencing  \alpha(t)  and  \beta(t) .

Theorem 4.1 (Decoupled DVFS Optimization): For a fixed workload phase with constant effective switching activity  A \cdot C_0  and memory access rate  \beta , and ignoring transient thermal effects within the phase, the energy-optimal voltage-frequency pair  (V^*, f^*)  for that phase must satisfy

\frac{\partial P_{\text{dyn}}}{\partial V} \Big/ \frac{\partial \mathcal{T}}{\partial V} = \frac{\partial P_{\text{dyn}}}{\partial f} \Big/ \frac{\partial \mathcal{T}}{\partial f},

where  \mathcal{T}  is the phase throughput. This leads to the well-known relationship where optimal operation often lies at the knee of the voltage-frequency-energy curve.

Proof Sketch: This follows from formulating a Lagrangian for energy minimization subject to a throughput constraint for the phase,  \mathcal{T}(V, f) \geq \mathcal{T}_{\text{req}} . Applying the Karush-Kuhn-Tucker (KKT) conditions and assuming the constraint is active yields the stated equality of marginal energy-perfomance ratios.

4.4 Specialized Optimization Strategies for AI Workloads

4.4.1 Joint Optimization of DVFS and Batch Size
In inference scenarios, batch size  B  is a key control knob. Increasing  B  improves hardware utilization (higher  \alpha ) and amortizes memory latency, but increases request latency  T_W  and peak memory footprint. The joint optimization problem for a fixed workload becomes:

\min_{V, f, B} \quad E(B, V, f) = \frac{N_{\text{ops}}}{B \cdot \mathcal{T}(V,f)} \cdot P_{\text{avg}}(V,f, B)

\text{s.t.} \quad B \cdot t_{\text{iter}}(V,f) \leq T_{\text{SLA}}, \quad B \in \mathbb{Z}^+.

This is a mixed-integer nonlinear program (MINLP). A practical solution involves profiling  \mathcal{T}  and  P_{\text{avg}}  over a discrete set of  (V,f,B)  tuples and selecting the Pareto-optimal point that meets the latency Service Level Agreement (SLA).

4.4.2 Energy-Aware Sparsity and Precision Selection
As shown in Chapter 3, the hardware activity factor  A(t)  can be directly controlled via algorithmic sparsity (pruning) and numerical precision (INT8 vs. FP16). This modifies the effective  C_{\text{eff}} . The optimization incorporates a accuracy loss function  \mathcal{L}_{\text{acc}}(A, \text{precision}) :

\min_{A, \text{prec}} E \quad \text{s.t.} \quad \mathcal{L}_{\text{acc}} \leq \epsilon.

This directly links the physical optimization layer to the algorithmic fidelity of the AI model, creating a cross-stack co-design opportunity.

4.5 Fundamental Limits and the Efficiency Frontier

No optimization can violate the fundamental limits established in Chapter 1. We can now express these limits as boundaries of a feasible region in the energy-performance space.

4.5.1 The Thermodynamic Limit
The absolute lower bound for processing  N  bits of information irreversibly is:

E_{\text{min}} = N \cdot k_B T \ln 2.

For a modern AI accelerator performing ~1e15 ops/s, this is ~3 mW, a bound 9 orders of magnitude below practical power consumption (~300W). The gap defines the scope for engineering optimization.

4.5.2 The Device and System Limits
Practical limits form a cascade:

1. Device Physics Limit: Defined by  V_{\text{min}}  (subthreshold operation) and  f_{\text{max}}  (carrier velocity).
2. Packaging/Power Delivery Limit: Defined by  I_{\text{max}} ,  P_{\text{max}} , and thermal impedance  R_{\theta} .
3. Facility Limit: Defined by the rack-level power budget and cooling capacity (PUE).

Definition 4.1 (Energy Efficiency Frontier): For a given AI workload  W  and hardware system, the Energy Efficiency Frontier is the Pareto-optimal surface in the multi-dimensional space of (Energy, Throughput, Model Accuracy), defined by the solutions to the optimization problem P1 under varying performance constraints.

Optimization strategies move system operation toward this frontier. The hierarchical controller of Chapter 5 will stabilize operation near it in the presence of disturbances.

4.6 Chapter Summary and Transition to Control

This chapter has translated the physical understanding of AI server energy (Chapters 1-2) and the workload characterization (Chapter 3) into a formal optimization framework. We have:

· Defined the core Energy Minimization and Throughput Maximization problems (P1).
· Proposed decomposition strategies (temporal/hierarchical) to render them tractable.
· Derived specialized formulations for AI-specific knobs like batch size and sparsity.
· Articulated the fundamental limits that define the efficiency frontier.

The solutions to these optimization problems are, however, open-loop policies  \mathbf{u}^*(t)  that assume perfect knowledge of the future workload  W(t) . In reality, workloads are dynamic and uncertain. Therefore, the optimal policy must be implemented in a closed-loop, feedback-driven manner. This leads directly to the next chapter.

Chapter 5 – Hierarchical Feedback Control for Energy-Aware AI Server Operation

5.1 The AI Accelerator Control Problem

The optimization framework in Chapter 4 defines a theoretical, omniscient policy \mathbf{u}^*(t). However, real-world AI workloads \hat{W}(t) are non-deterministic and only partially observable in real-time. Consequently, a dynamic control system is essential to approximate \mathbf{u}^*(t) by continuously modulating the server's physical state. The core challenge is to prevent performance-degrading thermal throttling while minimizing energy, which requires a controller that proactively manages thermal headroom. This chapter transforms the physics-based models from Chapters 1-3 and the optimization formalism of Chapter 4 into a practical, hierarchical feedback control architecture.

5.2 State-Space Representation of AI Server Dynamics

An AI accelerator (e.g., GPU, TPU) is a multi-domain physical plant. We define an augmented state vector that captures critical thermal and performance domains:

\mathbf{x}(t) = \begin{bmatrix} T_{\text{core}}(t) \\ T_{\text{mem}}(t) \\ P_{\text{avg}}(t) \\ f_{\text{eff}}(t) \end{bmatrix}

Here, T_{\text{core}} is the junction temperature (critical for leakage), T_{\text{mem}} is the HBM temperature, P_{\text{avg}} is a moving-average power (for cap compliance), and f_{\text{eff}} is the actual achieved clock frequency. The control vector comprises the available actuators:

\mathbf{u}(t) = \begin{bmatrix} f_{\text{req}}(t) \\ \text{CoolantFlow}(t) \end{bmatrix}

Voltage V(t) is often coupled to frequency in modern accelerators via predefined P-states. The primary disturbance is the workload itself:

\mathbf{d}(t) = \hat{W}(t) = \left[ \alpha(t), \beta(t), \gamma(t) \right]^T

The system dynamics are governed by the coupled differential equations from previous chapters:

\dot{\mathbf{x}}(t) = \mathbf{f}(\mathbf{x}(t), \mathbf{u}(t), \mathbf{d}(t))

with the core thermal dynamic being:

C_{\text{th}}\dot{T}_{\text{core}}(t) = P_{\text{dyn}}(\mathbf{u}, \mathbf{d}) + P_{\text{leak}}(T_{\text{core}}) - \kappa \cdot \text{CoolantFlow}(t) \cdot (T_{\text{core}}-T_{\text{coolant}})

This formulation captures the essential coupling where workload intensity drives power, which increases temperature, which exacerbates leakage.

5.3 A Hierarchical Control Architecture

Given the vast difference in time scales—electrical (µs), thermal (ms), and workload phase (seconds)—a single control loop is insufficient. We propose a three-tier hierarchical controller:

· Tier 1: Fast Power Cap Controller (Sub-ms)
  A saturated integral controller that acts as a safety valve. It monitors instantaneous power P_{\text{inst}}(t) and overrides the requested frequency if a hard ceiling P_{\text{max}} is exceeded:
  f_{\text{cap}}(t) = f_{\text{req}}(t) - K_I \int_{0}^{t} \max(0, P_{\text{inst}}(\tau) - P_{\text{max}}) \, d\tau
  This ensures hardware reliability despite workload surges.
· Tier 2: Thermal Governor (ms-100ms)
  A Proportional-Integral (PI) controller that regulates core temperature around a dynamic setpoint T_{\text{ref}}(t) by modulating cooling:
  \text{CoolantFlow}(t) = K_P (T_{\text{core}}(t)-T_{\text{ref}}(t)) + K_I \int (T_{\text{core}}(t)-T_{\text{ref}}(t)) dt
  It also implements a protective "thermal throttling" rule, gradually reducing f_{\text{req}} if T_{\text{core}} approaches T_{\text{safe}}.
· Tier 3: Workload-Aware Predictive Supervisor (Seconds)
  This is the energy optimization brain. Using a short-term forecast of workload intensity \hat{W}(t:t+H) (from Chapter 3.6), it solves a receding-horizon instance of the Chapter 4 optimization problem. Its outputs are the optimal trajectories for the setpoints used by Tier 2:
  \min_{\mathbf{u}} \int_{t}^{t+H} P_{\text{total}}(\tau) \, d\tau \quad \text{s.t.} \quad \dot{\mathbf{x}} = \mathbf{f}(\mathbf{x}, \mathbf{u}, \hat{W}), \quad T_{\text{core}} \le T_{\text{margin}}
  It strategically sets a lower T_{\text{ref}}(t) before a predicted compute-intensive phase to build thermal headroom, or allows a higher T_{\text{ref}} during memory-bound phases to save cooling energy.

5.4 Stability and Robustness in Practical Deployment

Stability analysis must account for real-world imperfections. The closed-loop system must be robust to:

1. Sensor Noise and Delay: Temperature and power readings are noisy and lagged. We employ a Kalman filter that fuses sensor data with the physics-based model (Ch2) to provide smoothed state estimates \hat{\mathbf{x}}(t) to the controllers.
2. Actuator Quantization: Frequency and voltage are set via discrete P-states. The control law must include hysteresis or dithering to prevent chattering at state boundaries.
3. Model Uncertainty and Disturbances: The controller's internal model of P_{\text{dyn}}(\mathbf{u}, \mathbf{d}) will always be approximate. The integral action in the PI controllers provides robustness against steady-state modeling error, while the hierarchical design localizes the impact of uncertainty.

A Lyapunov analysis of the thermal governor loop confirms stability for bounded disturbances \mathbf{d}(t), provided the gain parameters are chosen within limits defined by the thermal capacitance C_{\text{th}} and the worst-case cooling delay.

5.5 Integration and Synthesis: From Workload to Actuator

This control architecture completes the energy-shaping interface. The AI workload \hat{W}(t) is the disturbance. The predictive supervisor (Tier 3) interprets this disturbance within the physical model and defines an efficient state trajectory. The thermal governor (Tier 2) and power cap (Tier 1) act as lower-level servo controllers, ensuring the physical hardware tracks this trajectory despite noise and uncertainty. This forms a physically-grounded, workload-aware, and dynamically adaptive control system.

5.6 Summary and Thesis Synthesis

Chapter 5 provides the final, critical translation from theory to practice. It answers the "how" of energy-aware operation:

· Chapter 1 established the invariant physical laws (thermodynamics, CMOS switching) that govern all energy flows.
· Chapter 2 derived a server-level model quantifying these flows into computable components (compute, memory, cooling).
· Chapter 3 characterized AI workloads as the time-varying physical excitation W(t) that drives the model.
· Chapter 4 formulated the offline optimization problem \min E subject to the constraints of the previous chapters.
· Chapter 5 has now developed the online control system that solves this problem in real-time, using feedback and prediction to safely navigate the trade-offs between performance, power, and temperature.

This hierarchical controller is the engineered embodiment of the thesis's core proposition: that rigorous energy reduction in AI systems requires treating them not as abstract computational units, but as physical dynamical systems, where the workload is the control input and energy is the ultimate cost function. The following chapters will validate this integrated model and control strategy through simulation and empirical measurement.
---

References for Integration:

· Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press. (For KKT conditions and decomposition methods).
· Horowitz, M., et al. (2014). "Digital Circuit Design Trends." IEEE Proceedings. (For voltage-frequency-energy trade-off curves).
· Wu, C., et al. (2022). "Energy-Aware Batch Size and DVFS Co-optimization for Neural Network Inference." ACM SIGMETRICS. (For joint batch size and DVFS optimization).
· Han, S., et al. (2015). "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding." ICLR. (For accuracy-energy trade-offs of sparsity/precision).
· AI Workload Characterization: "The Computational Limits of Deep Learning" (Thompson et al., 2020) for growth trends in ops/parameters.
· Memory-Bound Analysis: "Demystifying AI Compute" (Rhu et al., 2022) or MLPerf Inference benchmarks for concrete data on memory bandwidth saturation.
· Thermal Feedback: Recent empirical studies on GPU throttling during LLM inference (e.g., "Benchmarking LLM Inference" from Fathom, 2024).
· IEA (2025), Energy and AI: For global energy projections and infrastructure trends.
· Hennessy & Patterson, Computer Architecture (7th ed.): For canonical power and memory hierarchy models.
· Wolpert, D.H. (2024). "The Stochastic Thermodynamics of Computation." J. Phys. A: For a modern treatment of Landauer's principle and non-equilibrium costs.
· Industry White Papers & Roadmaps (NVIDIA, Intel, 2024-2025): For current rack power densities, cooling solutions, and accelerator specifications.

