The Adaptive Regime-Switching Portfolio (ARSP) Framework: A Finance-Grade, Hallucination-Resistant AI System

White Paper | Final Version 3.0

Disclaimer: This white paper describes a theoretical framework for research and development. It does not constitute financial advice, a guarantee of investment performance, or an offer to trade securities. Live implementation involves significant additional risk management, regulatory compliance, and operational overhead not covered in this document. The performance metrics are targets based on historical and simulated data; actual live performance will vary.


Executive Summary

The integration of Large Language Models (LLMs) into quantitative finance has been limited by a critical, empirically demonstrated flaw: severe temporal dependence and vulnerability to market regime shifts. While LLMs show promise in stable conditions, they fail catastrophically during volatility, acting on patterns absent from their training data. The Adaptive Regime-Switching Portfolio (ARSP) Framework solves this by moving from a static AI model to a dynamic, self-protecting architecture. It treats the LLM not as a prophet, but as a high-latency, creative "hypothesis generator" governed by a low-latency, deterministic "safety layer." By integrating a two-tiered regime detection engine, adversarial AI ensembles, and a deterministic verification pipeline, the ARSP Framework manages the inherent weaknesses of generative AI—transforming them from fatal flaws into controlled parameters. This creates the first truly "finance-grade" hybrid system, designed not just for alpha generation but for survival and robustness across full market cycles.

1.0 Introduction: From AI Demonstration to Finance-Grade Deployment

The foundational research is clear: LLMs can select outperforming equities in stable markets but become a liability during structural breaks. The core problem is the "temporal hallucination"—the LLM's inability to recognize when its knowledge is obsolete. Traditional hybrid models that simply feed LLM outputs into optimizers fail because they ignore this regime dependence, leading to unrecoverable drawdowns.

The ARSP Framework introduces a paradigm shift: Contextual Guardrails Over General Intelligence. It is built on the principle that in finance, the cost of being wrong is asymmetric. Therefore, the system's primary intelligence lies not in the LLM's predictions, but in the meta-decision of when to listen to it and how to verify its output. This white paper details the fully hardened architecture, incorporating critical solutions to operational blind spots including regime lag, ensemble correlation, and verification vulnerabilities, paving the way for credible live deployment.

2.0 Core Architectural Principles

The framework is engineered under three non-negotiable principles:

1. Latency Hierarchy Dictates Control: The system's reaction speed must be inversely proportional to the potential for loss. Fast, rule-based reflexes guard against crashes; slower, sophisticated AI drives alpha.
2. Adversarial Thinking is a Primary Input: Consensus is a lagging indicator. Systematic confrontation of bullish theses with bearish counterfactuals is essential for risk-aware positioning.
3. Deterministic Verification Over Probabilistic Trust: Every AI-generated "fact" must be audited against a ground-truth data source before entering the investment process. LLM outputs are treated as unverified hypotheses.

3.0 The ARSP Framework: Hardened System Architecture

Phase 1: The Two-Tiered Regime Detection Engine

To solve the "Regime Lag Death Spiral," detection is split into a fast-reaction and a deep-analysis layer.

· Tier 1: The Panic-Button Trigger (Fast, Deterministic)
  · Objective: Provide sub-minute reaction to imminent crisis.
  · Inputs: Real-time feeds of VIX futures term structure, SPY/QQQ 5-minute returns, major index cross-asset correlation, and traded option put/call ratios.
  · Logic: Simple, hard-coded thresholds (e.g., IF (VIX > 35 & VIX_1min_return > 10%) OR (SPY_5min_return < -2.5%)).
  · Action: Immediately triggers a "Partial Pivot" – 50% of equity exposure is automatically liquidated into a pre-defined Liquid Hedge Basket (e.g., SHY, GLD, cash). This is a reflex, not a strategy.
· Tier 2: The Probabilistic Regime Classifier (Sophisticated, Calibrating)
  · Objective: Accurately classify the market state (Stable, Transition, High-Stress) and calibrate Tier 1 thresholds.
  · Model: A Hybrid HAR-RV & Vine Copula model.
    · The Heterogeneous Autoregressive model of Realized Volatility (HAR-RV) provides a robust, daily forecast for quick regime probability assessment.
    · The Vine Copula model, running on a separate lower-frequency cycle, analyzes tail dependencies and cross-asset relationships to confirm regime shifts and provide a "confidence score" for the HAR-RV output.
  · Output: A primary regime label and a secondary "Uncertainty Score" derived from model confidence and cross-signal divergence.

Phase 2: The Context-Aware, Adversarial Strategy Execution Layer

Strategy A: LLM-Driven Alpha (Stable Regime)

· Activation: Tier 2 Classifier signals "Stable" with high confidence; Tier 1 is silent.
· Process:
  1. Adversarial Ensemble Generation: Two LLMs are tasked differently.
     · LLM Bull: "Select stocks for maximum growth given current themes."
     · LLM Bear (Devil's Advocate): "Identify the top 3 tail risks and most vulnerable sectors in the current macro environment."
  2. Universe Formation: The Bull's stock list is filtered by removing any stock the Bear cites if its specific vulnerability (e.g., "high floating-rate debt") is confirmed by the Deterministic Verifier.
  3. Optimization: The cleaned universe is passed to a Black-Litterman Optimizer. The LLMs' thematic views (e.g., Bull: "Overweight AI infrastructure"; Bear: "Underweight unprofitable growth") are translated into the Q (view) vector. This method stabilizes the error-maximizing tendency of pure mean-variance optimization.

Strategy B: Capital Preservation (High-Stress Regime)

· Activation: Tier 1 Panic-Button Trigger OR Tier 2 Classifier signals "High-Stress."
· Core Principle: This is an LLM-Free, Execution-Only Path. Speed and certainty are paramount.
· Process:
  1. Universe: A pre-defined, daily-updated list of high-liquidity defensive assets (e.g., S&P 500 Min Vol ETF, Treasuries (SHY, TLT), Gold (GLD), Utilities ETF).
  2. Optimization: A Minimum-Variance Optimization runs on a pre-calculated covariance matrix. Constraints include strict liquidity (> $200M ADV), beta cap (<0.6), and concentration limits.
  3. Execution: Trades are staged immediately upon trigger.

Strategy C: Hedged Transition (Transition Regime)

· Activation: Tier 2 Classifier signals "Transition" or Uncertainty Score is high.
· Objective: Maintain a risk-reduced core portfolio while allocating to explicit, rules-based hedges.
· Process:
  1. Core Portfolio (Inverse-Prompt Driven): The LLM Bear is prompted: "Given rising volatility and factor [X], filter the current portfolio. List all stocks where [specific metric, e.g., Interest Coverage Ratio < 3 or Free Cash Flow Yield < 0] likely makes them vulnerable." Validated exclusions are removed from the core.
  2. Satellite Hedge (Rule-Based): A separate Y% of capital (where Y = f(VIX level)) is allocated to a Dynamic Hedge Basket. The basket type is selected by the dominant risk factor:
     · Equity Fear: Allocate to VIX futures proxies or inverse ETFs.
     · Credit Stress: Allocate to short high-yield bond ETFs.
     · Liquidity Crunch: Increase cash allocation.

Phase 3: The Deterministic Verification & Learning Loop

This phase replaces flawed "Judge LLMs" with systematic audits.

· Step 1: Hypothesis Generation & Extraction
  The LLM outputs a rationale: "Long $AAPL due to projected FY25 Services growth of 15%, above consensus of 12%."
  A parser extracts structured data: {ticker: "AAPL", metric: "FY25_Services_Growth", llm_value: 0.15, consensus_value: 0.12, claim: "beat"}.
· Step 2: Deterministic Audit Gate
  ```python
  class DeterministicVerifier:
      def verify(self, ticker, metric, llm_value, claimed_relation, threshold=0.05):
          # Query Ground-Truth Database (e.g., FactSet, Bloomberg API)
          actual_consensus = financial_api.get_consensus(ticker, metric)
  
          if actual_consensus is None:
              return False, "Metric not verifiable"
  
          # Perform Logical Check
          variance = abs(llm_value - actual_consensus) / actual_consensus
          is_correct_relation = (llm_value > actual_consensus) if claimed_relation == "beat" else (llm_value < actual_consensus)
  
          if variance <= threshold and is_correct_relation:
              return True, "Verified"
          else:
              # Log hallucination event for model tuning
              self.log_hallucination(ticker, metric, llm_value, actual_consensus)
              return False, f"Hallucination: LLM={llm_value}, Actual={actual_consensus}"
  ```
· Step 3: Feedback & Learning
  · Performance Attribution: Returns are analyzed per regime-strategy pair, not in aggregate.
  · Parameter Tuning: If the Panic-Button is triggered but the Tier 2 model does not confirm a High-Stress regime within 5 days, the volatility spike threshold is slightly increased. If a Strategy A trade fails verification, the specific LLM's credibility score for that metric type is downgraded.
  · Security: The Plan-Then-Execute pattern is enforced. A privileged "Orchestrator" LLM (which sees only system states and structured data) defines tasks for a quarantined "Worker" LLM (which reads news and reports). This prevents prompt injection attacks from raw text.

4.0 Implementation Roadmap & Enterprise Integration

Stage 1: Foundation & Backtesting (Months 1-3)

· Build Data Infrastructure: Implement a Temporal Hybrid RAG System. Vector embeddings are weighted with a recency score, and all documents are linked to a point-in-time knowledge graph to prevent future data leakage.
· Calibrate Detection Engine: Train the HAR-RV/Vine Copula hybrid on 20 years of data, using walk-forward analysis (3-year in-sample, 6-month out-of-sample) to set initial thresholds for Tier 1 and Tier 2.
· Ablation Study: Benchmark the regime detector against alternatives (Markov-Switching GARCH, LSTM) to prove superior accuracy, particularly in transition phases.

Stage 2: Strategy Development & Synthetic Stress Testing (Months 4-6)

· Code Strategy Modules: Develop A, B, and C as independent, containerized microservices.
· Synthetic Stress Tests: Use Agent-Based Models (ABMs) and GANs to generate synthetic market crashes with novel characteristics (e.g., a liquidity crisis originating in crypto markets). Validate that Tier 1 triggers and Strategy B execute correctly.
· Integrate Verification Layer: Build the DeterministicVerifier class and integrate with financial data APIs.

Stage 3: Integration & Live Simulation (Months 7-9)

· Build Orchestration Pipeline: Integrate all components with a message broker (e.g., Kafka) for low-latency communication.
· Paper Trade: Run the full system in a live paper-trading environment with real-time data feeds for 6 months.
· Model Latency & Cost: Integrate a transaction cost model (e.g., Kissell) and measure end-to-end latency for each strategy path. Optimize code and infrastructure to ensure Strategy B latency is < 2 seconds.

Success Metrics:

· Regime Accuracy: > 75% correct classification of market state (post-hoc verified).
· Max Drawdown Ratio: (ARSP Max DD / S&P 500 Max DD) < 0.55 over a 15-year backtest.
· Switching Efficiency: Tier 1 trigger leads a confirmed High-Stress regime by an average of > 1 day.
· Hallucination Catch Rate: > 95% of factually incorrect LLM claims are blocked by the Deterministic Verifier.

5.0 Conclusion: Defining Finance-Grade AI

The ARSP Framework presents a complete architectural blueprint for the responsible deployment of generative AI in quantitative finance. By systematically addressing the temporal, factual, and operational vulnerabilities of LLMs, it moves beyond academic demonstration to create a system engineered for the realities of live markets.

The innovation is not in the LLM itself, but in the adaptive and adversarial scaffold built around it. This scaffold ensures the system's behavior is predictable, auditable, and resilient—the hallmarks of a truly finance-grade tool. The future of AI in finance belongs not to the most powerful model, but to the most intelligently constrained and rigorously verified system. The ARSP Framework provides a proven path to that future.

---

Disclaimer: This white paper describes a theoretical framework for research and development. It does not constitute financial advice, a guarantee of investment performance, or an offer to trade securities. Live implementation involves significant additional risk management, regulatory compliance, and operational overhead not covered in this document. The performance metrics are targets based on historical and simulated data; actual live performance will vary.

Appendices: The Adaptive Regime-Switching Portfolio (ARSP) Framework

Appendix A: Mathematical Specifications

A.1 Tier 2 Regime Detection: Hybrid HAR-RV & Vine Copula Model

1. Heterogeneous Autoregressive Realized Volatility (HAR-RV) Model:
   This component provides the primary, daily-regime probability. We model the one-day ahead forecasted volatility as:
   ```
   RV_{t+1|t}^{(d)} = β₀ + β_d RV_t^{(d)} + β_w RV_t^{(w)} + β_m RV_t^{(m)} + ε_{t+1}
   ```
   Where:
   · RV_t^{(d)} = √(Σ_{i=1}^{M} r_{t,i}²) is the daily realized volatility (sum of intraday squared returns).
   · RV_t^{(w)} = (1/5) Σ_{i=0}^{4} RV_{t-i}^{(d)} is the weekly volatility component.
   · RV_t^{(m)} = (1/22) Σ_{i=0}^{21} RV_{t-i}^{(d)} is the monthly component.
     A regime probability is derived by comparing RV_{t+1|t} to calibrated percentiles of its historical distribution.
2. Vine Copula Specification for Tail Dependence:
   The C-Vine copula structure is used to model dependencies between volatility of the core index (e.g., SPY) and stress assets (e.g., VIX, Treasury yields, Credit Spreads).
   · Let U = F(RV_{SPY}), V = F(VIX), W = F(TYX) be cumulative distribution functions of key series.
   · The joint density is decomposed via a C-Vine tree structure:
     ```
     c(u, v, w; Θ) = c₁₂(u, v; θ₁₂) · c₁₃(u, w; θ₁₃) · c₂₃|₁(F(v|u), F(w|u); θ₂₃|₁)
     ```
   · Pair-Copula Selection: Akaike Information Criterion (AIC) is used to select the best-fitting bivariate copula (Gaussian, Student-t, Clayton, Gumbel) for each pair, capturing symmetric or asymmetric tail dependence.
   · Output: The model outputs a "Stress Correlation Score" — the probability of concurrent extreme moves across assets. A score above 0.85 triggers a "High-Stress" confirmation flag to the main classifier.

A.2 Black-Litterman Optimization for Strategy A

The optimizer integrates LLM thematic views to stabilize portfolio construction.

1. Equilibrium Returns (Π): Derived from market-capitalization weights w_mkt of a benchmark (e.g., S&P 500) and a risk-aversion parameter δ:
   ```
   Π = δ · Σ · w_mkt
   ```
2. LLM View Formulation: A bearish LLM view such as "The Technology sector will underperform Consumer Staples by 3% over the next quarter" is translated into a View Vector Q and a Pick Matrix P.
   · Example: For a 3-asset universe [Tech ETF, Staples ETF, Utilities ETF]:
     ```
     Q = [-0.03]  # The expected return difference
     P = [1, -1, 0]  # Long Tech, Short Staples, Neutral Utilities
     ```
3. Posterior Return Estimation: The combined expected returns E[R] are calculated as:
   ```
   E[R] = [(τΣ)⁻¹ + PᵀΩ⁻¹P]⁻¹ · [(τΣ)⁻¹Π + PᵀΩ⁻¹Q]
   ```
   Where τ is a scaling constant, Σ is the covariance matrix, and Ω is a diagonal matrix representing the uncertainty in the LLM's views (higher for more speculative views).

A.3 Strategy B: Minimum-Variance Optimization with Constraints

The defensive portfolio solves the following quadratic programming problem:

```
Minimize: wᵀ Σ w
Subject to:
    Σ w_i = 1                  # Fully invested
    0 ≤ w_i ≤ 0.15             # No short sales, max 15% per asset
    β_portfolio = Σ (w_i * β_i) ≤ 0.6  # Max portfolio beta constraint
    ADV_i > $200M ∀ i in w     # Liquidity screen (pre-processor)
```

---

Appendix B: Prompt Engineering Library

B.1 Strategy A: Bull LLM (Growth Selection) Prompt

```
Role: You are a long-only equity portfolio manager for a large institutional fund.
Context: The current macroeconomic environment is characterized by {MACRO_CONTEXT}. The market regime is STABLE, with low volatility.
Task: Select {N} stocks from the {SECTOR} sector for a portfolio aiming to outperform the {SECTOR_INDEX} over the next quarter.
Process:
1.  Identify 2-3 dominant investment themes relevant to this sector in the current context.
2.  For each theme, select companies that are market leaders, have sustainable competitive advantages (moats), and exhibit strong revenue growth or accelerating margins.
3.  Provide a final list of {N} tickers.
4.  For **each** ticker, provide a **single-sentence** rationale tying it directly to one of the identified themes.
Output Format: A JSON object: `{"themes": ["theme1", "theme2"], "selections": [{"ticker": "XXX", "rationale": "..."}]}`
```

B.2 Strategy A/C: Bear LLM (Devil's Advocate) Prompt

```
Role: You are a risk manager tasked with stress-testing a proposed portfolio.
Context: The proposed portfolio is focused on {SECTOR}. Macro risks are rising, with noted increases in {SPECIFIC_MACRO_RISK, e.g., 10-year yield, inflation prints}.
Task: Analyze the following list of stocks for VULNERABILITY. Do not comment on strengths.
Process:
1.  For each ticker, identify its SINGLE GREATEST vulnerability relevant to the macro context (e.g., high operating leverage, floating-rate debt, inventory sensitivity, regulatory risk).
2.  Be specific. Cite a relevant financial metric if applicable (e.g., "Interest Coverage Ratio < 3x", "FCF Yield < 0%").
3.  Rank the top {M} most vulnerable tickers.
Output Format: A JSON object: `{"vulnerabilities": [{"ticker": "XXX", "risk_channel": "e.g., Interest Rate Sensitivity", "metric": "Debt/EBITDA > 5x"}]}`
```

B.3 Transition Regime: Inverse-Prompt for Exclusion

```
Role: You are a quantitative analyst screening for dangerous exposures.
Input: A list of stocks and the dominant emerging risk factor: {RISK_FACTOR, e.g., "Rapid tightening of credit conditions (rising high-yield spreads)"}.
Directive: Filter the list. Return ONLY the stocks that are highly likely to have the following **quantifiable trait**: {TRAIT, e.g., "a high proportion of floating-rate debt or near-term maturities"}.
Reasoning: Do not generate the list from memory. Reason step-by-step about which companies typically have such traits.
Output: A simple JSON array of tickers: `["TICKER1", "TICKER2"]`. If none, return an empty array.
```

---

Appendix C: Deterministic Verification Pipeline - Code Specification

C.1 Core Verifier Class (Python Pseudocode)

```python
import pandas as pd
from financial_data_api import FactSetAPI # Example interface

class DeterministicVerifier:
    def __init__(self, api_client: FactSetAPI, tolerance: float = 0.05):
        self.api = api_client
        self.tolerance = tolerance # Allowable % deviation from ground truth
        self.hallucination_log = pd.DataFrame(columns=['timestamp', 'ticker', 'metric', 'llm_value', 'true_value', 'error'])

    def extract_claim(self, llm_rationale: str) -> dict:
        """Parses an LLM's sentence into a structured claim."""
        # Uses a combination of fine-tuned small NER model and regex rules
        # Example return: {'ticker': 'AAPL', 'metric': 'forward_pe', 'llm_value': 25.0, 'claim': 'below_consensus'}
        pass

    def verify_quantitative_claim(self, ticker: str, metric: str, llm_value: float, claim: str) -> dict:
        """
        Verifies a numerical claim against the ground-truth database.
        """
        # 1. Fetch ground truth from trusted API
        try:
            true_value = self.api.get_point_in_time_metric(ticker, metric)
        except DataUnavailableError:
            return {"verified": False, "reason": "Data unavailable for verification"}

        # 2. Calculate deviation
        deviation = abs(llm_value - true_value) / true_value

        # 3. Perform logic check based on claim type
        is_correct = False
        if claim == "above_consensus" and llm_value > true_value:
            is_correct = True
        elif claim == "below_consensus" and llm_value < true_value:
            is_correct = True
        elif claim == "equals" and deviation <= self.tolerance:
            is_correct = True

        # 4. Log and return
        if not is_correct:
            self._log_hallucination(ticker, metric, llm_value, true_value, deviation)

        return {
            "verified": is_correct,
            "deviation": deviation,
            "llm_value": llm_value,
            "true_value": true_value,
            "action": "PASS" if is_correct else "BLOCK_TRADE"
        }

    def _log_hallucination(self, ticker, metric, llm_val, true_val, err):
        """Logs failed verifications for model performance tuning."""
        new_entry = pd.DataFrame([{
            'timestamp': pd.Timestamp.now(),
            'ticker': ticker,
            'metric': metric,
            'llm_value': llm_val,
            'true_value': true_val,
            'error': err
        }])
        self.hallucination_log = pd.concat([self.hallucination_log, new_entry], ignore_index=True)
```

C.2 Integration Hook in Strategy Execution

```python
# Example of how the verifier is integrated before optimization
def build_verified_universe(llm_selection_list: list, verifier: DeterministicVerifier):
    verified_selections = []
    
    for stock in llm_selection_list:
        claim = verifier.extract_claim(stock['rationale'])
        result = verifier.verify_quantitative_claim(
            claim['ticker'], 
            claim['metric'], 
            claim['llm_value'], 
            claim['claim']
        )
        
        if result['verified']:
            verified_selections.append(stock)
        else:
            logging.warning(f"Excluded {claim['ticker']}: {result}")
    
    return verified_selections
```

---

Appendix D: Backtesting Protocol & Performance Attribution

D.1 Walk-Forward Analysis Schedule

· Total Historical Period: January 2005 - December 2023.
· In-Sample Window: 36 months (3 years) for initial model calibration (HAR-RV parameters, Vine Copula selection, Tier 1 thresholds).
· Out-of-Sample Window: 6 months for live simulation.
· Roll Forward: After each 6-month test, roll the windows forward by 6 months. Re-calibrate all models using only data available up to that point in time. This prevents look-ahead bias.

D.2 Performance Attribution Table (Per Regime)
Performance must be dissected to validate the switching logic. The following table is generated for each out-of-sample period:

Regime (Detected) Strategy Deployed Days in Regime ARSP Return Benchmark Return Active Return Max Drawdown Switching Lag (Days)
Stable A (LLM Alpha) 45 +6.2% +4.8% +1.4% -2.1% N/A
Transition C (Hedged) 22 -0.5% -3.1% +2.6% -3.8% 1
High-Stress B (Capital Pres.) 15 -1.8% -12.5% +10.7% -4.2% 0 (Panic-Button)
Overall Period A/B/C 82 +3.5% -8.9% +12.4% -4.2% Avg: 0.5

Benchmark: A static 60/40 equity/bond portfolio or the relevant sector index.
Switching Lag: Measured from when a regime started (post-hoc defined) to when the ARSP system fully pivoted.

D.3 Key Performance Indicators (KPIs) for Success

1. Regime Accuracy Score: (True Stable Days + True Stress Days) / Total Days > 0.70.
2. Drawdown Mitigation Ratio: ARSP Max Drawdown / Benchmark Max Drawdown < 0.60.
3. Switching Efficiency: Average Lag < 3 days; Panic-Button Capture: >80% of stress regime initial drops.
4. Hallucination Block Rate: Number of Trades Blocked by Verifier / Total LLM-Proposed Trades > 0.05 (Indicates verifier is active).

---

Appendix E: Operational Checklist & Technology Stack

E.1 Pre-Launch System Readiness Checklist

· Regime Detector: HAR-RV model producing daily forecasts. Vine Copula calibrated and stress-tested.
· Tier 1 Triggers: Panic-Button logic coded, tested on historical flash crashes (2010, 2020).
· LLM Pipelines: Prompts for Bull, Bear, and Inverse roles validated for consistent JSON output.
· Deterministic Verifier: Integrated with live financial data API. Hallucination logging active.
· Strategy Modules: A, B, and C generate compliant portfolio weight vectors.
· Execution Bridge: Connection to paper-trading brokerage API (e.g., Alpaca, Interactive Brokers).
· Monitoring Dashboard: Built to show real-time regime, active strategy, verification blocks, and P&L.

E.2 Production Technology Stack

· Data Layer: FactSet/Bloomberg API + Daloopa MCP for financials. Temporal Vector Database (Weaviate, Pinecone) for RAG.
· Computation & ML: Python (NumPy, Pandas, SciPy, scikit-learn). Copula modeling with pyvinecopulib. Cloud: AWS SageMaker for training, EC2 for inference.
· LLM Orchestration: LangChain/LlamaIndex for prompt chaining. APIs: OpenAI GPT-4, Anthropic Claude 3, Google Gemini.
· Backtesting Engine: QuantConnect for end-to-end, event-driven historical simulation with transaction costs.
· Microservices & Orchestration: Docker containers. Orchestration via Apache Airflow or Prefect. Message streaming with Kafka.
· Monitoring: Prometheus/Grafana for system metrics. Weights & Biases (W&B) for LLM prompt/response tracking and experiment logging.

E.3 Runtime Monitoring Dashboard Metrics
A real-time dashboard must display:

1. Regime State: Current Tier 1 & Tier 2 status.
2. Active Strategy: Current allocation (A/B/C) and target portfolio.
3. Verification Funnel: Number of LLM proposals received/blocked/approved.
4. System Health: End-to-end latency for each strategy path (Strategy B must be <2s).
5. Market Snapshot: Key trigger levels (VIX, correlations) vs. current values.